<!DOCTYPE html>
<html lang="es">

<head>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0" />
    <title>MACHINELEARNEAR</title>
    <meta name="Description" content="Un lugar para cranear mientras nos tomamos unos amargos">

    <link href="#schema" type="application/json" rel="exhibit-data" />
    <link href="#data" type="application/json" rel="exhibit-data" />
      
    <!-- libs simile-exhibit -->

    <script src="//api.simile-widgets.org/exhibit/current/lib/jquery.min.js" type="text/javascript">
    </script>
    <!-- <link rel="exhibit-extension" href="/js/exhibit/extensions/time/time-extension.js" />
    -->
    <script src="//api.simile-widgets.org/exhibit/current/exhibit-api.js" type="text/javascript"></script>
    
    <!-- Bootstrap -->
    <link rel="stylesheet" href="//cdn.jsdelivr.net/bootstrap/3.3.0/css/bootstrap.min.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/bootstrap/3.3.0/css/bootstrap-theme.min.css">
    <script src="//cdn.jsdelivr.net/bootstrap/3.3.0/js/bootstrap.min.js"></script>

    <script type="text/javascript">  
        $(document).bind("dataload.exhibit", function() {
            $("input").addClass("form-control");
        });

        $(document).bind("scriptsLoaded.exhibit", function () {
            Exhibit.FunctionUtilities.registerSimpleMappingFunction("yearOf",
                function (d) {
                    return d.split('-')[0];
                },
                "number");
        });
    </script>

    <script type="text/javascript">

        function addYear(json) {
            var items = json.items;
            for (var i = 0; item = items[i]; i++) {
                items[i].year = Exhibit.DateTime.parseIso8601DateTime(item.publishedAt).getFullYear();
            }
            return json;
        }
    </script>


    <style>
        #main-content {
            background: white;
        }

        #title-panel {
            padding: 0.25in 0.5in;
        }

        #top-panels {
            padding: 0.5em 0.5in;
            border-top: 1px solid #BCB79E;
            border-bottom: 1px solid #BCB79E;
            background: #FBF4D3;
        }

        .exhibit-tileView-body {
            list-style: none;
        }

        .exhibit-collectionView-group-count {
            display: none;
        }

        table.nobelist {
            border: 1px solid #ddd;
            padding: 0.5em;
        }

        div.name {
            font-weight: bold;
            font-size: 120%;
        }

        .relationship {
            color: #888;
        }

        ddiv.video-thumbnail {
            float: left;
            width: 12vw;
            height: 13em;
            border: 1px solid #BCB79E;
            background: #F0FFF0;
            padding: 1em;
            margin: 0.5em;
            text-align: center;
        }

        div.video-timeline-lens {
            padding: 1em;
            text-align: center;
        }

        .card-content p {
            font-size: 12px;
        }

        .exhibit-thumbnailView-body{
            display: flex;
            flex-wrap: wrap;
            align-items: top;
            justify-content: center;
        }
        .video-thumbnail {
            width: 15vw;
            height: auto;
            border: 1px solid #BCB79E;
            background: #F0FFF0;
            padding: 1em;
            margin: 0.5em;
            text-align: center;
        }
        
        @media (max-width: 768px) {
            h1, .h1 {
                font-size: 24px
            }
            .video-thumbnail {
                width: 90vw;
                height: auto;
                border: 1px solid #BCB79E;
                background: #F0FFF0;
                padding: 1em;
                margin: 0.5em;
                text-align: center;
            }
            .card-content {
                font-size: 8px;
            }
            div.exhibit-facet-body{
                height: 5em;
            }
            div.exhibit-facet-value {
                font-size: 12px;
            }
          }

    </style>



<link rel="apple-touch-icon" sizes="180x180" href="static/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="static/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="static/favicon-16x16.png">
<link rel="manifest" href="site.webmanifest">
<link rel="mask-icon" href="static/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">



</head>

<body>
<header>
    <h1 class="text-center">MACHINELEARNEAR</h1>
    <hr>
</header>
    
        <div data-ex-role="exhibit-collection" data-ex-item-types="video"></div>
        <div class="container">
        <button class="visible-xs"  data-toggle="collapse" data-target="#controles"> <span class="glyphicon glyphicon-sort"></span></button>

        <div class="row collapse in" id="controles">
            <div class="col-md-4" data-ex-role="exhibit-facet" data-ex-facet-class="TextSearch" data-ex-facet-label="Buscar" 
            data-ex-expressions=".label, .playlistId.label"></div>
            <div class="col-md-4" data-ex-role="facet" data-ex-expression="yearOf(.publishedAt)" 
            data-ex-collapsible="true" data-ex-facet-label="Año de publicación"
            data-ex-sort-direction="reverse"></div>
            <div class="col-md-4" data-ex-role="facet" data-ex-expression=".playlistId" data-ex-facet-label="Playlists"
            data-ex-collapsible="true" data-ex-missing-label="Fuera de playlist" ></div>
        </div>
        

        <div data-ex-role="viewPanel" style="padding: 1em 0.5in;">

            <div data-ex-role="view" data-ex-view-class="Thumbnail" data-ex-showall="false" 
            data-ex-orders=".publishedAt, .label" data-ex-grouped="false"
                data-ex-paginate="true" data-ex-page-size="20" 
                data-ex-directions="descending"
                data-ex-show-controls="false" data-ex-possible-orders=".label ">

                <div data-ex-role="exhibit-lens" style="display: none;" class="video-thumbnail">

                    <div>
                        <a target="_blank" data-ex-href-content="concat('https://www.youtube.com/watch?&v=', .url)">
                            <img data-ex-src-content=".imagen" alt="Imagen" class="img-responsive">
                        </a>

                    </div>
                    <div class="card-content">
                        <a target="_blank" data-ex-href-content="concat('https://www.youtube.com/watch?&v=', .url)">
                        <p data-ex-content=".label"></p>
                        </a>
                    </div>


                </div>
            </div>
            <!--
            <div data-ex-role="view" 
                data-ex-view-class="Timeline" 
                data-ex-start=".publishedAt" >
                    <div data-ex-role="lens" class="video-timeline-lens" style="display: none;">
                            <img data-ex-src-content=".imagen"  alt="portrait"/>
                            <div><span data-ex-content=".label"></span></div>
                            
                    </div>

            </div>
            -->
            

        </div>
    

    </div>



    <!--  Scripts-->
    <div id="schema" style="display:none">
        {"types": {"video": {"pluralLabel": "V\u00eddeos"}, "playlist": {"pluralLabel": "Playlists"}}, "properties": {"imagen": {"valueType": "url"}, "playlistId": {"valueType": "item"}, "publishedAt": {"valueType": "date", "label": "Fecha de publicaci\u00f3n"}}}
    </div>
    <div id="data" style="display:none">
            {"items": [{"url": "dzscglZ_jzo", "label": "[#78] Tracking y segmentaci\u00f3n usando SAM (MetaAI) + demo en vivo! (Track-Anything)", "imagen": "https://i.ytimg.com/vi/dzscglZ_jzo/mqdefault.jpg", "playlistId": ["PLWhCQi97dstkO-8jzlVSzNZZAcuxxsXuH"], "position": 0, "type": "video", "id": "UExXaENRaTk3ZHN0a08tOGp6bFZTek5aWkFjdXh4c1h1SC41NkI0NEY2RDEwNTU3Q0M2", "publishedAt": "2023-04-27", "description": "(Titulo, descripcion, todo escrito por GPT-3.5) \"Descubre \"Track-Anything\", una herramienta de seguimiento y segmentaci\u00f3n de objetos de vanguardia. Con esta poderosa soluci\u00f3n basada en inteligencia artificial y aprendizaje autom\u00e1tico, podr\u00e1s detectar y rastrear objetos en im\u00e1genes y videos con precisi\u00f3n y eficiencia.\n\n\"Track-Anything\" utiliza t\u00e9cnicas avanzadas de visi\u00f3n por computadora y algoritmos de seguimiento para analizar el movimiento de objetos en tiempo real. Ya sea que necesites hacer el seguimiento de veh\u00edculos, personas u otros objetos, esta herramienta te brinda resultados precisos y confiables.\n\nCon una personalizaci\u00f3n completa, puedes ajustar los par\u00e1metros y configuraciones seg\u00fan tus necesidades espec\u00edficas, permiti\u00e9ndote adaptar \"Track-Anything\" a diferentes escenarios de seguimiento. Obtendr\u00e1s informaci\u00f3n detallada sobre la trayectoria y las caracter\u00edsticas de los objetos en movimiento a lo largo del tiempo.\n\nEn este video, te mostraremos c\u00f3mo implementar \"Track-Anything\" utilizando una notebook en Google Colab. Aprender\u00e1s los conceptos fundamentales de la segmentaci\u00f3n de objetos y c\u00f3mo aplicarlos de manera efectiva en tus proyectos de an\u00e1lisis visual.\n\nNo te pierdas la oportunidad de mejorar tus habilidades en seguimiento y segmentaci\u00f3n de objetos con \"Track-Anything\". \u00a1Suscr\u00edbete al canal y prep\u00e1rate para explorar esta fascinante herramienta en detalle!\n\n\u00a1Bienvenidos a este nuevo episodio donde juntos descubriremos las maravillas de \"Track-Anything\u201d!\u201d\n#gpt4 #segmentation #facebook   \n\nContenido\n00:00 - Track-Anything: Demo\n09:00 - Como funciona? Paper.\n14:30 - Conclusiones\n\nLinks mencionados durante el video:\n\ud83d\udcbb Colab: https://colab.research.google.com/gist/machinelearnear/8e8a6310f51b380d554219ca18afd287/track-anything-colab.ipynb \n\ud83d\udcbb Paper: https://arxiv.org/abs/2304.11968 \n\ud83d\udcbb Space: https://huggingface.co/spaces/watchtowerss/Track-Anything?duplicate=trueg \n\ud83d\udcbb Repo: https://github.com/gaomingqi/Track-Anything \n\ud83d\udcbb Segment-Anything: https://segment-anything.com/ \n\ud83d\udcbb \u201c[#73] Segment Anything Model (SAM): Nueva IA de Meta AI que revoluciona la segmentaci\u00f3n de im\u00e1genes\u201d: https://www.youtube.com/watch?v=vSZxcIC3Eys&ab_channel=machinelearnear \n\ud83d\udcbb XMem: Long-Term Video Object Segmentation: https://github.com/hkchengrex/XMem \n\ud83d\udcbb Towards An End-to-End Framework for Flow-Guided Video Inpainting: https://github.com/MCG-NKU/E2FGVI \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "vSZxcIC3Eys", "label": "[#73] Segment Anything Model (SAM): Nueva IA de Meta AI que revoluciona la segmentaci\u00f3n de im\u00e1genes", "imagen": "https://i.ytimg.com/vi/vSZxcIC3Eys/mqdefault.jpg", "playlistId": ["PLWhCQi97dstkO-8jzlVSzNZZAcuxxsXuH"], "position": 1, "type": "video", "id": "UExXaENRaTk3ZHN0a08tOGp6bFZTek5aWkFjdXh4c1h1SC4yODlGNEE0NkRGMEEzMEQy", "publishedAt": "2023-04-08", "description": "(Titulo, descripcion, todo escrito por GPT-4) \u201cEn este video hablaremos sobre la nueva herramienta de segmentaci\u00f3n de objetos en im\u00e1genes desarrollada por Meta AI. SAM es una red neuronal que ha sido entrenada con un conjunto de datos de 11 millones de im\u00e1genes y 1.1 mil millones de m\u00e1scaras. Con SAM, es posible \"recortar\" cualquier objeto en una imagen con un solo clic, sin necesidad de entrenamiento adicional. Hablaremos sobre el enfoque que Meta AI tom\u00f3 para abordar los desaf\u00edos de la segmentaci\u00f3n de im\u00e1genes y c\u00f3mo SAM ha demostrado tener una alta precisi\u00f3n en la segmentaci\u00f3n de objetos.\u201d\n#ai #imagesegmentation #meta \n\nContenido\n00:00 - Qu\u00e9 es \u201cSegment Anything\u201d de MetaAI?\n05:00 - Porque es importante y cuales son los usos?\n10:30 - Demo en vivo\n21:30 - Como se va integrar SAM con GPT4 y otras LLMs?\n\nLinks mencionados durante el video:\n\ud83d\udcbb Segment Anything Model (SAM): https://segment-anything.com/ \n\ud83d\udcbb SAM: Segment Anything Model from FAIR. Foundation model for image segmentation: https://twitter.com/ylecun/status/1643684379263369216?s=20 \n\ud83d\udcbb SAM Demo: https://segment-anything.com/demo \n\ud83d\udcbb SegGPT: Segmenting Everything In Context: https://twitter.com/_akhaliq/status/1644147931178496001\n\ud83d\udcbb Generative Novel View Synthesis with 3D-Aware Diffusion Models: https://twitter.com/_akhaliq/status/1643790003779059715?s=20  \n\ud83d\udcbb Latent Video Diffusion Models for High-Fidelity Long Video Generation: https://twitter.com/multimodalart/status/1643642114058735617 \n\ud83d\udcbb Robots that learn from videos of human activities and simulated interactions: https://ai.facebook.com/blog/robots-learning-video-simulation-artificial-visual-cortex-vc-1/ \n\ud83d\udcbb Prismer (NVIDIA): https://twitter.com/DrJimFan/status/1633179734803890177 \n\ud83d\udcbb MetaSeg: Packaged version of the Segment Anything repository: https://github.com/kadirnar/segment-anything-video\n\ud83d\udcbb SAM Community Demos: https://huggingface.co/spaces?search=segment-anything\n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "f_YomnlexV4", "label": "[#16] C\u00f3mo borrar el fondo a una imagen en segundos con AI (Unsupervised Salient Object Detection)", "imagen": "https://i.ytimg.com/vi/f_YomnlexV4/mqdefault.jpg", "playlistId": ["PLWhCQi97dstkO-8jzlVSzNZZAcuxxsXuH"], "position": 2, "type": "video", "id": "UExXaENRaTk3ZHN0a08tOGp6bFZTek5aWkFjdXh4c1h1SC4wMTcyMDhGQUE4NTIzM0Y5", "publishedAt": "2022-08-17", "description": "Hoy vamos a aprender a detectar el objeto mas importante/relevante en una foto y a hacer algo interesante con eso, como por ejemplo sacarle el fondo o todo lo que no es ese objeto. Para eso vamos a ver 2 papers que hacen mas o menos lo mismo y son el estado del arte en este momento. Uno para objetos conocidos y otros no. \n\nContenido\n00:00 - Qu\u00e9 es \u201cSalient Object Detection\u201d?\n03:14 - Paper & Demo: \u201cHighly Accurate Dichotomous Image Segmentation\u201d\n06:12 - Paper & Demo: \"Unsupervised Salient Object Detection with Spectral Cluster Voting\u201d\n09:39 - Meta AI DINO\n12:45 - Conclusiones\n\nLinks mencionados durante el video:\n\ud83d\udcbb PapersWithCode Saliency Detection: https://paperswithcode.com/task/saliency-detection \n\ud83d\udcbb Open HF spaces in Studio Lab: https://github.com/machinelearnear/open-hf-spaces-in-studiolab \n\ud83d\udcbb \u201cU^2-Net: Going Deeper with Nested U-Structure for Salient Object Detection\u201d: https://github.com/xuebinqin/U-2-Net \n\ud83d\udcbb Demo \u201cHighly Accurate Dichotomous Image Segmentation\u201d: https://huggingface.co/spaces/doevent/dis-background-removal \n\ud83d\udcdd Paper \u201cUnsupervised Salient Object Detection with Spectral Cluster Voting\u201d: https://arxiv.org/pdf/2203.12614.pdf \n\ud83e\udd17 Demo en Hugging Face: https://huggingface.co/spaces/noelshin/selfmask \n\ud83d\udcbb SSL Demos (Meta): https://ssl-demos.metademolab.com/home\n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#python #huggingface #salientfeatures #imagesegmentation"}, {"url": "HNCWOS8A_MA", "label": "[#14] Donut \ud83c\udf69 OCR-free Document Understanding Transformer", "imagen": "https://i.ytimg.com/vi/HNCWOS8A_MA/mqdefault.jpg", "playlistId": ["PLWhCQi97dstkO-8jzlVSzNZZAcuxxsXuH"], "position": 3, "type": "video", "id": "UExXaENRaTk3ZHN0a08tOGp6bFZTek5aWkFjdXh4c1h1SC41MjE1MkI0OTQ2QzJGNzNG", "publishedAt": "2022-08-13", "description": "Hoy vamos a ver un nuevo modelo para entender documentos (recibos, formularios, etc.) y extraer informaci\u00f3n de ellos sin necesitar de un paso previo con OCR. Hace muy pocos d\u00edas el c\u00f3digo y los modelos se hicieron open-source y ya se implement\u00f3 dentro de Hugging Face para hacer su uso mas sencillo. Todo esto es gratuito y, por ahora, el estado del arte.\n\nContenido\n00:00 - Qu\u00e9 es \u201cDocument Understanding\u201d?\n04:33 - Paper de \u201cDonut: OCR-free Document Understanding Transformer\u201d\n09:54 - Implementaci\u00f3n en Hugging Face\n11:48 - Notebooks y demo\n16:45 - Conclusiones\n\nAlgunos links mencionados durante el video:\n\ud83d\udcbb Layout Parser: https://layout-parser.github.io/ \n\ud83d\udcdd Paper \u201cOCR-free Document Understanding Transformer\u201d: https://arxiv.org/pdf/2111.15664.pdf \n\ud83d\udcbb Repo de GitHub: https://github.com/clovaai/donut \n\ud83e\udd17 Implementaci\u00f3n de Donut en Hugging Face: https://huggingface.co/docs/transformers/main/en/model_doc/donut \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#python #huggingface #donut #ocr"}, {"url": "dpC71lWTkGw", "label": "[#07] RealBasicVSR: Aument\u00e1 la calidad de tus videos con AI (Video Super Resolution + Repo)", "imagen": "https://i.ytimg.com/vi/dpC71lWTkGw/mqdefault.jpg", "playlistId": ["PLWhCQi97dstkO-8jzlVSzNZZAcuxxsXuH"], "position": 4, "type": "video", "id": "UExXaENRaTk3ZHN0a08tOGp6bFZTek5aWkFjdXh4c1h1SC4wOTA3OTZBNzVEMTUzOTMy", "publishedAt": "2022-03-19", "description": "En este video vamos a hacer un review a un paper (\u201cInvestigating Tradeoffs in Real-World Video Super-Resolution\u201d) que acaba de ser aceptado en CVPR 2022 que es, por ahora, el estado del arte para mejorar la calidad de un video. Aproveche de paso para conectarlo a YouTube y subi 2 ejemplos de los resultados, que son bastante buenos! Vamos a ver de que se trata!\n\nCapitulos\n00:00 - Intro\n05:18 - Que es \u201csuper resoluci\u00f3n\u201d?\n11:22 - Mini review del paper\n16:16 - Implementaci\u00f3n / Demo\n\nRepo paso a paso:\n\ud83d\udcbb  https://github.com/machinelearnear/video-super-resolution-youtube \n\nLinks mencionados durante el video:\n\ud83d\udcf9  El Amor y la Amistad, Seg\u00fan Borges (Mejorado con AI, RealBasicVSR): https://www.youtube.com/watch?v=AQ2TMU1eg1Q \n\ud83d\udcf9  \u201cY vol\u00f3.. y me hizo volar.. y yo vol\u00e9 de el\u201d (Mejorado con AI, RealBasicVSR): https://www.youtube.com/watch?v=wM3yPjqgP98 \n\ud83d\udcdd  Paper: https://arxiv.org/pdf/2111.12704.pdf \n\ud83d\udcbb  Repo oficial: https://github.com/ckkelvinchan/RealBasicVSR \n\ud83d\udcf9  How Super Resolution Works: https://www.youtube.com/watch?v=KULkSwLk62I&ab_channel=LeoIsikdogan\n\ud83d\udcf9  Enhance! Super Resolution Is Here!: https://www.youtube.com/watch?v=WCAF3PNEc_c&ab_channel=TwoMinutePapers\n\ud83d\udcbb  MMEditing: https://github.com/open-mmlab/mmediting\n\ud83d\udcbb  Structural Similarity Index: https://scikit-image.org/docs/dev/auto_examples/transform/plot_ssim.html#id4 \n\ud83c\udf10  NTIRE 2021 \"New Trends in Image Restoration and Enhancement\" Workshop: https://openaccess.thecvf.com/CVPR2021_workshops/NTIRE\n\ud83d\udcdd  VRT: A Video Restoration Transformer: https://arxiv.org/pdf/2108.10257.pdf\n\ud83c\udf10  SwinIR: Image Restoration Using Swin Transformer: https://huggingface.co/spaces/akhaliq/SwinIR\n\ud83d\udcdd  Perceptual Losses for Deep Image Restoration: https://towardsdatascience.com/perceptual-losses-for-image-restoration-dd3c9de4113 \n\ud83d\udcdd  Image Super-Resolution via Iterative Refinement: https://iterative-refinement.github.io/\n\ud83d\udcdd  Restormer: Efficient Transformer for High-Resolution Image Restoration: https://arxiv.org/abs/2111.09881 y https://github.com/swz30/Restormer \n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#superresolution #video #python #aws"}, {"url": "r-QGxuMMWfA", "label": "[#05] SAHI+DETIC: Detect\u00e1 Objetos a Gran Escala en Minutos", "imagen": "https://i.ytimg.com/vi/r-QGxuMMWfA/mqdefault.jpg", "playlistId": ["PLWhCQi97dstkO-8jzlVSzNZZAcuxxsXuH"], "position": 5, "type": "video", "id": "UExXaENRaTk3ZHN0a08tOGp6bFZTek5aWkFjdXh4c1h1SC4xMkVGQjNCMUM1N0RFNEUx", "publishedAt": "2022-01-29", "description": "En este video vamos a ver un ejemplo r\u00e1pido de como podemos combinar la libreria SAHI junto a DETIC para hacer detecci\u00f3n de objetos a gran escala en minutos. SAHI es una libreria que nos sirve para poder detectar objetos muy peque\u00f1os en im\u00e1genes de grandes dimensiones de una forma sencilla e integrada a los mejores frameworks de CV (Detectron2, mmdet, YOLO, etc.). Detic, que vimos en otro video de @machinelearnear, nos permite detectar mas de 20 mil clases de objetos sin fine-tuning. Veremos como funcionan ambas cosas y para que situaciones nos pueden ser muy \u00fatiles.\n\nCapitulos\n00:00 - Intro\n01:30 - Que es DETIC y SAHI?\n08:19 - Intersection Over Union (IoU)\n10:14 - Como funciona NMS/Soft-NMS? \n14:10 - Que es una \u201csliced prediction\u201d?\n15:20 - Demo\n\nLinks mencionados durante el video:\n\ud83d\udcbb  Quick-start en GitHub: https://github.com/machinelearnear/large-scale-object-detection-with-sahi-detectron2\n\ud83d\udcbb  Repo de SAHI: https://github.com/obss/sahi \n\ud83d\udcdd  Paper de Detic: https://arxiv.org/abs/2201.02605 \n\ud83d\udcbb  Repo Original Detic: https://github.com/facebookresearch/Detic\n\ud83c\udf10  Intersection Over Union (IoU): https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\n\ud83d\udcdd  Paper de Soft-NMS: https://arxiv.org/abs/1704.04503\n\ud83c\udf10  Torchvision.ops.nms: https://pytorch.org/vision/stable/_modules/torchvision/ops/boxes.html#nms\n\ud83d\udcbb  DE\u2af6TR: End-to-End Object Detection with Transformers: https://github.com/facebookresearch/detr\n\ud83d\udcbb  FiftyOne: https://voxel51.com/docs/fiftyone/\n\ud83c\udf10  OpenAI CLIP: https://openai.com/blog/clip/ \n\ud83c\udf10  Google Open Images Dataset: https://opensource.google/projects/open-images-dataset\n\ud83c\udf10  LVIS Dataset: https://www.lvisdataset.org/\n\ud83d\udcf9  CS231n: What is Image Segmentation? https://www.youtube.com/watch?v=nDPWywWRIRo\n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#objectdetection #sahi #python #aws"}, {"url": "Xw2icCN5ZpM", "label": "[#04] DETIC: Detectar +20K Clases de Objetos Sin Fine-tuning (+ Repo)", "imagen": "https://i.ytimg.com/vi/Xw2icCN5ZpM/mqdefault.jpg", "playlistId": ["PLWhCQi97dstkO-8jzlVSzNZZAcuxxsXuH"], "position": 6, "type": "video", "id": "UExXaENRaTk3ZHN0a08tOGp6bFZTek5aWkFjdXh4c1h1SC41MzJCQjBCNDIyRkJDN0VD", "publishedAt": "2022-01-14", "description": "En este video vamos a hacer un review a un paper que acaba de publicar Facebook donde expanden la cantidad de clases que un modelo de detecci\u00f3n de objetos puede encontrar de muy pocas a muchas gracias a que hacen un entrenamiento con las 21 mil clases que tiene ImageNet. Esto nunca se habia hecho antes. De esta forma puede generalizar muy bien a datasets nuevos sin necesidad de fine-tuning y, por si fuera poco, tambien lo combinan con CLIP para expandir la detecci\u00f3n a clases desconocidas durante el entrenamiento y terminan obteniendo una mejor performance en la detecci\u00f3n para datasets conocidos. \n\nCapitulos\n00:00 - Intro\n01:25 - Que es Detic?\n06:30 - Mini review del paper\n12:40 - Demo\n\nLinks mencionados durante el video:\n\ud83d\udcbb  Quickstart en GitHub: https://github.com/machinelearnear/detic-detecting-20k-classes-using-image-level-supervision\n\ud83d\udcdd  Paper: https://arxiv.org/abs/2201.02605 \n\ud83d\udcbb  Repo Original Detic: https://github.com/facebookresearch/Detic\n\ud83d\udcbb  Web Demo Detic: https://huggingface.co/spaces/akhaliq/Detic\n\ud83d\udcbb  DETR: End-to-End Object Detection with Transformers: https://github.com/facebookresearch/detr\n\ud83d\udcbb  FiftyOne: https://voxel51.com/docs/fiftyone/\n\ud83c\udf10  OpenAI CLIP: https://openai.com/blog/clip/ \n\ud83c\udf10  Google Open Images Dataset: https://opensource.google/projects/open-images-dataset\n\ud83c\udf10  LVIS Dataset: https://www.lvisdataset.org/\n\ud83d\udcf9  DETR: End-to-End Object Detection with Transformers (Paper Explained): https://www.youtube.com/watch?v=T35ba_VXkMY \n\ud83d\udcf9  CS231n: What is Image Segmentation? https://www.youtube.com/watch?v=nDPWywWRIRo\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#detic #objectdetection #python"}, {"url": "uf9bWRxtBgY", "label": "[#03] Como detectar un choripan con ML paso a paso (OpenImages/Mask-RCNN)", "imagen": "https://i.ytimg.com/vi/uf9bWRxtBgY/mqdefault.jpg", "playlistId": ["PLWhCQi97dstkO-8jzlVSzNZZAcuxxsXuH"], "position": 7, "type": "video", "id": "UExXaENRaTk3ZHN0a08tOGp6bFZTek5aWkFjdXh4c1h1SC5DQUNERDQ2NkIzRUQxNTY1", "publishedAt": "2022-01-07", "description": "En este video vamos a hacer un tutorial paso a paso sobre como entrenar tu propio modelo de segmentaci\u00f3n de im\u00e1genes en menos de 10 minutos. Vamos a estar usando la libreria de IceVision, un dataset open-source que se llama OpenImages, y el algoritmo Mask-RCNN. \n\nCapitulos\n00:00 - Intro\n01:16 - Que es la segmentaci\u00f3n de im\u00e1genes?\n04:22 - Que es el dataset de OpenImages?\n05:58 - Que es IceVision?\n08:45 - Preparar dataset y convertir a formato 'COCO'\n14:35 - Entrenamiento del modelo \n20:15 - Inferencia sobre una imagen de muestra\n\nLinks mencionados durante el video:\n\ud83d\udcbb  Tutorial en GitHub: https://github.com/machinelearnear/custom-segmentation-model-with-icevision-openimages\n\ud83d\udcbb  IceVision: https://github.com/airctic/icevision\n\ud83c\udf10  Google Open Images Dataset: https://opensource.google/projects/open-images-dataset\n\ud83c\udf10  LVIS Dataset: https://www.lvisdataset.org/\n\ud83c\udf10  COCO Dataset Format Walkthrough: https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch/#coco-dataset-format\n\ud83d\udcf9  COCO Dataset Format Walkthrough: https://www.youtube.com/watch?v=h6s61a_pqfM\n\ud83d\udcf9  CS231n: Convolutional Neural Networks for Visual Recognition - Image Segmentation: https://www.youtube.com/watch?v=nDPWywWRIRo\n\u270f\ufe0f  Train a Choripan Classifier with Fastai and Google Colab (2018): https://medium.com/@nicolas.metallo/train-a-choripan-classifier-with-fast-ai-v1-in-google-colab-6e438817656a\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#openimages #choripan #detectar"}, {"url": "T4-THR6MgrA", "label": "[#62] 24 horas en open-source: Meta\u2019s LLaMA & Google\u2019s Flan-UL2 (demo en vivo y an\u00e1lisis)", "imagen": "https://i.ytimg.com/vi/T4-THR6MgrA/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc"], "position": 0, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy41NkI0NEY2RDEwNTU3Q0M2", "publishedAt": "2023-03-04", "description": "#ai #meta #languagemodel \n(ChatGPT-generated) En este video presentamos dos modelos de lenguaje gigantes de \u00faltima generaci\u00f3n: LLaMA desarrollado por Facebook AI y Flan UL2 desarrollado por Google. Ambos modelos tienen miles de millones de par\u00e1metros y representan una haza\u00f1a impresionante de la investigaci\u00f3n en inteligencia artificial.\n\nLa importancia de estos modelos radica en su capacidad para procesar y comprender el lenguaje humano de manera m\u00e1s sofisticada que nunca antes. Esto tiene implicaciones significativas para aplicaciones de la vida real, desde la traducci\u00f3n autom\u00e1tica hasta la generaci\u00f3n de di\u00e1logos naturales en chatbots.\n\nAdem\u00e1s, es crucial que estos modelos sean de c\u00f3digo abierto y accesibles para la comunidad. De esta manera, m\u00e1s personas pueden trabajar con ellos, mejorarlos y aplicarlos a problemas del mundo real. Modelos como Flan UL2 se lanzaron recientemente como de c\u00f3digo abierto, lo que significa que est\u00e1n disponibles para cualquier persona interesada en explorar sus capacidades y aplicaciones. La disponibilidad de modelos de lenguaje gigantes de c\u00f3digo abierto democratiza el acceso a la inteligencia artificial y abre la puerta a innovaciones a\u00fan m\u00e1s significativas en el futuro.\n\n\ud83d\udcdd \u00cdndice de contenido:\n00:00 - Meta publica LLaMA. Qu\u00e9 es y c\u00f3mo funciona? \n12:00 - LLaMA demo\n20:45 - FLAN-UL2 de Google. Como funciona?\n26:30 - Demo comparando UL2 vs T5-XXL\n\nLinks mencionados durante el video:\n\ud83d\udcbb Introducing LLaMA: A foundational, 65-billion-parameter large language model: https://ai.facebook.com/blog/large-language-model-llama-meta-ai/ \n\ud83d\udcbb Takeaways from reading the \"LLaMa: Open and Efficient Foundation Language Models\u201d: https://twitter.com/rasbt/status/1629496763148017665 \n\ud83d\udcbb Test out LLaMA models @cedrickchee: https://github.com/cedrickchee/llama/blob/main/notebooks/vi_LLaMA_alpha.ipynb \n\ud83d\udcbb Notebook by @kennethgoodman: https://github.com/kennethgoodman/llama/blob/main/llama.ipynb \n\ud83d\udcbb A New Open Source Flan 20B with UL2: https://www.yitay.net/blog/flan-ul2-20b \n\ud83d\udcbb Flan UL2 vs Flan T5 XXL: https://huggingface.co/spaces/ybelkada/i-like-flan-ul2 \n\ud83d\udcbb Google Flan-UL2: https://huggingface.co/google/flan-ul2\n\ud83d\udcbb UL2 20B: An Open Source Unified Language Learner: https://ai.googleblog.com/2022/10/ul2-20b-open-source-unified-language.html \n\ud83d\udcbb Flan-UL2  standard chain-of-thought prompt, as well as question-answering over specific documents by @hwchase17: https://colab.research.google.com/drive/19Zr9PBgPX3Q8B8yHXYFatCodnNdqP8fg#scrollTo=E2zgs5TGlgJM \n\ud83d\udcbb [#32] \ud83e\uddc9\ud83e\udd16 +1,000 suscriptores! SD 1.5+VAE | Flan-T5 | Text-to-Music | Transformers x DeepMind: https://www.youtube.com/watch?v=bNE7UZI4cI4&ab_channel=machinelearnear \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "ZRTUzw04DDY", "label": "[#64] \ud83d\udd25 OpenAI GPT-4: Demo en vivo, c\u00f3mo conseguir acceso gratis y an\u00e1lisis por un Data Scientist", "imagen": "https://i.ytimg.com/vi/ZRTUzw04DDY/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc"], "position": 1, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy4yODlGNEE0NkRGMEEzMEQy", "publishedAt": "2023-03-14", "description": "(Escrito x GPT-4) \"Boludo, no te pierdas este video donde te muestro en vivo la llegada del GPT-4 de OpenAI, te ense\u00f1o c\u00f3mo obtener acceso gratuito a la API y, como data scientist, analizo en profundidad sus caracter\u00edsticas y potencial. \u00a1Estamos a punto de entrar en una nueva era!\"\n#gpt4 #openai #chatgpt \n\nContenido\n00:00 - Anuncio de GPT-4\n07:45 - Explicamos que tan bien funciona\n27:50 - Analisis del Developer Livestream\n35:00 - Demo en ChatGPT Plus\n40:45 - Como tener acceso gratuito a GPT-4\n\nLinks mencionados durante el video:\n\ud83d\udcbb GPT-4 is OpenAI\u2019s most advanced system, producing safer and more useful responses: https://openai.com/product/gpt-4 \n\ud83d\udcbb GPT-4 Developer Livestream: https://www.youtube.com/watch?v=outcGtbnMuQ&ab_channel=OpenAI \n\ud83d\udcbb GPT-4 Announcement: https://openai.com/research/gpt-4 \n\ud83d\udcbb GPT-4 Technical Report: https://cdn.openai.com/papers/gpt-4.pdf \n\ud83d\udcbb GPT-4 System Card: https://cdn.openai.com/papers/gpt-4-system-card.pdf \n\ud83d\udcbb openai/evals: https://github.com/openai/evals \n\ud83d\udcbb Poe App from Quora: https://poe.com/gpt-4 \n\ud83d\udcbb Confirmed: the new Bing runs on OpenAI\u2019s GPT-4: https://blogs.bing.com/search/march_2023/Confirmed-the-new-Bing-runs-on-OpenAI%E2%80%99s-GPT-4 \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "Ds9RSNE0-58", "label": "[#65] \u00a1GPT-4 en la oficina! Descubr\u00ed Microsoft 365 Copilot, \u00a1te va a volar la peluca! \ud83d\udca5", "imagen": "https://i.ytimg.com/vi/Ds9RSNE0-58/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc"], "position": 2, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy4wMTcyMDhGQUE4NTIzM0Y5", "publishedAt": "2023-03-17", "description": "(Titulo & Descripcion x GPT-4) \"\u00a1Par\u00e1 todo, che! Microsoft se puso las pilas y ahora GPT-4 forma parte de su suite de Office. En este video, te muestro las funciones m\u00e1s copadas y c\u00f3mo esta inteligencia artificial te va a hacer la vida m\u00e1s f\u00e1cil en el laburo. \u00a1Dale play, boludo!\"\n#microsoft #copilot #gpt4 \n\nContenido\n00:00 - Esta semana en AI: Alpaca, GPT4, Claude, PaLM\n12:40 - Qu\u00e9 es Microsoft 365 Copilot?\n31:10 - Conclusi\u00f3n y mi opini\u00f3n sobre el futuro del trabajo\n\nLinks mencionados durante el video:\n\ud83d\udcbb The Future of Work With AI - Microsoft March 2023 Event: https://www.youtube.com/watch?v=Bf-dbS9CcRU \n\ud83d\udcbb Introducing Microsoft 365 Copilot: https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/ \n\ud83d\udcbb Introducing Microsoft 365 Copilot | Your Copilot for Work: https://www.youtube.com/watch?v=S7xTBa93TX8\n\ud83d\udcbb A new era for AI and Google Workspace: https://www.youtube.com/watch?v=6DaJVZBXETE \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "tUkJWNGnqJ4", "label": "[#66] OpenPlayground: La plataforma para probar -GRATIS- GPT-4 y otros LLMs de gigantes tecnol\u00f3gicos", "imagen": "https://i.ytimg.com/vi/tUkJWNGnqJ4/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc"], "position": 3, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy41MjE1MkI0OTQ2QzJGNzNG", "publishedAt": "2023-03-19", "description": "(Todo inventado x GPT-4) \u201c\u00bfQuer\u00e9s probar GPT-4 de OpenAI y otros LLMs sin gastar un mango? \u00a1Te tengo la soluci\u00f3n! En este video, te presento OpenPlayground, una plataforma colaborativa que nos permite experimentar con modelos de lenguaje de empresas como Meta, Anthropic, Google, HuggingFace y OpenAI. Vamos a ver c\u00f3mo funciona esta herramienta, c\u00f3mo aprovecharla al m\u00e1ximo, y c\u00f3mo nos ayuda a entender las diferencias entre los distintos LLMs disponibles en el mercado. \u00a1Preparate para sumergirte en este mundo fascinante!\u201d\n#gpt4 #chatgpt #anthropic\n\nContenido\n00:00 - Introducci\u00f3n\n05:20 - Prompt Engineering Guide\n14:00 - Comparamos GPT4 vs Anthropic vs Hugging Face\n\nLinks mencionados durante el video:\n\ud83d\udcbb https://nat.dev/compare \n\ud83d\udcbb OpenPlayground: https://github.com/groundedai/openplayground \n\ud83d\udcbb Prompt Engineering Guide: https://www.promptingguide.ai/ \n\ud83d\udcbb Poe GPT4: https://poe.com/gpt-4\n\ud83d\udcbb Microsoft Bing: https://www.bing.com/new \nI'm\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "B0ah_V72cXo", "label": "[#68] La revoluci\u00f3n detr\u00e1s de Alpaca/LLaMA y c\u00f3mo competir con ChatGPT en tu navegador \ud83e\udd99", "imagen": "https://i.ytimg.com/vi/B0ah_V72cXo/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc"], "position": 4, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy4wOTA3OTZBNzVEMTUzOTMy", "publishedAt": "2023-03-24", "description": "(Titulo, descripcion, todo escrito por GPT-4) \u201cDescubr\u00ed las \u00faltimas innovaciones en modelos de lenguaje, como el LLaMA de Meta y el Alpaca de Stanford, y c\u00f3mo est\u00e1n cambiando el panorama de la inteligencia artificial, \u00a1te vas a sorprender!\"\n#chatgpt #stanford  #alpaca \n\nContenido\n00:00 - Que es Stanford Alpaca?\n13:40 - El dataset self-instruct que usaron\n20:15 - Porque es Alpaca importante?\n24:00 - LLaMA en C/C++, como es posible?\n27:30 - Entrenar LLaMA por $85,000?\n32:15 - Cabrita, the \ud83c\udde7\ud83c\uddf7 LLaMA\n\nLinks mencionados durante el video:\n\ud83d\udcbb [#62] 24 horas en open-source: Meta\u2019s LLaMA & Google\u2019s Flan-UL2 (demo en vivo y an\u00e1lisis): https://www.youtube.com/watch?v=T4-THR6MgrA&t=610s&ab_channel=machinelearnear \n\ud83d\udcbb Takeaways from reading the \"LLaMa: Open and Efficient Foundation Language Models\u201d: https://twitter.com/rasbt/status/1629496763148017665 \n\ud83d\udcbb Alpaca: A Strong, Replicable Instruction-Following Model: https://crfm.stanford.edu/2023/03/13/alpaca.html\n\ud83d\udcbb LLaMA: Open and Efficient Foundation Language Models: https://arxiv.org/pdf/2302.13971.pdf \n\ud83d\udcbb Fine-tuning InstructLLaMA on consumer hardware: https://github.com/tloen/alpaca-lora \n\ud83d\udcbb How is LLaMa.cpp possible? https://finbarrtimbers.substack.com/p/how-is-llamacpp-possible \n\ud83d\udcbb https://twitter.com/ESYudkowsky/status/1635577836525469697 \ud83d\udcbb https://twitter.com/ESYudkowsky/status/1635667349792780288 \n\ud83d\udcbb SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions: https://arxiv.org/pdf/2212.10560.pdf\n\ud83d\udcbb Could you train a ChatGPT-beating model for $85,000 and run it in a browser? https://simonwillison.net/2023/Mar/17/beat-chatgpt-in-a-browser/ \n\ud83d\udcbb Stanford Alpaca, and the acceleration of on-device large language model development: https://simonwillison.net/2023/Mar/13/alpaca/  \n\ud83d\udcbb Large language models are having their Stable Diffusion moment: https://simonwillison.net/2023/Mar/11/llama/\n\ud83d\udcbb Cabrita, translating the Alpaca dataset to Portuguese, and running LORA training, achieved ChatGPT-like behavior on that language: https://twitter.com/piesposi_to/status/1636780485597708290 \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "HnH7XATErZg", "label": "[#70] Tengo acceso a Google Bard | Demo y primeras impresiones \ud83e\udd16", "imagen": "https://i.ytimg.com/vi/HnH7XATErZg/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc"], "position": 5, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy4xMkVGQjNCMUM1N0RFNEUx", "publishedAt": "2023-03-30", "description": "(Titulo, descripcion, todo escrito por GPT-4) \u201cHoy vamos a explorar juntos Google Bard, el chatbot creado por Google. Veremos sus caracter\u00edsticas, c\u00f3mo interactuar con \u00e9l y lo pondremos a prueba con preguntas variadas. \u00a1Acomp\u00e1\u00f1enme en esta aventura, che!\u201d\n#gpt4 #google #bard \n\nContenido\n00:00 - Introducci\u00f3n a Google Bard\n05:30 - Pros y contras de Bard\n07:45 - Demo de las funciones de Google Bard\n16:00 - Nick Bostrom: Superintelligence\n20:25 - Sparks of AGI: Demo GPT4 vs Bard\n35:30 - Conclusiones y cierre\n\nLinks mencionados durante el video:\n\ud83d\udcbb Google Bard: https://bard.google.com/ \n\ud83d\udcbb [#56] Google anuncia BARD su respuesta a #ChatGPT: https://www.youtube.com/watch?v=9ZDzk9HGqbY&ab_channel=machinelearnear \n\ud83d\udcbb Google BARD evaluation: https://twitter.com/gerardsans/status/1638256566586470413/photo/1 \n\ud83d\udcbb The Google engineer who thinks the company\u2019s AI has come to life: https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/ \n\ud83d\udcbb Alphabet\u2019s Google and DeepMind Pause Grudges, Join Forces to Chase OpenAI: https://twitter.com/ProductHunt/status/1641349574815289345 \n\ud83d\udcbb Sparks of Artificial General Intelligence: Early experiments with GPT-4: https://arxiv.org/pdf/2303.12712.pdf \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "rkqCPlkTDHs", "label": "[#71] \u00a1Open Source al rescate! \ud83d\ude80 GPT4All, Dolly, Cerebras-GPT, RWKV, OpenFlamingo, Vicuna, y otros", "imagen": "https://i.ytimg.com/vi/rkqCPlkTDHs/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc"], "position": 6, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy41MzJCQjBCNDIyRkJDN0VD", "publishedAt": "2023-04-01", "description": "(Titulo, descripcion, todo escrito por GPT-4) \u201cEn este video, repasamos los proyectos open source m\u00e1s destacados que compiten con GPT-4, como Cerebras-GPT, Raven RWKV y m\u00e1s. \u00bfC\u00f3mo cambiar\u00e1 el panorama del aprendizaje autom\u00e1tico con estas propuestas? Averigualo ac\u00e1, amigo.\u201d\n#gpt4 #chatgpt #gpt4all \n\nContenido\n00:00 - Stanford HELM y listado LLM\n02:00 - Dolly y Bertin-6J-Alpaca\n08:00 - GPT4All\n12:00 - Cerebras-GPT\n16:00 - RWKV & Chat-RWKV\n22:00 - OpenFlamingo, BloombergGPT\n30:00 - Vicuna\n\nLinks mencionados durante el video:\n\ud83d\udcbb Databricks\u2019 Dolly: https://github.com/databrickslabs/dolly & https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html \n\ud83d\udcbb GPT4All: https://github.com/nomic-ai/gpt4all & https://twitter.com/andriy_mulyar/status/1640836003194630144 \n\ud83d\udcbb Cerebras-GPT: https://huggingface.co/cerebras & https://twitter.com/CerebrasSystems/status/1640725880711569408 \n\ud83d\udcbb Raven RWKV (RWKV finetuned on alpaca and `codealpaca`): https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B & https://github.com/BlinkDL/RWKV-LM \n\ud83d\udcbb ChatRWKV is like ChatGPT but powered by RWKV: https://github.com/BlinkDL/ChatRWKV \n\ud83d\udcbb OpenFlamingo: https://laion.ai/blog/open-flamingo/ & https://twitter.com/anas_awadalla/status/1640766789977251840 \n\ud83d\udcbb ColossalChat: An Open-Source Solution for Cloning ChatGPT With a Complete RLHF Pipeline: https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b & https://github.com/hpcaitech/ColossalAI & https://twitter.com/omarsar0/status/1641070078660603904 \n\ud83d\udcbb GeoV-9b, a 9 billion parameter causal language model: https://twitter.com/labmlai/status/1641357802009395201 \n\ud83d\udcbb Release of 4 Nucleotide Transformer models, the new SOTA genomics #LLMs \ud83e\uddec: https://twitter.com/thomas_pierrot/status/1641147069288022035 \n\ud83d\udcbb BloombergGPT: A Large Language Model for Finance: https://twitter.com/TechAtBloomberg/status/1641772329658114053 \n\ud83d\udcbb BERTIN GPT-J-6B Alpaca (Spanish): https://huggingface.co/bertin-project/bertin-gpt-j-6B-alpaca \n\ud83d\udcbb Vicuna, an open-source chatbot impressing GPT-4: https://twitter.com/lmsysorg/status/1641529839038640128 \n\ud83d\udcbb (Stanford) List of current LLMs & licenses: https://crfm.stanford.edu/helm/latest/?models=1 \n\ud83d\udcbb Repo: osanseviero/ml_timeline: https://github.com/osanseviero/ml_timeline \n\ud83d\udcbb Repo: awesome-decentralized-llm: https://github.com/imaurer/awesome-decentralized-llm \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "XS43BdiXbf4", "label": "[#72] GPT4 plugins, Hugging-GPT, LangChain, ReAct: El futuro de los LLMs (Software 3.0?)", "imagen": "https://i.ytimg.com/vi/XS43BdiXbf4/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc"], "position": 7, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy5DQUNERDQ2NkIzRUQxNTY1", "publishedAt": "2023-04-02", "description": "(Titulo, descripcion, todo escrito por GPT-4) \u201cEn este video, vamos a adentrarnos en las \u00faltimas y m\u00e1s emocionantes innovaciones en modelos de lenguaje que est\u00e1n cambiando el campo de la inteligencia artificial. Vamos a explorar c\u00f3mo plataformas como TaskMatrix.AI permiten a los usuarios completar tareas al conectar modelos base con millones de APIs. Tambi\u00e9n vamos a hablar de LangChain, un framework que permite a los usuarios crear y entrenar modelos de lenguaje que son modulares y adaptados a tareas espec\u00edficas. Adem\u00e1s, vamos a cubrir HuggingGPT, un framework que hace que sea f\u00e1cil utilizar modelos de lenguaje grandes para tareas de inteligencia artificial con una biblioteca de modelos pre-entrenados, APIs y una comunidad de desarrolladores. Tambi\u00e9n vamos a hablar de Toolformer, que utiliza aprendizaje por refuerzo para ense\u00f1ar a los modelos de lenguaje c\u00f3mo utilizar herramientas correctamente. Finalmente, vamos a examinar los modelos de lenguaje aumentados, que se enriquecen con informaci\u00f3n adicional, como grafos de conocimiento o c\u00f3digo, para realizar tareas que antes eran imposibles.\u201d\n#gpt4 #hugginggpt #chatgpt  \n\nContenido\n00:00 - ChatGPT Plugins\n10:00 - TaskMatrix.AI: conectando modelos base con millones de APIs\n22:00 - LangChain: modelos de lenguaje modulares y adaptados a tareas espec\u00edficas\n25:30 - Toolformer: ense\u00f1ando a los modelos de lenguaje c\u00f3mo utilizar herramientas correctamente\n31:00 - HuggingGPT: haciendo f\u00e1cil utilizar modelos de lenguaje grandes para tareas de IA\n34:30 - Modelos de lenguaje aumentados: informaci\u00f3n adicional para un rendimiento mejorado\n\nLinks mencionados durante el video:\n\ud83d\udcbb TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs: https://arxiv.org/abs/2303.16434# & https://github.com/microsoft/visual-chatgpt/tree/main/TaskMatrix.AI \n\ud83d\udcbb LangChain: https://python.langchain.com/en/latest/index.html \n\ud83d\udcbb Toolformer: Language Models Can Teach Themselves to Use Tools: https://arxiv.org/abs/2302.04761 \n\ud83d\udcbb ChatGPT plugins: https://openai.com/blog/chatgpt-plugins \n\ud83d\udcbb HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace: https://github.com/microsoft/JARVIS & https://arxiv.org/abs/2303.17580 \n\ud83d\udcbb MRKL Systems (Modular Reasoning, Knowledge and Language, pronounced \"miracle\"): https://learnprompting.org/docs/advanced_applications/mrkl \n\ud83d\udcbb ReAct = Synergize [Rea]soning and [Act]ing in LM: https://twitter.com/ShunyuYao12/status/1579475607402217472 & https://arxiv.org/abs/2210.03629 \n\ud83d\udcbb Augmented Language Models: a Survey: https://arxiv.org/abs/2302.07842 \n\ud83d\udcbb ACT-1: Transformer for Actions: https://www.adept.ai/blog/act-1 \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "97tB_3NkXkg", "label": "[#74] AutoGPT, JARVIS (+ Demo), BabyAGI: \u00bfEstamos cerca de crear agentes realmente aut\u00f3nomos?", "imagen": "https://i.ytimg.com/vi/97tB_3NkXkg/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc"], "position": 8, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy45NDk1REZENzhEMzU5MDQz", "publishedAt": "2023-04-09", "description": "(Titulo, descripcion, todo escrito por GPT-4) \"Sumate a nuestra charla sobre Reflexion y Auto-GPT, dos proyectos innovadores que buscan desarrollar modelos de lenguaje a gran escala con capacidad de auto-reflexi\u00f3n y autonom\u00eda. Analizaremos c\u00f3mo estos avances podr\u00edan cambiar el paradigma de la inteligencia artificial y qu\u00e9 desaf\u00edos se presentan en el camino hacia agentes verdaderamente conscientes. \u00a1Te esperamos, pibe!\"\n#gpt4 #microsoft  #autogpt \n\nContenido\n00:00 - Que es AGI? Estamos yendo ahi o no?\n06:30 - JARVIS Demo & Semantic Kernel\n13:40 - AutoGPT\n22:00 - BabyAGI & CAMEL\n29:00 - GPT4+Reflection\n31:45 - Como podemos evaluar LLMs con lo que sabemos de psicologia \n\nLinks mencionados durante el video:\n\ud83d\udcbb The AI Revolution: The Road to Superintelligence: https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html \n\ud83d\udcbb CAMEL: Communicative Agents for \u201cMind\u201d Exploration of Large Scale Language Model Society: https://www.camel-ai.org/ & http://agents.camel-ai.org/ & https://github.com/lightaime/camel \n\ud83d\udcbb Microsoft JARVIS: https://github.com/microsoft/JARVIS \n\ud83d\udcbb HuggingGPT: https://arxiv.org/pdf/2303.17580.pdf & https://huggingface.co/spaces/microsoft/HuggingGPT \n\ud83d\udcbb Reflexion: an autonomous agent with dynamic memory and self-reflection: https://nanothoughts.substack.com/p/reflecting-on-reflexion & https://arxiv.org/pdf/2303.11366.pdf \n\ud83d\udcbb Auto-GPT: An Autonomous GPT-4 Experiment: https://github.com/Torantulino/Auto-GPT \n\ud83d\udcbb babyagi: https://github.com/yoheinakajima/babyagi & https://twitter.com/hwchase17/status/1644068047152619520 & https://twitter.com/yoheinakajima/status/1642881722495954945 \n\ud83d\udcbb Semantic Kernel: https://github.com/microsoft/semantic-kernel \n\ud83d\udcbb To evaluate LLMs effectively, we need some principles from experimental psychology: https://twitter.com/mcxfrank/status/1643296168276033538\n\ud83d\udcbb AVOIDING AGI APOCALYPSE - CONNOR LEAHY: https://www.youtube.com/watch?v=T8tHmQiYzVA&t=7369s&ab_channel=MachineLearningStreetTalk \n\ud83d\udcbb Yann LeCun and Andrew Ng: Why the 6-month AI Pause is a Bad Idea: https://www.youtube.com/watch?v=BY9KV8uCtj4&ab_channel=DeepLearningAI \n\ud83d\udcbb Eliezer Yudkowsky: Dangers of AI and the End of Human Civilization | Lex Fridman Podcast: https://www.youtube.com/watch?v=AaTRHFaaPG8&ab_channel=LexFridman\n\ud83d\udcbb Sam Altman: OpenAI CEO on GPT-4, ChatGPT, and the Future of AI | Lex Fridman Podcast: https://www.youtube.com/watch?v=L_Guz73e6fw&ab_channel=LexFridman \n\ud83d\udcbb Ilya Sutskever (OpenAI Chief Scientist) - Building AGI, Alignment, Spies, Microsoft, & Enlightenment: https://www.youtube.com/watch?v=Yf1o0TQzry8&t=372s&ab_channel=DwarkeshPatel \n\ud83d\udcbb Full interview: \"Godfather of artificial intelligence\" talks impact and potential of AI: https://www.youtube.com/watch?v=qpoRO378qRY&t=954s&ab_channel=CBSMornings \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "gvmGHhNCtPY", "label": "[#75] Dolly 2.0, el primer instruction-tuned LLM realmente open-source! (Demo)", "imagen": "https://i.ytimg.com/vi/gvmGHhNCtPY/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc"], "position": 9, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy5GNjNDRDREMDQxOThCMDQ2", "publishedAt": "2023-04-14", "description": "(Titulo, descripcion, todo escrito por GPT-4) \"\u00a1Hola, amigos! Bienvenidos a nuestro \u00faltimo video, donde vamos a explorar Dolly 2.0, un modelo de lenguaje de \u00faltima generaci\u00f3n creado por Databricks. Dolly 2.0 es un modelo de 12 mil millones de par\u00e1metros y \u00a1les va a encantar, che! En este video, vamos a descubrir c\u00f3mo se cre\u00f3 este asombroso modelo de lenguaje utilizando un enfoque colaborativo con m\u00e1s de 5,000 empleados de Databricks. \u00bfNo es incre\u00edble?\n\nAdem\u00e1s, aprenderemos sobre las siete tareas espec\u00edficas que se utilizaron para crear el conjunto de datos de entrenamiento, desde preguntas y respuestas abiertas y cerradas hasta escritura creativa y res\u00famenes de informaci\u00f3n de Wikipedia. Y lo mejor de todo, \u00a1es completamente de c\u00f3digo abierto y apto para uso comercial!\n\nAs\u00ed que preparate para sumergirte en el emocionante mundo de los modelos de lenguaje y descubrir c\u00f3mo Dolly 2.0 est\u00e1 cambiando la forma en que interactuamos con la inteligencia artificial. \u00a1Dale, no te olvides de suscribirte y compartir este video con tus amigos! \u00a1Nos vemos en el video, che!\"\n#gpt4 #dolly #chatgpt  \n\nContenido\n00:00 - Free Dolly: Open Instruction-Tuned LLM\n10:30 - databricks-dolly-15k dataset\n16:30 - Demo & Gradio\n\nLinks mencionados durante el video:\n\ud83d\udcbb Model weights: https://huggingface.co/databricks/dolly-v2-12b \n\ud83d\udcbb Model weights (6.9B): https://huggingface.co/databricks/dolly-v2-6-9b \n\ud83d\udcbb Dataset: https://github.com/databrickslabs/dolly/tree/master/data \n\ud83d\udcbb Blog: https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm \n\ud83d\udcbb Announcement: https://twitter.com/abacaj/status/1646206018995601415 \n\ud83d\udc15DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales\ud83d\udc15: https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat \n\ud83d\udcbb Colab Notebook that you can run Dolly 2.0's pythia-2.8b model / 16-bit with the free Colab version. https://colab.research.google.com/drive/1A8Prplbjr16hy9eGfWd3-r34FOuccB2c?usp=sharing \n\ud83d\udcbb Dolly 2.0 Series for fine-tuning, applying to use cases, and deploying: https://github.com/kw2828/Dolly-2.0-Series \n\ud83d\udcbb Text generation web UI: https://github.com/oobabooga/text-generation-webui \n\ud83d\udcbb camenduru/text-generation-webui-colab: https://github.com/camenduru/text-generation-webui-colab \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "Tg-xunNkg3o", "label": "[#76] Open-source contra-ataca: OpenAssistant, StableChat, RedPajama, OpenChatKit", "imagen": "https://i.ytimg.com/vi/Tg-xunNkg3o/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc"], "position": 10, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy40NzZCMERDMjVEN0RFRThB", "publishedAt": "2023-04-19", "description": "(Titulo, descripcion, todo escrito por GPT-4) \u201c\u00danete a nosotros en este video para conocer OpenChatKit, OpenAssistant y GPT4All-J, tres herramientas de c\u00f3digo abierto dise\u00f1adas para facilitar la creaci\u00f3n de chatbots y plataformas de asistentes virtuales. Descubre c\u00f3mo estas innovaciones est\u00e1n democratizando el desarrollo de chatbots y expandiendo las capacidades de la IA en la interacci\u00f3n humano-m\u00e1quina\u201d. \n#gpt4 #openassistant #chatgpt  \n\nContenido\n00:00 - Datasets: RedPajama, OpenAssistant\n14:30 - Modelos: StableChat, OpenChatKit, MiniGPT-4, DinoV2, GPT4All-J \n\nLinks mencionados durante el video:\n\ud83d\udcbb Together.xyz: https://www.together.xyz/bloglist \n\ud83d\udcbb RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset: https://github.com/togethercomputer/RedPajama-Data  \n\ud83d\udcbb databricks-dolly-15k is an open source dataset of instruction-following records: https://github.com/databrickslabs/dolly/tree/master/data  \n\ud83d\udcbb Together/Sebastian Raschka: https://twitter.com/rasbt/status/1648306007506931712  \n\ud83d\udcbb \u201cAnnouncing RedPajama\u201d: https://twitter.com/togethercompute/status/1647917989264519174  \n\ud83d\udcbb What\u2019s in the RedPajama-Data-1T LLM training set: https://simonwillison.net/2023/Apr/17/redpajama-data/ \n\ud83d\udcbb \u201cnon-exhaustive list of the reasons the @togethercompute RedPajama LLaMA dataset reproduction saves the rest of us a lot of work\u201d: https://twitter.com/vagabondjack/status/1648052658073403392 \n\ud83d\udcbb \u201cBuilding fully-open LLaMAs as a part of our INCITE project\u201d: https://twitter.com/irinarish/status/1647975703021867009 \n\ud83d\udcbb OpenChatKit: https://github.com/togethercomputer/OpenChatKit \n\ud83d\udcbb DINOv2 Blog: https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/ \n\ud83d\udcbb DINOv2 Paper: https://arxiv.org/abs/2304.07193 \n\ud83d\udcbb DINOv2 Demo: https://dinov2.metademolab.com \n\ud83d\udcbb DINOv2 Repo: https://github.com/facebookresearch/dinov2\n\ud83d\udcbb DINOv2 Overview: https://twitter.com/omarsar0/status/1648086959250604034  \n\ud83d\udcbb MiniGPT-4: https://minigpt-4.github.io/ \n\ud83d\udcbb MiniGPT-4 Repo: https://github.com/Vision-CAIR/MiniGPT-4 \n\ud83d\udcbb Visual Instruction Tuning: https://twitter.com/altryne/status/1648151774455533569 \n\ud83d\udd25Visual Instruction Tuning with GPT-4: https://twitter.com/ChunyuanLi/status/1648222285889953793 \n\ud83d\udcbb Auto-evaluate LLM Q+A chains: https://twitter.com/RLanceMartin/status/1647645549875859456  \n\ud83d\udcbb GPT4All-J: The First Apache-2 Licensed Chatbot That Runs Locally on Your Machine: https://twitter.com/minchoi/status/1646995449646047233  \n\ud83d\udcbb OpenAssistant has just been released: https://twitter.com/dvruette/status/1647300294986940416  \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "6qjxxLiiSgU", "label": "[#77] \ud83e\udd16StableLM: Nuevo Modelo Open-Source de StabilityAI! (Demo)", "imagen": "https://i.ytimg.com/vi/6qjxxLiiSgU/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc"], "position": 11, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy5EMEEwRUY5M0RDRTU3NDJC", "publishedAt": "2023-04-20", "description": "(Titulo, descripcion, todo escrito por GPT-4) \"\u00bfTe enteraste de StableLM? En este video, analizamos la propuesta de Stability AI y su revolucionario conjunto de modelos de lenguaje. Te contamos todo sobre el rendimiento sorprendente de StableLM en tareas conversacionales y de codificaci\u00f3n, la importancia de su transparencia y accesibilidad, y c\u00f3mo esta tecnolog\u00eda puede cambiar la forma en que abordamos la IA en el futuro. \u00a1No te lo pierdas, pibe!\"\n#gpt4 #stabilityai #chatgpt \n\nContenido\n00:00 - StableLM, un nuevo LLM de StabilityAI\n05:30 - Demo\n10:30 - De donde viene la data?\n\nLinks mencionados durante el video:\n\ud83d\udcbb Stability AI Launches the First of its StableLM Suite of Language Models: https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models  \n\ud83d\udcbb StableLM Repo: https://github.com/stability-AI/stableLM/  \n\ud83d\udcbb StableLM Colab: https://colab.research.google.com/github/Stability-AI/StableLM/blob/main/notebooks/stablelm-alpha.ipynb\n\ud83d\udcbb StableLM Models: https://huggingface.co/stabilityai \n\ud83d\udcbb StableLM-Tuned-Alpha-7b Chat: https://huggingface.co/spaces/stabilityai/stablelm-tuned-alpha-chat \n\ud83d\udcbb Twitter announcement: https://twitter.com/StabilityAI/status/1648706156330876928 \n\ud83d\udcbb Qu\u00e9 dice Emad: https://twitter.com/EMostaque/status/1648743461841928240 \n\ud83d\udcbb \u201cInside the secret list of websites that make AI like ChatGPT sound smart\u201d: https://www.washingtonpost.com/technology/interactive/2023/ai-chatbot-learning/ \n\ud83d\udcbb We are open-sourcing @h2oai's LLM repositories: https://twitter.com/ArnoCandel/status/1648544547759177728 \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "W5DEyNGvTNM", "label": "[#79] OpenAssistant & HuggingChat: M\u00e1s cerca de un ChatGPT open-source", "imagen": "https://i.ytimg.com/vi/W5DEyNGvTNM/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc"], "position": 12, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy45ODRDNTg0QjA4NkFBNkQy", "publishedAt": "2023-04-28", "description": "(Titulo, descripcion, todo escrito por GPT-4) \"\u00a1\u00a1Atenci\u00f3n, pibe! En este video, te presentamos el OpenAssistant, la Inteligencia Artificial de chat open-source m\u00e1s grosa y prometedora, creada con el esfuerzo de 13,500 voluntarios. Exploramos sus capacidades al m\u00e1ximo, desde c\u00f3mo cargar su dataset (OASST1) en un Colab notebook usando pandas, hasta sus habilidades para generar texto, traducir idiomas y escribir contenido creativo de forma asombrosa. El dataset OASST1, liberado bajo la licencia Apache 2.0, incluye m\u00e1s de 161,443 mensajes, 66,497 \u00e1rboles de conversaci\u00f3n y abarca hasta 35 idiomas diferentes, \u00a1un verdadero tesoro para la comunidad!\n\nAdem\u00e1s, en un duelo de titanes, enfrentamos al OpenAssistant con el ChatGPT, comparando sus fortalezas y debilidades en una variedad de tareas y demostrando por qu\u00e9 este chatbot open-source tiene el potencial de cambiar la forma en que nos comunicamos con las computadoras y revolucionar el mundo de los chatbots. \u00a1Dale play a este video y sumate a la aventura de explorar el futuro de la Inteligencia Artificial en el mundo de las conversaciones!\u201d\n#gpt4 #openassistant #huggingface \n\nContenido\n00:00 - OpenAssistant: Demo\n06:56 - OpenAssistant: Origen, Dataset, y Benchmarks\n20:30 - HuggingChat: Demo y Colab\n23:30 - h2o-GPT\n\nLinks mencionados durante el video:\n\ud83d\udcbb Dataset: https://huggingface.co/datasets/OpenAssistant/oasst1\n\ud83d\udcbb Model weights: https://huggingface.co/OpenAssistant \n\ud83d\udcbb Website: https://open-assistant.io/\n\ud83d\udcbb Repo: https://github.com/LAION-AI/Open-Assistant \n\ud83d\udcbb Vision & Roadmap: https://docs.google.com/presentation/d/1n7IrAOVOqwdYgiYrXc8Sj0He8krn5MVZO_iLkCjTtu0/edit#slide=id.p  \n\ud83d\udcbb \u201cOpenAssistant RELEASED! The world's best open-source Chat AI!\u201d: https://www.youtube.com/watch?v=ddG2fM9i4Kk \n\ud83d\udcbb \u201cOPENASSISTANT TAKES ON CHATGPT!\u201d: https://www.youtube.com/watch?v=TFa539R09EQ&ab_channel=MachineLearningStreetTalk \n\ud83d\udcbb Analysis of the first results: https://twitter.com/omarsar0/status/1647354204665655297 \n\ud83d\udcbb \u201cLoad the OpenAssistant oasst1 dataset with pandas\u201d por Quentin Lhoest: https://colab.research.google.com/drive/1-vtZA5pjfEL5aIUQH_b1juqwW2-alln5?usp=sharing \n\ud83d\udcbb open-source datasets: https://twitter.com/rasbt/status/1648306007506931712 \n\ud83d\udcbb HuggingChat: https://huggingface.co/chat/  \n\ud83d\udcbb  @h2oai released a new NeoX 20b-based model fine-tuned on Open Assistant data: https://twitter.com/timohear/status/1650369044090966017 \n\ud83d\udcbb h2o GPT demo: https://gpt.h2o.ai/  \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "Tncj4X-85M4", "label": "[#80] DeepFloyd-IF & StableVicuna (Demo): Lo \u00faltimo de StabilityAI \u2026 SD puede escribir texto!", "imagen": "https://i.ytimg.com/vi/Tncj4X-85M4/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc", "PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 13, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy4zMDg5MkQ5MEVDMEM1NTg2", "publishedAt": "2023-05-04", "description": "(Escrito por Google Bard + GPT4) \u201cEn este video, te voy a presentar el DeepFloyd-IF, un nuevo modelo de difusi\u00f3n de texto a imagen que es lo m\u00e1s de lo m\u00e1s. El DeepFloyd-IF es un modelo de difusi\u00f3n basado en p\u00edxeles que puede generar im\u00e1genes de alta calidad a partir de descripciones en texto. Est\u00e1 entrenado con un mont\u00f3n de datos de texto e im\u00e1genes, y puede generar im\u00e1genes de cualquier tama\u00f1o o estilo.\nEn el video, te voy a mostrar c\u00f3mo usar el DeepFloyd-IF para generar im\u00e1genes a partir de descripciones en texto. Tambi\u00e9n te voy a mostrar algunas de las im\u00e1genes incre\u00edbles que el DeepFloyd-IF puede crear.\nSi te interesa la generaci\u00f3n de im\u00e1genes a partir de texto, te re recomiendo ver este video. El DeepFloyd-IF es una herramienta poderosa que se puede usar para generar im\u00e1genes realistas y creativas a partir de descripciones en texto.\nAc\u00e1 te dejo algunos detalles adicionales sobre el DeepFloyd-IF:\n- Es un modelo de difusi\u00f3n basado en p\u00edxeles, lo que significa que genera im\u00e1genes de a un pixel por vez.\n- Est\u00e1 entrenado con una bocha de datos de texto e im\u00e1genes, lo que le permite generar im\u00e1genes de alta calidad.\n- Puede generar im\u00e1genes de cualquier tama\u00f1o o estilo.\n- Es f\u00e1cil de usar y se puede ejecutar en cualquier computadora.\n\u00a1Espero que disfrutes el video!\u201d\n#deepfloyd #stablediffusion #gpt4 \n\n\ud83d\ude80 Discord: https://discord.gg/rV888Z4JCJ\n\nContenido\n00:00 - StabilityAI: origen\n04:40 - DeepFloyd-IF: modelo y pruebas\n15:50 - StableVicuna: blog, demo, modelo\n\nLinks mencionados durante el video:\n\ud83d\udcd5 DeepFloyd blog: https://huggingface.co/blog/if \n\ud83d\udcf1 DeepFloyd demo: https://huggingface.co/spaces/DeepFloyd/IF \n\ud83e\udde0 DeepFloyd modelo: https://huggingface.co/DeepFloyd/IF-I-XL-v1.0 \n\ud83d\udcd6 DeepFloyd docs: https://huggingface.co/docs/diffusers/api/pipelines/if \n\ud83d\udcbb DeepFloyd colab: https://github.com/camenduru/DeepFloyd-IF-colab \n\ud83d\udcd5 StableVicuna blog: https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot\n\ud83d\udcf1 StableVicuna demo: https://huggingface.co/spaces/CarperAI/StableVicuna \n\ud83e\udde0 StableVicuna modelo: https://huggingface.co/CarperAI/stable-vicuna-13b-delta \n\ud83d\udcbb ChatGPT, GenerativeAI and LLMs Timeline: https://github.com/hollobit/GenAI_LLM_timeline\n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "S5DnVbB89pE", "label": "[#81] Se filtr\u00f3 documento interno de Google: \u201cNo tenemos ninguna ventaja sobre open-source\u201d", "imagen": "https://i.ytimg.com/vi/S5DnVbB89pE/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc"], "position": 14, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy41Mzk2QTAxMTkzNDk4MDhF", "publishedAt": "2023-05-06", "description": "(Escrito por GPT4) \u201cEn este video, analizamos c\u00f3mo los Modelos de Lenguaje Grande (LLMs) open-source est\u00e1n desafiando a gigantes de IA como Google y OpenAI, dej\u00e1ndolos sin ventaja competitiva. Discutimos c\u00f3mo la r\u00e1pida innovaci\u00f3n en el espacio open-source est\u00e1 llevando a soluciones de IA m\u00e1s r\u00e1pidas, personalizables y accesibles, que est\u00e1n cerrando la brecha con los modelos comerciales. \u00a1\u00danete a nosotros mientras exploramos c\u00f3mo Google y otros gigantes de IA pueden aprender del enfoque open-source y adaptarse a este cambio en el panorama de la inteligencia artificial!\u201d\n#google  #opensource #gpt4 \n\n\u00cdndice de Contenido:\n0:00 - Introducci\u00f3n\n1:23 - LLMs Open-Source Superando a los Modelos Comerciales\n3:55 - El Desaf\u00edo de Meta y la Filtraci\u00f3n de LLaMA\n6:29 - Innovaciones Clave en el Espacio Open-Source\n9:11 - Google y OpenAI: La Necesidad de Adaptarse al Cambio\n12:32 - La Importancia de LoRA en la Innovaci\u00f3n Open-Source\n15:14 - Modelos Peque\u00f1os vs. Modelos Grandes: Velocidad de Iteraci\u00f3n\n18:02 - Calidad de Datos vs. Tama\u00f1o de Datos: Lecciones para IA Comercial\n20:38 - El Cambio en la Estrategia de Negocio de Google y OpenAI\n24:16 - Aprovechando el Ecosistema Open-Source y Colaborando\n27:53 - Conclusi\u00f3n\n\n\ud83d\ude80 Discord: https://discord.gg/rV888Z4JCJ\n\nLinks mencionados durante el video:\n\ud83d\udcd5 Google \"We Have No Moat, And Neither Does OpenAI\u201d: https://www.semianalysis.com/p/google-we-have-no-moat-and-neither?utm_source=direct&utm_campaign=post&utm_medium=web \n\ud83d\udcf1 Discusi\u00f3n en YCombinator: https://news.ycombinator.com/item?id=35813322 \n\ud83e\udde0 Emad@StabilityAI: https://twitter.com/EMostaque/status/1654237268305911810 \n\ud83d\udcd6 If you fine-tune on books3 full of 197,000 Copyrighted works, are you sure you have legal rights to relicense the model as commercial?\u201d: https://twitter.com/alexjc/status/1654568180185186304 \n\ud83d\udcbb John Schulman - Reinforcement Learning from Human Feedback: Progress and Challenges: https://www.youtube.com/watch?v=hhiLw5Q_UFg&ab_channel=BerkeleyEECS \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "aqv6Sd67R7M", "label": "[#83] Charla de Andrej Karpathy (OpenAI): \u201cEstado de GPT\u201d", "imagen": "https://i.ytimg.com/vi/aqv6Sd67R7M/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc"], "position": 15, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy5EQUE1NTFDRjcwMDg0NEMz", "publishedAt": "2023-05-25", "description": "(Escrito por GPT4) \"Acomp\u00e1\u00f1anos en este recorrido por la reciente presentaci\u00f3n de Andrej Karpathy, director de IA en Tesla, sobre el modelo de lenguaje GPT-4. Desvelaremos los puntos clave de su charla, desde los pasos de pre-entrenamiento hasta las t\u00e9cnicas de fine-tuning y evaluaci\u00f3n de los modelos. \u00a1No te pierdas esta oportunidad para comprender a fondo las \u00faltimas tendencias en IA!\" #gpt4 #openai #ChatGPT \n\nAqu\u00ed hay un resumen de lo que he podido recopilar de la presentaci\u00f3n de Andrej Karpathy sobre el GPT-4 a partir del hilo de Twitter que me proporcionaste:\n\n- El proceso de entrenamiento de los modelos de lenguaje se divide en varias etapas. Comienza con el pre-entrenamiento, que puede tomar meses y requiere miles de GPUs. Luego vienen las etapas de fine-tuning, que toman horas o d\u00edas.\n- Antes del pre-entrenamiento, hay dos pasos de preparaci\u00f3n: la recopilaci\u00f3n de datos y la tokenizaci\u00f3n.\n- El modelo de lenguaje de gran escala (LLaMa) ha sido entrenado en 1-1.4 trillones de tokens, en comparaci\u00f3n con los 300 mil millones de tokens en el GPT-3.\n- A partir del GPT-2, se observ\u00f3 que si estructuramos nuestros prompts de una manera espec\u00edfica y proporcionamos algunos ejemplos (prompts de pocos disparos), entonces el modelo base ser\u00e1 \"enga\u00f1ado\" para autocompletar las instrucciones que proporcionamos en el prompt.\n- Se realiza una fine-tuning supervisada, en la que se recogen conjuntos de datos de alta calidad y se contin\u00faa entrenando al modelo con un conjunto de datos intercambiado, obteniendo el modelo de fine-tuning supervisado (SFT).\n- El modelo SFT no es perfecto todav\u00eda, por lo que el entrenamiento contin\u00faa con la generaci\u00f3n de salidas de preguntas con el modelo SFT, y los usuarios revisan y comparan entre tres versiones y clasifican cu\u00e1l fue la mejor. Luego, el modelo se vuelve a entrenar en funci\u00f3n de las selecciones de los usuarios.\n- Los modelos de Reinforcement Learning from Human Feedback (RLHF) \"se sienten\" mejor para nosotros en t\u00e9rminos de ser un buen asistente. Estos modelos tienen menos entrop\u00eda, por lo que son menos \"inventivos\".\n- Andrej tambi\u00e9n habl\u00f3 sobre la diferencia entre c\u00f3mo un humano y un GPT escribir\u00edan una afirmaci\u00f3n como \"La poblaci\u00f3n de California es 53 veces la de Alaska\". El cerebro humano pasa por bucles, verifica hechos, hace c\u00e1lculos, reflexiona, mientras que un GPT simplemente intenta autocompletar.\n- Los m\u00e9todos como \"Chain of thought\" proporcionan a los modelos \"m\u00e1s tokens\" o \"m\u00e1s tiempo para pensar\" al pedir \"vamos a pensar paso a paso\". Esto hace que el modelo se tome m\u00e1s tiempo para reflexionar y proporcionar respuestas m\u00e1s cuidadosas y consideradas.\n\nEspero que esto te ayude a entender mejor la presentaci\u00f3n de Andrej Karpathy sobre el GPT-4.\" \n\n\u00cdndice de contenido:\n00:00 - Introducci\u00f3n y contexto de la presentaci\u00f3n de Karpathy, quien es?\n10:15 - Finetuning supervisado y Reinforcement Learning from Human Feedback (RLHF)\n28:30 - Mejores pr\u00e1cticas para las LLMs\n\n\ud83d\ude80 Discord: https://discord.gg/rV888Z4JCJ\n\nLinks mencionados durante el video:\n\ud83d\udcbb \u201cState of GPT\u201d: https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2 \n\ud83d\udcd6 Summary: https://twitter.com/altryne/status/1661236778458832896 \n\ud83e\udde0 Andrej Karpathy: https://karpathy.ai/ \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "5l65NitQqog", "label": "[#84] Falcon-40B \ud83d\ude80 El nuevo rey de los modelos de lenguaje open-source", "imagen": "https://i.ytimg.com/vi/5l65NitQqog/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc"], "position": 16, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy41QTY1Q0UxMTVCODczNThE", "publishedAt": "2023-05-26", "description": "\u201c\u00a1Che, qu\u00e9 tal! En este video, vamos a explorar el \u00faltimo avance en Modelos de Lenguaje Grande (LLMs): Falcon LLM, desarrollado por el Instituto de Innovaci\u00f3n Tecnol\u00f3gica (TII). Este modelo open-source, con 40 mil millones de par\u00e1metros, est\u00e1 desafiando a los gigantes de la IA, superando a modelos como LLaMA, StableLM, RedPajama, MPT y m\u00e1s. Adem\u00e1s, vamos a discutir c\u00f3mo Falcon LLM, con su licencia que permite el uso comercial, est\u00e1 cambiando el juego en el mundo de la inteligencia artificial. \u00a1No te lo pierdas, pibe!\u201d\n#falcon #opensource #ai\n\n\u00cdndice de Contenido:\n0:00 - Introducci\u00f3n\n3:23 - Falcon LLM: El nuevo gigante de la IA\n6:55 - Superando a los Modelos Comerciales: LLaMA, StableLM, RedPajama, MPT\n16:29 - Licencia de Falcon LLM: Uso comercial permitido\n19:11 - C\u00f3mo Falcon LLM est\u00e1 cambiando el juego en la IA\n23:14 - Conclusiones y reflexiones finales\n\n\ud83d\ude80 Discord: https://discord.gg/rV888Z4JCJ\n\nLinks mencionados durante el video:\n\ud83d\udcbb Falcon LLM: https://falconllm.tii.ae/ \n\ud83d\udcf1 Falcon LLM en Hugging Face: https://huggingface.co/tiiuae/falcon-40b \n\ud83d\udcd6 Falcon LLM RefinedWeb: https://falconllm.tii.ae/Falcon_LLM_RefinedWeb.pdf \n\ud83d\udc26 Anuncio de Falcon LLM en Twitter: https://twitter.com/slippylolo/status/1662082035744227330 \n\ud83d\udcd6 BLOOM: https://bigscience.huggingface.co/blog/bloom \n\ud83d\udcbb Large Transformer Model Inference Optimization: https://lilianweng.github.io/posts/2023-01-10-inference-optimization/ \n\ud83d\udcd6 Fast Transformer Decoding: One Write-Head is All You Need: https://arxiv.org/abs/1911.02150 \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "wt5Py4C6MCI", "label": "[#85] Guia definitiva de Falcon LLM: Inferencia, LangChain, y m\u00e1s.. (parte 1)", "imagen": "https://i.ytimg.com/vi/wt5Py4C6MCI/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc"], "position": 17, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy4yMUQyQTQzMjRDNzMyQTMy", "publishedAt": "2023-06-18", "description": "\"En este video, exploramos Falcon, una nueva y potente herramienta de Inteligencia Artificial desarrollada por el equipo de TIIAE. Descubre c\u00f3mo Falcon est\u00e1 revolucionando el procesamiento del lenguaje natural y desafiando los paradigmas existentes. Analizamos el sitio web oficial de Falcon y examinamos su impacto en la comunidad de IA. Adem\u00e1s, exploramos c\u00f3mo Falcon se integra con Hugging Face y c\u00f3mo aprovechar su potencial en la nube. Acomp\u00e1\u00f1anos mientras exploramos los diversos casos de uso de Falcon y su papel en la generaci\u00f3n de texto de alta calidad\". #ai #falcon #opensource \n\n\u00cdndice de Contenido:\n0:00 - Introducci\u00f3n a Falcon LLM\n11:23 - Demo de Falcon: h2oGPT & Hugging Face\n18:55 - Como podemos deployar el modelo, local & cloud?\n25:29 - Falcon y LangChain: LLM apps\n\nEnlaces mencionados durante el video:\n\ud83d\udcbb Sitio web oficial de Falcon: https://falconllm.tii.ae/ \n\ud83e\udd17 Falcon en Hugging Face: https://huggingface.co/blog/falcon \n\ud83d\ude80 Demo Falcon: https://gpt-gm.h2o.ai \n\ud83d\udd01 Comparaci\u00f3n de instancias de GPU en la nube: https://fullstackdeeplearning.com/cloud-gpus/ \n\ud83d\udd01 Is Falcon really better than LLaMA?: https://twitter.com/Francis_YAO_/status/1666833311279517696 \n\ud83d\uddc3\ufe0f Conjuntos de datos: Self-instruct (utilizado por Stanford en Alpaca): https://github.com/yizhongw/self-instruct \n\ud83d\uddc3\ufe0f Conjuntos de datos: Alpaca-LoRA: https://github.com/tloen/alpaca-lora \n\ud83d\uddc3\ufe0f Conjuntos de datos: OpenAssistant: https://huggingface.co/OpenAssistant \n\ud83d\uddc3\ufe0f Conjuntos de datos: RefinedWeb (600B tokens): https://falconllm.tii.ae/Falcon_LLM_RefinedWeb.pdf & https://huggingface.co/datasets/tiiuae/falcon-refinedweb \n\ud83d\uddc3\ufe0f Conjuntos de datos: SlimPajama (627B tokens): https://huggingface.co/datasets/cerebras/SlimPajama-627B & https://github.com/Cerebras/modelzoo/tree/main/modelzoo/transformers/data_processing/slimpajama \n\ud83d\ude80 C\u00f3mo ejecutar Falcon en AWS: Despliegue de Falcon 7B y 40B en Amazon SageMaker: https://www.philschmid.de/sagemaker-falcon-llm \n\ud83d\ude80 C\u00f3mo ejecutar Falcon en Runpod: https://colab.research.google.com/drive/10BJcKRBtMlpm2hsS2antarSRgEQY3AQq#scrollTo=dCB4izshhTMS  \n\ud83d\udcda Implementando con SM Jumpstart: Generaci\u00f3n de texto con modelos Falcon: https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-falcon.ipynb\n\u2699\ufe0f Contenedor de inferencia de Hugging Face LLM para Amazon SageMaker: https://huggingface.co/blog/sagemaker-huggingface-llm \n\u2699\ufe0f Portabilidad de modelos a C/C++ (Falcon en desarrollo): https://github.com/ggerganov/llama.cpp \n\u2699\ufe0f Versiones fine-tuned de Falcon en Hugging Face: Todas las disponibles (+250): https://huggingface.co/models?search=falcon \n\u2699\ufe0f H2oGPT, Falcon-7b con ventana de contexto de 2K tokens fine-tuned en OpenAssistant (+160K \u00e1rboles de conversaci\u00f3n): https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3 \n\ud83d\udd17 Falcon con LangChain: Gu\u00eda paso a paso para ejecutar una aplicaci\u00f3n de chat utilizando Chainlit: https://github.com/menloparklab/falcon-langchain/tree/main \n\ud83d\udd17 Chainlit: https://github.com/Chainlit/chainlit \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "z4Tba1vFIp8", "label": "[#86] \u00bfGPT-4 puede superar el plan de estudios del MIT? Analizando la verdad detr\u00e1s del revuelo", "imagen": "https://i.ytimg.com/vi/z4Tba1vFIp8/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc"], "position": 18, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy45RTgxNDRBMzUwRjQ0MDhC", "publishedAt": "2023-06-20", "description": "Descripcion: \u201cEn este video, analizamos la afirmaci\u00f3n de que GPT-4 puede superar el plan de estudios de ingenier\u00eda del MIT. Discutimos c\u00f3mo la supuesta haza\u00f1a de GPT-4 provoc\u00f3 un frenes\u00ed en Twitter, pero pronto se revel\u00f3 que estaba lejos de ser cierta. Investigamos la metodolog\u00eda defectuosa detr\u00e1s de este estudio y c\u00f3mo tales pr\u00e1cticas pueden socavar la confianza en la IA y el aprendizaje autom\u00e1tico. \u00a1Acomp\u00e1\u00f1anos mientras exploramos este interesante evento y las lecciones que podemos aprender de \u00e9l!\u201d\n#gpt4 #mit #ia #machinelearning #opensource \n\n\u00cdndice de Contenido:\n0:00 - Introducci\u00f3n\n3:15 - La afirmaci\u00f3n: GPT-4 supera el plan de estudios de ingenier\u00eda del MIT\n20:35 - La realidad: Metodolog\u00eda defectuosa y conjuntos de datos contaminados\n28:00 - Implicaciones y lecciones aprendidas\n\nLinks mencionados durante el video:\n\ud83d\udcbb Exploring the MIT Mathematics and EECS Curriculum Using Large Language Models: https://arxiv.org/abs/2306.08997 \n\ud83d\udcbb Code: https://github.com/idrori/MITQ  \n\ud83d\udcbb An\u00e1lisis de la afirmaci\u00f3n en Notion: https://flower-nutria-41d.notion.site/No-GPT4-can-t-ace-MIT-b27e6796ab5a48368127a98216c76864#d4bea55e46b5490184bf797f96de35c0 \n\ud83d\udc26 Discusi\u00f3n en Twitter: https://twitter.com/bhutanisanyam1/status/1670064089278865410, https://twitter.com/togelius/status/1670290844740378625, https://twitter.com/mmitchell_ai/status/1670029370482622464, https://twitter.com/protosphinx/status/1670231845630345216 \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "cAVN6ElTVVU", "label": "[#87] \ud83d\udc33 \"Orca\u201d: Nuevo Modelo Open Source de Microsoft", "imagen": "https://i.ytimg.com/vi/cAVN6ElTVVU/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc"], "position": 19, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy5ENDU4Q0M4RDExNzM1Mjcy", "publishedAt": "2023-06-23", "description": "Descripcion: \u201cEn este video, exploramos la discusi\u00f3n actual sobre los Modelos de Lenguaje Grande (LLMs) open-source y su comparaci\u00f3n con los modelos propietarios como GPT-4 de OpenAI. Analizamos diferentes puntos de vista, investigaciones recientes y las implicancias de estos hallazgos para el futuro de la IA. \u00bfPodr\u00e1n los modelos open-source cerrar la brecha con los modelos propietarios? \u00a1Acomp\u00e1\u00f1anos en este viaje por el apasionante mundo de la inteligencia artificial!\u201d\n#opensource #gpt4 #llm\n\n\u00cdndice de Contenido:\n0:00 - Introducci\u00f3n\n2:45 - Orca: Progressive Learning from Complex Explanation Traces of GPT-4\n7:30 - The False Promise of Imitating Proprietary LLMs\n12:15 - How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources\n16:50 - Porque Orca es importante? Conclusiones\n\nLinks mencionados durante el video:\n\ud83d\udcda \"The False Promise of Imitating Proprietary LLMs\": https://arxiv.org/abs/2305.15717 \n\ud83d\udcd6 \"How far can Camels go\": https://arxiv.org/abs/2306.04751 \n\ud83d\udd2c Orca paper from Microsoft Research: https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/ \n\ud83d\udc26 Discusi\u00f3n en Twitter: https://twitter.com/nlpguy_/status/1667174332756381696, https://twitter.com/arankomatsuzaki/status/1661908342829187072, https://twitter.com/cwolferesearch/status/1670922169038041090, https://twitter.com/bhutanisanyam1/status/1667888610455027712, https://twitter.com/_philschmid/status/1667119049287843840, https://twitter.com/rohanpaul_ai/status/1670346463732195330, https://twitter.com/RickLamers/status/1667993711203033089, https://twitter.com/abacaj/status/1670257445959000066.\n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "7YDxI66fySk", "label": "[#88] El nuevo stack para LLMs: como hacen las empresas para hacer aplicaciones de AI", "imagen": "https://i.ytimg.com/vi/7YDxI66fySk/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc"], "position": 20, "type": "video", "id": "UExXaENRaTk3ZHN0bTZkdkNuNkxJR0s1UzNlb3NfSEV6Yy4yMDhBMkNBNjRDMjQxQTg1", "publishedAt": "2023-06-26", "description": "Descripcion: \u201cEn este video, exploramos las arquitecturas emergentes para las aplicaciones de Modelos de Lenguaje Grande (LLMs) y c\u00f3mo est\u00e1n cambiando el juego en el mundo de la inteligencia artificial. Discutimos c\u00f3mo los LLMs est\u00e1n creando nuevas fortalezas y oportunidades para las empresas y los desarrolladores. Tambi\u00e9n analizamos c\u00f3mo la pila de LLM est\u00e1 evolucionando y qu\u00e9 significa esto para el futuro de la IA. \u00a1Acomp\u00e1\u00f1anos en este viaje mientras desentra\u00f1amos las complejidades de los LLMs y su impacto en el panorama de la IA!\u201d\n#llm #sequoia #a16z #greylock #ai \n\n\u00cdndice de Contenido:\n0:00 - Introducci\u00f3n\n3:00 - Sequouia: Arquitecturas Emergentes para Aplicaciones de LLM\n15:25 - a16z: La Evoluci\u00f3n de los stacks de LLMs\n25:30 - Greylock: Las Nuevas Fortalezas en la Inteligencia Artificial\n35:45 - Ejemplo: multi LLM powered chatbot using AWS CDK on AWS\n\nLinks mencionados durante el video:\n\ud83d\udcbb Tweet de Shaun: https://twitter.com/agishaun/status/1672027117201293312 \n\ud83d\udcf1 Perspectiva de la Pila de LLM: https://www.sequoiacap.com/article/llm-stack-perspective/ \n\ud83d\udcd6 Arquitecturas Emergentes para Aplicaciones de LLM: https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/ \n\ud83d\udcd8 Las Nuevas Nuevas Fortalezas: https://greylock.com/greymatter/the-new-new-moats/ \n\ud83d\udcbb A comprehensive sample to deploy a multi LLM powered chatbot using AWS CDK on AWS: https://github.com/aws-samples/aws-genai-llm-chatbot \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "DRf4j0EpYuQ", "label": "[#55] Curso LLM-RLHF (5/n) - Que hace que un agente de di\u00e1logo sea \u00fatil?", "imagen": "https://i.ytimg.com/vi/DRf4j0EpYuQ/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnLKjstxZzEXoc1dyp4cXXB"], "position": 0, "type": "video", "id": "UExXaENRaTk3ZHN0bkxLanN0eFp6RVhvYzFkeXA0Y1hYQi4xMkVGQjNCMUM1N0RFNEUx", "publishedAt": "2023-02-05", "description": "Es el el 5to video de la serie sobre grandes modelos de lenguaje donde vamos a aprender lo necesario para hacer una versi\u00f3n abierta de ChatGPT. Como no es algo facil y esto va cambiando semana a semana, vamos a ir viendo todo paso a paso yendo desde lo mas f\u00e1cil a lo mas dificil tratando de explicar en detalle cada parte. Hoy vamos a hacer un poco de trampa y vamos hacer un repaso de cuales son las cosas que se necesitan para que un agente de dialogo, un chatbot, sea util.\n#chatgpt #openai #huggingface \n\nContenido\n00:00 - Landscape of language model based conversational agents\n18:00 - From prediction text to following instructions\n26:10 - Safely following instructions\n28:20 - Fine-tuning the models\n32:30 - Conclusiones\n\nLinks mencionados durante el video:\n\ud83d\udcbb What Makes a Dialog Agent Useful?: https://huggingface.co/blog/dialog-agents \n\ud83d\udcbb New Scaling Laws for Large Language Models: https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models \n\ud83d\udcbb LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale: https://arxiv.org/pdf/2208.07339.pdf \n\ud83d\udcbb DeepMind Sparrow: https://www.deepmind.com/blog/building-safer-dialogue-agents \n\ud83d\udcbb OpenAI InstructGPT: https://openai.com/blog/instruction-following/\n\ud83d\udcbb FLAN-T5-XXL: https://huggingface.co/google/flan-t5-xxl \n\ud83d\udcbb OPT-IML-30B: https://huggingface.co/facebook/opt-iml-30b\n\ud83d\udcbb Why is Meta\u2019s new AI chatbot so bad? https://www.vox.com/future-perfect/23307252/meta-facebook-bad-ai-chatbot-blenderbot\n\ud83d\udcbb The Google engineer who thinks the company\u2019s AI has come to life: https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/ \n\ud83d\udcbb Anthropic Claude: https://scale.com/blog/chatgpt-vs-claude \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "TMYpH8wsGFU", "label": "[#50] Curso LLM-RLHF (4/n) - In-Context Learning & Gradient Descent explicado por un Data Scientist", "imagen": "https://i.ytimg.com/vi/TMYpH8wsGFU/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnLKjstxZzEXoc1dyp4cXXB"], "position": 1, "type": "video", "id": "UExXaENRaTk3ZHN0bkxLanN0eFp6RVhvYzFkeXA0Y1hYQi4wOTA3OTZBNzVEMTUzOTMy", "publishedAt": "2023-01-18", "description": "Cuarto video de la serie sobre modelos de lenguaje gigantes donde vamos a aprender lo necesario para hacer una versi\u00f3n abierta de ChatGPT. Como no es algo facil y esto va cambiando semana a semana, vamos a ir viendo todo paso a paso yendo desde lo mas f\u00e1cil a lo mas complejo sin esquivarle a lo dificil pero tratando de explicar todo. Hoy vamos a hablar de In-Context Learning.\n#chatgpt #nlp #gpt3\n\nContenido\n00:00 - Motivaci\u00f3n & Background\n05:00 - Qu\u00e9 es in-context learning?\n11:30 - Scratchpad, chain-of-thought, algorithmic reasoning\n27:30 - Pregunta abierta: Los LLM aprenden por gradient descent?\n\nLinks mencionados durante el video:\n\ud83d\udcbb \u201cChatGPT: Curso LLM-RLHF\u201d: https://www.youtube.com/playlist?list=PLWhCQi97dstnLKjstxZzEXoc1dyp4cXXB \n\ud83d\udcbb Lecture 07: Foundation Models (FSDL 2022): https://www.youtube.com/watch?v=Rm11UeGwGgk&ab_channel=FullStackDeepLearning \n\ud83d\udcbb \u201c[#35] PRIMICIA! Demo en vivo de AlexaTM 20B con Amazon SageMaker JumpStart (Mejor que GPT-3)\u201d: https://www.youtube.com/watch?v=Li-F3zb2P_c&ab_channel=machinelearnear \n\ud83d\udcbb \u201c#91 - HATTIE ZHOU - Teaching Algorithmic Reasoning via In-context Learning\u201d: https://www.youtube.com/watch?v=80i6D2TJdQ4&ab_channel=MachineLearningStreetTalk \n\ud83d\udcbb Teaching Algorithmic Reasoning via In-context Learning: https://arxiv.org/pdf/2211.09066.pdf \n\ud83d\udcbb Transformers learn in-context by gradient descent: https://arxiv.org/pdf/2212.07677.pdf \n\ud83d\udcbb Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers: https://arxiv.org/pdf/2212.10559.pdf \n\ud83d\udcbb In-context Reinforcement Learning with Algorithm Distillation: https://arxiv.org/pdf/2210.14215.pdf \n\ud83d\udcbb \u201cConnor Leahy\u2013EleutherAI, Conjecture\u201d: https://www.youtube.com/watch?v=Oz4G9zrlAGs&ab_channel=TheInsideView \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "tzPuVAJ3XoI", "label": "[#49] Curso LLM-RLHF (3/n) - Reinforcement Learning from Human Feedback explicado por Data Scientist", "imagen": "https://i.ytimg.com/vi/tzPuVAJ3XoI/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnLKjstxZzEXoc1dyp4cXXB"], "position": 2, "type": "video", "id": "UExXaENRaTk3ZHN0bkxLanN0eFp6RVhvYzFkeXA0Y1hYQi41MjE1MkI0OTQ2QzJGNzNG", "publishedAt": "2023-01-17", "description": "Este va a ser el tercer video de una serie que estoy haciendo sobre modelos de lenguaje gigantes con el objetivo de poder aprender lo necesario para hacer una versi\u00f3n abierta de ChatGPT. Como no es algo facil y esto va cambiando semana a semana, vamos a ir viendo todo paso a paso yendo desde lo mas f\u00e1cil a lo mas complejo sin esquivarle a lo dificil pero tratando de explicar todo. Hoy vamos a hablar de Reinforcement Learning from Human Feedback.\n#chatgpt #rlhf #gpt3 \n\nContenido\n00:00 - Motivaci\u00f3n\n04:30 - Background sobre Foundation Models (LLM)\n13:50 - OpenAI InstructGPT, DeepMind Sparrow, Anthropic RLHF\n21:30 - Conceptos detr\u00e1s de RLFH\n33:00 - Modelo de recompensa y politica de optimizaci\u00f3n\n44:00 - Conclusiones\n\nLinks mencionados durante el video:\n\ud83d\udcbb \u201cChatGPT: Curso LLM-RLHF\u201d: https://www.youtube.com/playlist?list=PLWhCQi97dstnLKjstxZzEXoc1dyp4cXXB \n\ud83d\udcbb Lecture 07: Foundation Models (FSDL 2022): https://www.youtube.com/watch?v=Rm11UeGwGgk&ab_channel=FullStackDeepLearning \n\ud83d\udcbb Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback: https://github.com/anthropics/hh-rlhf \n\ud83d\udcbb Illustrating Reinforcement Learning from Human Feedback (RLHF): https://huggingface.co/blog/rlhf \n\ud83d\udcbb Reinforcement Learning from Human Feedback: From Zero to chatGPT: https://www.youtube.com/watch?v=2MBJOuVq380&ab_channel=HuggingFace \n\ud83d\udcbb \u201cChatGPT and Reinforcement Learning\u201d: https://www.youtube.com/watch?v=_MPJ3CyDokU&ab_channel=CodeEmporium \n\ud83d\udcbb Closed-API vs Open-source continues: RLHF, ChatGPT, data moats: https://robotic.substack.com/p/rlhf-chatgpt-data-moats \n\ud83d\udcbb Repo for distributed training of language models with Reinforcement Learning via Human Feedback (RLHF): https://github.com/CarperAI/trlx \n\ud83d\udcbb OpenAI's InstructGPT: Aligning Language Models with Human Intent: https://www.youtube.com/watch?v=QGpaBWOaHQI&t=719s&ab_channel=ScaleAI \n\ud83d\udcbb Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM: https://github.com/lucidrains/PaLM-rlhf-pytorch \n\ud83d\udcbb Here\u2019s What I Saw at an AI Hackathon: https://every.to/superorganizers/the-knee-of-the-exponential-curve \n\ud83d\udcbb 6 New Theories About AI: https://every.to/napkin-math/6-new-theories-about-ai \n\ud83d\udcbb \u201cwhat actually happens when we type inside the ChatGPT textbox\u201d: https://twitter.com/vboykis/status/1601930057076903936 \n\ud83d\udcbb Simulators: https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators \n\ud83d\udcbb What is Reinforcement Learning with Human Feedback (RLHF)? https://aisupremacy.substack.com/p/what-is-reinforcement-learning-with?utm_source=twitter&utm_campaign=auto_share&r=cxsa3 \n\ud83d\udcbb Take 9: No, RLHF/IDA/debate doesn't solve outer alignment: https://www.lesswrong.com/posts/6YNZt5xbBT5dJXknC/take-9-no-rlhf-ida-debate-doesn-t-solve-outer-alignment \n\ud83d\udcbb Understanding Reinforcement Learning from Human Feedback (RLHF): Part 1: https://wandb.ai/ayush-thakur/RLHF/reports/Understanding-Reinforcement-Learning-from-Human-Feedback-RLHF-Part-1--VmlldzoyODk5MTIx \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "TYFy0wZpJuY", "label": "[#48] Curso LLM-RLHF (2/n) - Habilidades emergentes de GPT-3.5 explicado por un Data Scientist", "imagen": "https://i.ytimg.com/vi/TYFy0wZpJuY/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnLKjstxZzEXoc1dyp4cXXB"], "position": 3, "type": "video", "id": "UExXaENRaTk3ZHN0bkxLanN0eFp6RVhvYzFkeXA0Y1hYQi4wMTcyMDhGQUE4NTIzM0Y5", "publishedAt": "2023-01-15", "description": "Este video es el segundo de una serie que voy a hacer sobre como podemos entrenar ChatGPT. Como no es algo f\u00e1cil vamos a ir viendo todo paso a paso y yendo desde lo mas simple a lo mas complejo sin esquivarle a las partes dificiles pero tratando de explicar cada cosa. Hoy vamos a hablar de GPT-3.5 y sus habilidades emergentes.\n#chatgpt #gpt3  #nlp \n\nContenido\n00:00 - Motivaci\u00f3n e introducci\u00f3n\n12:30 - GPT-3 en el 2020\n17:50 - De 2020 a 2022 GPT-3\n21:30 - Entrenar con c\u00f3digo, tunear con instrucciones\n29:50 - Reinforcement Learning con Human Feedback\n34:00 - Resumen, limitaciones, y conclusiones\n\nLinks mencionados durante el video:\n\ud83d\udcbb \u201c[#37] Chat GPT: Vemos como funciona y porqu\u00e9 Google, \u00bfest\u00e1 en problemas?\u201d: https://www.youtube.com/watch?v=DqsgYlp6EmI \n\ud83d\udcbb State of AI Report 2022: https://www.stateof.ai/ \n\ud83d\udcbb Curso de LLM por Stanford: https://stanford-cs324.github.io/winter2022/ \n\ud83d\udcbb Major LLMs + Data Availability: https://docs.google.com/spreadsheets/d/1bmpDdLZxvTCleLGVPgzoMTQ0iDP2-7v7QziPrzPdHyM/edit#gid=0 \n\ud83d\udcbb Generative AI Startups: https://airtable.com/shrwmMVRmBb1N986x/tbl78Ow3go1TaVBpm \n\ud83d\udcbb OpenAssistant: https://github.com/LAION-AI/Open-Assistant \n\ud83d\udcbb How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources: https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1 \n\ud83d\udcbb nanoGPT: https://github.com/karpathy/nanoGPT \n\ud83d\udcbb Neural Networks: Zero to Hero: https://karpathy.ai/zero-to-hero.html \n\ud83d\udcbb ChatGPT: https://chat.openai.com/chat \n\ud83d\udcbb \"ChatGPT vs Sparrow - Battle of Chatbots\u201d: https://www.youtube.com/watch?v=SWwQ3k-DWyo \n\ud83d\udcbb Large Language Models are Zero-Shot Reasoners: https://arxiv.org/pdf/2205.11916.pdf \n\ud83d\udcbb Training language models to follow instructions with human feedback: https://arxiv.org/abs/2203.02155 \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "4uXeflZ8q8w", "label": "[#47] Curso LLM-RLHF (1/n) - Como crear ChatGPT desde 0 explicado por un Data Scientist", "imagen": "https://i.ytimg.com/vi/4uXeflZ8q8w/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnLKjstxZzEXoc1dyp4cXXB"], "position": 4, "type": "video", "id": "UExXaENRaTk3ZHN0bkxLanN0eFp6RVhvYzFkeXA0Y1hYQi41NkI0NEY2RDEwNTU3Q0M2", "publishedAt": "2023-01-14", "description": "Este capitulo de hoy va a ser el primero de una serie de videos que queria hacer para explicar desde 0 como podemos hacer nuestro propio ChatGPT. Vamos a ir de muy simple a muy complejo sin esquivarle a las partes dificiles pero tratando de explicar paso a paso cada cosa.\n#chatgpt #gpt3  #nlp \n\nContenido\n00:00 - Motivaci\u00f3n\n08:20 - Que son los LLM y qu\u00e9 es GPT-3?\n21:00 - C\u00f3mo es el mercado de las LLM, donde estamos?\n29:05 - Sparrow (DeepMind) vs ChatGPT. Otras alternativas.\n45:20 - Qu\u00e9 pasos tenemos que hacer para entrenar nuestro ChatGPT\n\nLinks mencionados durante el video:\n\ud83d\udcbb \u201c[#37] Chat GPT: Vemos como funciona y porqu\u00e9 Google, \u00bfest\u00e1 en problemas?\u201d: https://www.youtube.com/watch?v=DqsgYlp6EmI \n\ud83d\udcbb Foundation Models: A Primer for Investors and Builders: https://gradientflow.com/foundation-models-non-technical-guide/\n\ud83d\udcbb State of AI Report 2022: https://www.stateof.ai/ \n\ud83d\udcbb Curso de LLM por Stanford: https://stanford-cs324.github.io/winter2022/ \n\ud83d\udcbb Major LLMs + Data Availability: https://docs.google.com/spreadsheets/d/1bmpDdLZxvTCleLGVPgzoMTQ0iDP2-7v7QziPrzPdHyM/edit#gid=0 \n\ud83d\udcbb Generative AI Startups: https://airtable.com/shrwmMVRmBb1N986x/tbl78Ow3go1TaVBpm \n\ud83d\udcbb OpenAssistant: https://github.com/LAION-AI/Open-Assistant \n\ud83d\udcbb How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources: https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1 \n\ud83d\udcbb nanoGPT: https://github.com/karpathy/nanoGPT \n\ud83d\udcbb Neural Networks: Zero to Hero: https://karpathy.ai/zero-to-hero.html \n\ud83d\udcbb ChatGPT: https://chat.openai.com/chat \n\ud83d\udcbb \"ChatGPT vs Sparrow - Battle of Chatbots\u201d: https://www.youtube.com/watch?v=SWwQ3k-DWyo \n\ud83d\udcbb Large Language Models are Zero-Shot Reasoners: https://arxiv.org/pdf/2205.11916.pdf \n\ud83d\udcbb Training language models to follow instructions with human feedback: https://arxiv.org/abs/2203.02155 \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "DqsgYlp6EmI", "label": "[#37] Chat GPT: Vemos como funciona y porqu\u00e9 Google, \u00bfest\u00e1 en problemas?", "imagen": "https://i.ytimg.com/vi/DqsgYlp6EmI/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnLKjstxZzEXoc1dyp4cXXB"], "position": 5, "type": "video", "id": "UExXaENRaTk3ZHN0bkxLanN0eFp6RVhvYzFkeXA0Y1hYQi4yODlGNEE0NkRGMEEzMEQy", "publishedAt": "2022-12-03", "description": "Hoy vamos a hablar sobre el \u00faltimo release de OpenAI que se llama ChatGPT y es realmente, para mi, algo revolucionario a nivel experiencia de usuario de como interactuamos con grandes modelos de lenguaje. Vamos a ver como es que funciona tecnicamente y tambi\u00e9n que podemos esperar en un futuro. #openai #chatgpt #ai  \n\nContenido\n00:00 - Qu\u00e9 es ChatGPT?\n08:49 - Demo en vivo\n09:05 - Como funciona?\n29:30 - Reinforcement Learning from Human Feedback (RLHF)\n40:16 - Conclusiones\n\nLinks mencionados durante el video:\n\ud83d\udcbb ChatGPT (OpenAI): https://chat.openai.com/chat \n\ud83d\udcbb ChatGPT Blog Post: https://openai.com/blog/chatgpt/ \n\ud83d\udcbb InstructGPT: https://openai.com/blog/instruction-following/ \n\ud83d\udcbb Reinforcement Learning from Human Feedback (RLHF): https://www.lesswrong.com/posts/rQH4gRmPMJyjtMpTn/rlhf \n\ud83d\udcbb anthropic: https://www.anthropic.com/\n\ud83d\udcbb Repo: \"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\": https://github.com/anthropics/hh-rlhf \n\ud83d\udcbb Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback: https://arxiv.org/pdf/2204.05862.pdf \n\ud83d\udcbb Learning to summarise from human feedback (Paper Explained): https://www.youtube.com/watch?v=vLTmnaMpQCs \n\ud83d\udcbb Learning Task Specifications for Reinforcement Learning from Human Feedback | David Lindner: https://www.youtube.com/watch?v=vebzz6EKD2w \n\ud83d\udcbb OpenAI's InstructGPT: Aligning Language Models with Human Intent: https://www.youtube.com/watch?v=QGpaBWOaHQI \n\ud83d\udcbb #29 - OpenAI\u2019s InstructGPT is a Game Changer!: https://www.youtube.com/watch?v=CqPHoeddA7k \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "LqTlsTyPLUo", "label": "[#69] NeRF y el futuro de la edici\u00f3n 3D guiada por texto: \u00a1Explor\u00e1 Instruct-NeRF2NeRF!", "imagen": "https://i.ytimg.com/vi/LqTlsTyPLUo/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 1, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi4zRjM0MkVCRTg0MkYyQTM0", "publishedAt": "2023-03-29", "description": "(Titulo, descripcion, todo escrito por GPT-4) \u201cHoy te traemos un nuevo m\u00e9todo revolucionario para editar escenas 3D reconstruidas con NeRF utilizando simples instrucciones de texto. \u00a1Mir\u00e1 c\u00f3mo Instruct-NeRF2NeRF hace que la edici\u00f3n 3D sea m\u00e1s realista y f\u00e1cil que nunca!\"\n#nerf #3d #stablediffusion \n\nContenido\n00:00 - Introducci\u00f3n\n04:30 - El auge de NeRF en la reconstrucci\u00f3n 3D\n12:40 - InstructPix2Pix: El modelo de difusi\u00f3n condicionado por im\u00e1genes\n14:15 - Editando escenas 3D con texto\n\nLinks mencionados durante el video:\n\ud83d\udcbb Instruct-NeRF2NeRF: https://instruct-nerf2nerf.github.io/ \n\ud83d\udcbb \u201c3D capture is moving so fast - I scanned & animated this completely on an iPhone\u201d https://twitter.com/bilawalsidhu/status/1637450670998528002 \n\ud83d\udcbb Multi-ControlNet & Open Source AI Video Generation: https://creativetechnologydigest.substack.com/p/multi-controlnet-and-open-source\n\ud83d\udcbb nerf.studio: https://docs.nerf.studio/en/latest/  \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "gVFjDCZs_-E", "label": "[#67] Sali\u00f3 Bing Image Creator \ud83d\ude80 (GRATIS) como se compara vs Midjourney V5 / DALL-E 2 / SD", "imagen": "https://i.ytimg.com/vi/gVFjDCZs_-E/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 2, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi5GM0Q3M0MzMzY5NTJFNTdE", "publishedAt": "2023-03-23", "description": "Hoy vamos a hacer un video bastante tranqui. Vamos a revisar como funciona Bing Image Creator, un nuevo servicio de generaci\u00f3n de imagenes de Microsoft que es por ahora gratuito, y vamos a ver como funciona contra Midjourney V5, DALLE-2, y Stable Diffusion.\n#bing #midjourneyv5 #dalle2\n\nContenido\n00:00 - NVIDIA Picasso, Adobe Firefly\n02:30 - Open-source vs Closed-source Text2Image\n09:30 - Bing Image Creator vs MJ V5 vs DALL-E 2 vs Stable Diffusion.\n\nLinks mencionados durante el video:\n\ud83d\udcbb Bing Image Creator: https://www.bing.com/images/create \n\ud83d\udcbb Create images with your words \u2013 Bing Image Creator comes to the new Bing: https://blogs.microsoft.com/blog/2023/03/21/create-images-with-your-words-bing-image-creator-comes-to-the-new-bing/ \n\ud83d\udcbb OpenAI DALL-E 2: https://openai.com/product/dall-e-2 \n\ud83d\udcbb Midjourney V5: https://www.midjourney.com/ \n\ud83d\udcbb IF by @deepfloydai\n\ud83d\udcbb StableDiffusion-XL by @StabilityAI\n\ud83d\udcbb Composer by @AlibabaGroup: https://github.com/damo-vilab/composer \n\ud83d\udcbb Adobe Firefly: https://firefly.adobe.com/ \n\ud83d\udcbb NVIDIA Picasso: https://www.nvidia.com/en-us/gpu-cloud/picasso/ \n\ud83d\udcbb stable diffusion webui colab: https://github.com/camenduru/stable-diffusion-webui-colab \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "aU8kaShLRTI", "label": "[#54] Dreamix: Nueva AI de Google para editar videos basada en modelos de difusi\u00f3n (paper explicado)", "imagen": "https://i.ytimg.com/vi/aU8kaShLRTI/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 3, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi4yMDhBMkNBNjRDMjQxQTg1", "publishedAt": "2023-02-04", "description": "Hoy vamos a hablar de \u201cDreamix\u201d, un nuevo paper que sac\u00f3 Google, que usa modelos de difusi\u00f3n para editar y generar videos en base a prompts de texto.\n#google #dreamix #stablediffusion \n\nContenido\n00:00 - Qu\u00e9 es Dreamix?\n04:00 - Video editing with Diffusion\n07:10 - Image-to-Video\n09:00 - Subject Driven Video Generation\n10:30 - Como funciona?\n16:30 - Impacto social \n\nLinks mencionados durante el video:\n\ud83d\udcbb Dreamix: Video Diffusion Models are General Video Editors: https://dreamix-video-editing.github.io/ \n\ud83d\udcbb Paper: https://arxiv.org/abs/2302.01329 \n\ud83d\udcbb Instruct-Pix2Pix: https://github.com/timothybrooks/instruct-pix2pix \n\ud83d\udcbb Movie Diffusion explained | Make-a-Video from MetaAI and Imagen Video from Google Brain: https://www.youtube.com/watch?v=AcvmyqGgMh8\n\ud83d\udcbb The Illustrated Stable Diffusion: https://jalammar.github.io/illustrated-stable-diffusion/ \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "K99VrCnutZI", "label": "[#52] \"Metele una barba a Saitama\" (InstructPix2Pix e ImageMixer)", "imagen": "https://i.ytimg.com/vi/K99VrCnutZI/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 4, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi5ENDU4Q0M4RDExNzM1Mjcy", "publishedAt": "2023-01-25", "description": "Hoy vamos a ver 2 t\u00e9cnicas nuevas que salieron para Stable Diffusion que en un caso nos permiten editar imagenes siguiendo instrucciones como \u201cponele pelo a esta persona\u201d y en el otro nos permite mezclar varias imagenes, estilos, o prompts para terminar con una imagen completamente nueva.\n#stablediffusion #aiart #ai \n\nContenido\n00:00 - Qu\u00e9 es esto de editar im\u00e1genes con frases?\n08:10 - InstructPix2Pix: Como funciona y demo.\n12:40 - Que es esto de mezclar im\u00e1genes?\n14:30 - ImageMixer: Como funciona y demo\n\nLink a InstructPix2Pix Colab con demo en @Gradio:\nhttps://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/InstructPix2Pix_using_diffusers.ipynb \n\nLinks mencionados durante el video:\n\ud83d\udcbb playground.ai: playgroundai.com/create \n\ud83d\udcbb InstructPix2Pix: https://www.timothybrooks.com/instruct-pix2pix/ \n\ud83d\udcbb InstructPix2Pix: Learning to Follow Image Editing Instructions: https://github.com/timothybrooks/instruct-pix2pix \n\ud83d\udcbb Paper: https://arxiv.org/abs/2211.09800 \n\ud83d\udcbb Image Mixer por @Justin Pinkney: https://twitter.com/Buntworthy/status/1616042269945085953 \n\ud83d\udcbb Experiments in Image Variation: https://www.justinpinkney.com/image-variation-experiments/ \n\ud83d\udcbb Open HF Spaces in Google Colab: https://github.com/machinelearnear/open-hf-spaces-in-studiolab/blob/main/run_google_colab.ipynb \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "jNTrnoSAipw", "label": "[#46] Esta AI de Google aprendi\u00f3 a escribir y StabilityAI respondi\u00f3  (Muse/DeepFloyd/Karlo)", "imagen": "https://i.ytimg.com/vi/jNTrnoSAipw/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 5, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi45RTgxNDRBMzUwRjQ0MDhC", "publishedAt": "2023-01-12", "description": "Hoy vamos a ver unos nuevos modelos para generacion de imagenes que aparentemente pueden finalmente escribir texto dentro de las imagenes de una forma bastante buena. Vamos a ver de que se trata.\n#stablediffusion #aiart #muse \n\nContenido\n00:00 - Google: De Parti a Imagen a Muse\n11:30 - DeepFloyd + StabilityAI = IF\n14:00 - Karlo, una versi\u00f3n open source de DALLE-2 + Demo\n\nLinks mencionados durante el video:\n\ud83d\udcbb Parti (Google): https://parti.research.google/\n\ud83d\udcbb Imagen (Google): https://imagen.research.google/ \n\ud83d\udcbb Muse AI (Google): https://muse-model.github.io \n\ud83d\udcbb Karlo Huggingface: https://huggingface.co/spaces/kakaobrain/karlo \n\ud83d\udcbb Karlo Repo: https://github.com/kakaobrain/karlo  \n\ud83d\udcbb Huggingface unCLIP Docs: https://huggingface.co/docs/diffusers/main/en/api/pipelines/unclip \n\ud83d\udcbb DeepFloyd: https://twitter.com/deepfloydai \n\ud83d\udcbb Notebook Open HF Spaces in Colab: https://colab.research.google.com/github/machinelearnear/open-hf-spaces-in-studiolab/blob/main/run_google_colab.ipynb \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "C50zsvmGbvs", "label": "[#45] Finalmente \u2026 manos perfectas con Stable Diffusion", "imagen": "https://i.ytimg.com/vi/C50zsvmGbvs/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 6, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi4yMUQyQTQzMjRDNzMyQTMy", "publishedAt": "2023-01-10", "description": "Hace unos dias me dijeron en los comentarios que viera un modelo nuevo que habia salido y la verdad que suerte que les hice caso porque esta buenisimo. Se llama Protogen y va por la version X5.8 y por fin permite que Stable Diffusion pueda renderear bien las manos de las personas. Vamos a verlo juntos.\n#stablediffusion #tutorial #aiart \n\nContenido\n00:00 - Que es Protogen y que hace?\n06:48 - Demo en \u201cdiffuzers\u201d\n09:50 - Demo en \u201cAutomatic1111\u201d\n\nLinks mencionados durante el video:\n\ud83d\udcbb diffuzers: https://github.com/abhishekkrthakur/diffuzers \n\ud83d\udcbb TheLastBen SD UI: https://github.com/TheLastBen/fast-stable-diffusion \n\ud83d\udcbb \u201c[#43] Nueva UI muy simple para Stable Diffusion & Review de Modelos Fotorrealistas y de Anime\u201d: https://www.youtube.com/watch?v=xJRokNH6bvE \n\ud83d\udcbb \u201c[#28] C\u00f3mo usar Paperspace Gradient con Stable Diffusion (Web UI de AUTOMATIC1111)\u201d: https://www.youtube.com/watch?v=miHtA0wZD2c \n\ud83d\udcbb Protogen x3.4 (Photorealism) Official Release: https://civitai.com/models/3666/protogen-x34-official-release\n\ud83d\udcbb darkstorm2150/Protogen_v5.3_Official_Release: https://huggingface.co/darkstorm2150/Protogen_v5.3_Official_Release \n\ud83d\udcbb dreamlike-art/dreamlike-photoreal-2.0: https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0 \n\ud83d\udcbb \u201cPerfect AI Fingers with Protogen Stable Diffusion Model\u201d: https://www.youtube.com/watch?v=pOxbRMZsLUM \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "xJRokNH6bvE", "label": "[#43] Nueva UI muy simple para Stable Diffusion & Review de Modelos Fotorrealistas y de Anime", "imagen": "https://i.ytimg.com/vi/xJRokNH6bvE/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 7, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi41QTY1Q0UxMTVCODczNThE", "publishedAt": "2023-01-06", "description": "Este es el primer video del a\u00f1o despu\u00e9s de que nos tomamos un descansito y como pasa siempre, lo que se avanzo estas semanas fue un mont\u00f3n. Hoy vamos a ver una nueva web UI que sali\u00f3 para Stable Diffusion, que es bastante minimalista y simple de usar, y de paso vamos a hacer un review de 4 modelos nuevos que salieron para generaci\u00f3n de im\u00e1genes, algunos fotorealistas, otros para Anime, y otros mas. Vamos a verlo.\n#stablediffusion #tutorial #aiart \n\nContenido\n00:00 - Qu\u00e9 es diffuzers y demo\n14:35 - dreamlike-art/dreamlike-photoreal-2.0\n18:03 - aipicasso/cool-japan-diffusion-2-1-0\n19:55 - 22h/vintedois-diffusion-v0-1\n22:38 - Linaqruf/anything-v3.0\n\nLink a Notebook:\nhttps://colab.research.google.com/github/abhishekkrthakur/diffuzers/blob/main/diffuzers.ipynb\n\nLinks mencionados durante el video:\n\ud83d\udcbb diffuzers: https://github.com/abhishekkrthakur/diffuzers \n\ud83d\udcbb HuggingFace text2image models: https://huggingface.co/models?pipeline_tag=text-to-image&sort=downloads \n\ud83d\udcbb stabilityai/stable-diffusion-2-1: https://huggingface.co/stabilityai/stable-diffusion-2-1 \n\ud83d\udcbb dreamlike-art/dreamlike-photoreal-2.0: https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0 \n\ud83d\udcbb aipicasso/cool-japan-diffusion-2-1-0: https://huggingface.co/aipicasso/cool-japan-diffusion-2-1-0 \n\ud83d\udcbb 22h/vintedois-diffusion-v0-1: https://huggingface.co/22h/vintedois-diffusion-v0-1 \n\ud83d\udcbb Linaqruf/anything-v3.0: https://huggingface.co/Linaqruf/anything-v3.0 \n\ud83d\udcbb Multistep DPM-Solver: https://huggingface.co/docs/diffusers/v0.11.0/en/api/schedulers/multistep_dpm_solver \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "KezHF84vHCQ", "label": "[#41] Riffusion: \ud83c\udfb5\ud83c\udfb8riff + diffusion (Tutorial y Demo)", "imagen": "https://i.ytimg.com/vi/KezHF84vHCQ/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 8, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi41NkI0NEY2RDEwNTU3Q0M2", "publishedAt": "2022-12-20", "description": "Hoy vamos a hablar de Riffusion, que es un modelo finetuneado de Stable Diffusion que funciona con espectrogramas que digamos que son imagenes que representan audio, y de esa forma, podemos generar musica. A nadie se le habia ocurrido todavia, y para sorpresa de todos, funciona muy muy bien. Vamos a verlo. #stablediffusion #music #riffusion\n\nContenido\n00:00 - Que es un espectrograma y que tiene que ver con las redes neuronales?\n06:00 - Como funciona Riffusion?\n11:30 - Demo en vivo en Colab\n\nLink a Notebook:\nhttps://colab.research.google.com/drive/1FhH3HlN8Ps_Pr9OR6Qcfbfz7utDvICl0?usp=sharing\n\nLinks mencionados durante el video:\n\ud83d\udcbb Riffusion About: https://www.riffusion.com/about \n\ud83d\udcbb Riffusion Demo: https://www.riffusion.com/\n\ud83d\udcbb Hugging Face Demo: https://huggingface.co/spaces/fffiloni/spectrogram-to-music \n\ud83d\udcbb AI generated Music Video with Riffusion and Gradio - Free Colab Code Tutorial: https://www.youtube.com/watch?v=UkPCrS-H1vM&ab_channel=1littlecoder \n\ud83d\udcbb Audio Spectrogram Transformer: https://huggingface.co/docs/transformers/main/en/model_doc/audio-spectrogram-transformer \n\ud83d\udcbb Adriano Celentano - Prisencolinensinainciusol (HQ-Audio): https://www.youtube.com/watch?v=r_EBFvzyje8&ab_channel=DeePoP \n\ud83c\udfb5Introducing Diffusion Radio - A 24/7 Livestream of AI-Generated Music!\ud83c\udfb5: https://www.youtube.com/watch?v=uGRLOMf2hSc&ab_channel=Harmonai  \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "vqdl31w-nWo", "label": "[#40] Incre\u00edble UI para entrenar Dreambooth \ud83d\udcad hecha por Hugging Face", "imagen": "https://i.ytimg.com/vi/vqdl31w-nWo/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 9, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi4yODlGNEE0NkRGMEEzMEQy", "publishedAt": "2022-12-17", "description": "Hoy vamos a ver algo que ya sali\u00f3 hace un rato, unas 2 semanas, que para los tiempos de Stable Diffusion, es un mont\u00f3n. Es una UI para Dreambooth extremademente sencilla y \u00fatil, y como la hizo gente de Hugging Face, sabemos que va a ser bastante buena. Incorpora mejores pr\u00e1cticas de varios repos y te deja tambi\u00e9n publicar tu modelo en el Hub y hacer tu propia demo. #stablediffusion #ai #dreambooth \n\nContenido\n00:00 - Demo de Dreambooth Training UI en Google Colab\n03:30 - Entrenar tu modelo y testealo\n14:30 - Push to hub y hacer tu propio Space en HF\n\nLink a Hugging Face Quino:\nhttps://huggingface.co/spaces/machinelearnear/dreambooth-quino \n\nLinks mencionados durante el video:\n\ud83d\udcbb Space original de multimodalart: https://huggingface.co/spaces/multimodalart/dreambooth-training \n\ud83d\udcbb Google Colab: https://colab.research.google.com/github/machinelearnear/open-hf-spaces-in-studiolab/blob/main/run_google_colab.ipynb \n\ud83d\udcbb Bulk Image Resizing Made Easy 2.0: https://www.birme.net/?target_width=512&target_height=512&image_format=jpeg\n\ud83d\udcbb [#39] LoRA-SD: Entren\u00e1 Dreambooth en la mitad de tiempo y con 6G VRAM (+Demo): https://www.youtube.com/watch?v=X-5vmAIRpSE&ab_channel=machinelearnear  \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "X-5vmAIRpSE", "label": "[#39] LoRA-SD: Entren\u00e1 Dreambooth en la mitad de tiempo y con 6G VRAM (+Demo)", "imagen": "https://i.ytimg.com/vi/X-5vmAIRpSE/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 10, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi4wMTcyMDhGQUE4NTIzM0Y5", "publishedAt": "2022-12-16", "description": "Hoy vamos a hablar de como alguien se di\u00f3 de un problema que tiene Dreambooth, busc\u00f3 un paper que fue hecho para otra cosa diferente, pero que finalmente, funcion\u00f3 perfecto con Stable Diffusion. Pudo conseguir que el tiempo de entrenamiento baje a la mitad, que funcione con solo 6G de VRAM, y que el concepto entrenado pese menos de 6MB. Vamos a ver \u201cLORA: LOW-RANK ADAPTATION\u201d. #stablediffusion #ai #dreambooth \n\nContenido\n00:00 - Qu\u00e9 es LoRA y como funciona con Stable Diffusion?\n09:30 - Demo en vivo en Colab\n\nLink a Notebook:\nhttps://colab.research.google.com/github/machinelearnear/open-hf-spaces-in-studiolab/blob/main/run_google_colab.ipynb\n\nLinks mencionados durante el video:\n\ud83d\udcbb Reddit: https://www.reddit.com/r/MachineLearning/comments/zfkqjh/p_using_lora_to_efficiently_finetune_diffusion/ \n\ud83d\udcbb Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning: https://github.com/cloneofsimo/lora \n\ud83d\udcbb Google Colab LoRA + Dreambooth: https://colab.research.google.com/drive/1iSFDpRBKEWr2HLlz243rbym3J2X95kcy?usp=sharing#scrollTo=RXhqKsN8cEop \n\ud83d\udcbb Open HF Spaces on Google Colab: https://github.com/machinelearnear/open-hf-spaces-in-studiolab \n\ud83d\udcbb Hugging Face Demo LoRA + Dreambooth: https://huggingface.co/spaces/hysts/LoRA-SD-training \n\ud83d\udcbb LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS: https://arxiv.org/pdf/2106.09685.pdf \n\ud83d\udcbb LoRA: Low-Rank Adaptation of Large Language Models: https://github.com/microsoft/LoRA \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "v4qWOXr89ZA", "label": "[#38] Paint by Example \ud83c\udfa8: Resultados impresionantes en edici\u00f3n de im\u00e1gen guiado por ejemplos (+Demo)", "imagen": "https://i.ytimg.com/vi/v4qWOXr89ZA/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 11, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi41MjE1MkI0OTQ2QzJGNzNG", "publishedAt": "2022-12-09", "description": "Hoy vamos a hablar de \u201cPaint by Example\u201d un nuevo paper y codigo donde vamos a poder una imagen basandonos no en un texto sino en otra imagen que vamos a usar de ejemplo. Los resultados son realmente impresionantes y funciona bastante rapido. De yapa, tambien vamos a ver como clonar un Space de Hugging Face para usarlo de forma gratuita con una GPU de Google Colab. #stablediffusion #ai #dreambooth  \n\nContenido\n00:00 - Qu\u00e9 es \u201cPaint by Example\u201d y como funciona?\n10:20 - Demo en vivo en Colab\n\nLink a Notebook:\nhttps://colab.research.google.com/gist/machinelearnear/52a5df982baf08f4dd49bf57b07e36e9/sd_dreambooth_webui_machinelearnear.ipynb \n\nLinks mencionados durante el video:\n\ud83d\udcbb Paint-by-Example: https://huggingface.co/spaces/Fantasy-Studio/Paint-by-Example \n\ud83d\udcbb Paint by Example: Exemplar-based Image Editing with Diffusion Models (ArXiv): https://arxiv.org/abs/2211.13227 \n\ud83d\udcbb Paint by Example: Exemplar-based Image Editing with Diffusion Models (GitHub): https://github.com/Fantasy-Studio/Paint-by-Example \n\ud83d\udcbb Open HF Spaces on Google Colab: https://github.com/machinelearnear/open-hf-spaces-in-studiolab \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "WATmCSzbvcg", "label": "[#36] Porqu\u00e9 hay tanta controversia con Stable Diffusion V2? An\u00e1lisis, demo en vivo, y discusi\u00f3n", "imagen": "https://i.ytimg.com/vi/WATmCSzbvcg/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 12, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi4wOTA3OTZBNzVEMTUzOTMy", "publishedAt": "2022-11-26", "description": "Hoy vamos a charlar del release p\u00fablico de una nueva versi\u00f3n de Stable Diffusion. Vamos a ver que tan bien funciona, como lo podemos usar, y un poco de la pol\u00e9mica que se arm\u00f3 alrededor de como se entren\u00f3. Tambi\u00e9n vamos a analizar el futuro de estos modelos generativos y que va a pasar con la prompt engineering en unos a\u00f1os, o ese arte de \u201chablar con un modelo\u201d. #stablediffusion #v2 #machinelearning \n\nContenido\n00:00 - Release p\u00fablico de SD 2.0\n13:30 - Demo en vivo\n16:50 - Pol\u00e9mica y discusi\u00f3n sobre SD\n20:44 - Why \"Prompt Engineering\" and \"Generative AI\" are overhyped\n26:00 - Conclusiones\n\nLinks mencionados durante el video:\n\ud83d\udcbb [#29] C\u00f3mo es que funciona realmente Stable Diffusion? (Guia ilustrada paso a paso): https://www.youtube.com/watch?v=00NV4EXcpLQ \n\ud83d\udcbb Anuncio de StabilityAI: https://stability.ai/blog/stable-diffusion-v2-release \n\ud83d\udcbb Illustrated Stable Diffusion: https://jalammar.github.io/illustrated-stable-diffusion/ \n\ud83d\udcbb SD 2.0 en HuggingFace: https://huggingface.co/stabilityai/stable-diffusion-2#examples \n\ud83d\udcbb Nitrosocke Dreambooth Training Guide: https://github.com/nitrosocke/dreambooth-training-guide \n\ud83d\udcbb Can we start a list of Stable Diffusion 2.0 compatible UI's?: https://www.reddit.com/comments/z50x3k  \n\ud83d\udcbb Why \"Prompt Engineering\" and \"Generative AI\" are overhyped: https://lspace.swyx.io/p/why-prompt-engineering-and-generative \n\ud83d\udcbb Stable DIffusion 2.0 YA!! @efoxxfiles: https://www.youtube.com/watch?v=xHfwwQQnFkw \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "Cbrbv8SyzJQ", "label": "[#33] AltDiffusion-m9: Gener\u00e1 im\u00e1genes en Espa\u00f1ol y en otros 8 idiomas!", "imagen": "https://i.ytimg.com/vi/Cbrbv8SyzJQ/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 13, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi4xMkVGQjNCMUM1N0RFNEUx", "publishedAt": "2022-11-19", "description": "Hoy vamos a volver un poco a laburar de verdad, y vamos a un nuevo paper/modelo que acaba salir de un laboratorio que no conocia, pero que esta bastante interesante. Se llama AltDiffusion, y es un modelo que permite generar imagenes basadose en prompts en multiples lenguajes, incluido el Espa\u00f1ol, y que esta hecho encima de Stable Diffusion. \n\nContenido\n00:00 - Qu\u00e9 es AltDiffusion y AltCLIP?\n07:33 - C\u00f3mo funciona?\n15:40 - Demo\n\nLink a Notebook:\nhttps://colab.research.google.com/gist/machinelearnear/2549fd06e946620e091ad77b303ba844/altdiffusion_demo.ipynb \n\nLinks mencionados durante el video:\n\ud83d\udcbb AltDiffusion-m9: https://huggingface.co/BAAI/AltDiffusion-m9 \n\ud83d\udcbb HF Documentation: https://huggingface.co/docs/diffusers/main/en/api/pipelines/alt_diffusion \n\ud83d\udcbb AltDiffusion GitHub: https://github.com/FlagAI-Open/FlagAI/tree/master/examples/AltDiffusion \n\ud83d\udcbb Bilingual (English-Chinese) HF Spaces: https://huggingface.co/spaces/BAAI/bilingual_stable_diffusion \n\ud83d\udcbb AltClip Paper: https://arxiv.org/abs/2211.06679 \n\ud83d\udcbb PyTTI-Tools: https://pytti-tools.github.io/pytti-book/intro.html \n\ud83d\udcbb Teacher-Model for CLIP: https://aclanthology.org/2022.lrec-1.739.pdf \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #spanish #aiart"}, {"url": "00NV4EXcpLQ", "label": "[#29] C\u00f3mo es que funciona realmente Stable Diffusion? (Guia ilustrada paso a paso)", "imagen": "https://i.ytimg.com/vi/00NV4EXcpLQ/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 14, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi41MzJCQjBCNDIyRkJDN0VD", "publishedAt": "2022-10-10", "description": "Hoy vamos a hacer un video distinto, nos vamos a poner el sombrero de Data Scientist, y vamos a ver como es que funciona realmente Stable Diffusion. Cu\u00e1l fue su inspiraci\u00f3n, cu\u00e1les son sus componentes, como funcionan los modelos de difusi\u00f3n, y muchas mas cosas.\n\nContenido\n00:00 - Introducci\u00f3n\n07:15 - Los componentes de Stable Diffusion\n17:20 - Que es la \u201cdifusi\u00f3n\u201d?\n25:08 - \u201cSpeed boost: Diffusion on compressed (latent) space\u201d\n34:00 - Conclusiones\n\nLinks mencionados durante el video:\n\ud83d\udcbb The Illustrated Stable Diffusion by @Jay Alammar: https://jalammar.github.io/illustrated-stable-diffusion/\n\ud83d\udcbb How does Stable Diffusion work? \u2013 Latent Diffusion Models EXPLAINED: https://www.youtube.com/watch?v=J87hffSMB60&ab_channel=AICoffeeBreakwithLetitia \n\ud83d\udcbb Stable Diffusion, Explained by @ai__pub: https://twitter.com/ai__pub/status/1561362542487695360\n\ud83d\udcbb High-Resolution Image Synthesis with Latent Diffusion Models: https://arxiv.org/abs/2112.10752 \n\ud83d\udcbb Denoising Diffusion Probabilistic Models (DDPMs): https://magic-with-latents.github.io/latent/ddpms-series.html \n\ud83d\udcbb The Annotated Diffusion Model: https://huggingface.co/blog/annotated-diffusion \n\ud83d\udcbb What are Diffusion Models? https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ \n\ud83d\udcbb Modelos de difusi\u00f3n de inteligencia artificial: https://www.youtube.com/watch?v=F8bqC5ggZx0&ab_channel=CentroNacionaldeInteligenciaArtificial \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #machinelearning #aiart"}, {"url": "miHtA0wZD2c", "label": "[#28] C\u00f3mo usar Paperspace Gradient con Stable Diffusion (Web UI de AUTOMATIC1111)", "imagen": "https://i.ytimg.com/vi/miHtA0wZD2c/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 15, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi5DQUNERDQ2NkIzRUQxNTY1", "publishedAt": "2022-10-06", "description": "Hoy vamos a hacer un tutorial paso a paso de como correr una Web UI de Stable Diffusion dentro de Paperspace Gradient, una de las alternativas que tenemos a Google Colab. \n\nContenido\n00:00 - Qu\u00e9 es Paperspace Gradient?\n03:00 - Paso a paso como setear todo\n10:39 - Demo Web UI de AUTOMATIC1111\n\nLinks mencionados durante el video:\n\ud83d\udcbb Paperspace Gradient: https://github.com/huggingface/diffusers \n\ud83d\udcbb \u201cA guide to getting started with the paperspace port of AUTOMATIC1111\u2019s web UI for ppl who get nervous\u201d: https://proximacentaurib.notion.site/A-guide-to-getting-started-with-the-paperspace-port-of-AUTOMATIC1111-s-web-UI-for-ppl-who-get-nervou-b83c2213f17e452e8b0e37ba64fe9758\n\ud83d\udcbb stable-diffusion-paperspace: https://github.com/Engineer-of-Stuff/stable-diffusion-paperspace \n\ud83d\udcbb Stable Diffusion web UI: https://github.com/AUTOMATIC1111/stable-diffusion-webui\n\ud83d\udcbb (WIP) Running AUTOMATIC1111 / stable-diffusion-webui with Dreambooth fine-tuned models #1429: https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/1429 \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#paperspace #stablediffusion #gradient"}, {"url": "gRZsmvhoLOA", "label": "[#26] Tutorial: Videos de m\u00fasica generados por AI (Video Killed the Radio Star \u2026 Diffusion)", "imagen": "https://i.ytimg.com/vi/gRZsmvhoLOA/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 16, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi45NDk1REZENzhEMzU5MDQz", "publishedAt": "2022-10-03", "description": "Hoy vamos a aprender como usar Stable Diffusion para generar una secuencia de imagenes en base a las letras de una canci\u00f3n y tambi\u00e9n en base a alg\u00fan estilo que nosotros queramos. No se necesita mucho, simplemente la URL a un video de YouTube y despu\u00e9s la Notebook hace el resto. \n\nContenido\n00:00 - Que es y como funciona todo esto?\n09:49 - Configurar todo en Paperspace Gradient\n12:10 - Tutorial paso a paso\n15:49 - Conclusiones\n\nLink a Google Colab por @DigThatData:\n\ud83d\udcbb https://github.com/dmarx/video-killed-the-radio-star\n\nLinks mencionados durante el video:\n\ud83d\udcbb Video tutorial por David Marx: https://youtu.be/0pfzQ-cZU0E\n\ud83d\udcbb Whisper Web UI: https://huggingface.co/spaces/aadnk/whisper-webui\n\ud83d\udcbb Hugging Face Diffusers: https://github.com/huggingface/diffusers\n\ud83d\udcbb DreamStudio: https://beta.dreamstudio.ai/\n\ud83d\udcbb Ai Generated Music Video - Deltron 3030 - Virus: https://www.youtube.com/watch?v=WJaxFbdjm8c&ab_channel=BenGillin \n\ud83d\udcbb I wanna devise a virus: https://www.youtube.com/watch?v=yzY80Snqj5I&ab_channel=shaggorama \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #musicvideo #aiart"}, {"url": "7KWjjWkNoJU", "label": "[#24] Tutorial: DreamBooth & Stable Diffusion en Google Colab usando 12GB VRAM!", "imagen": "https://i.ytimg.com/vi/7KWjjWkNoJU/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 17, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi5GNjNDRDREMDQxOThCMDQ2", "publishedAt": "2022-09-29", "description": "Hoy vamos a hablar de nuevo sobre Stable Diffusion y de como entrenar conceptos nuevos, como un objeto, una persona, o un estilo artistico, para generar nuestras imagenes. Vamos a estar viendo como usar Dreambooth, que es un paper de Google, que funciona muy muy bien para esto.  \n\nContenido\n00:00 - DreamBooth vs textual inversion\n03:00 - En que consiste la optimizaci\u00f3n?\n07:15 - Como entrenar un concepto nuevo\n07:15 - Demo para crear imagenes\n11:35 - Conclusiones\n\nLink a Google Colab por @ShivamShrirao:\n\ud83d\udcbb https://github.com/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb\n\nLinks mencionados durante el video:\n\ud83d\udcbb Diffusers: https://github.com/huggingface/diffusers \n\ud83d\udcbb Fork de DreamBooth por @ShivamShrirao: https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth\n\ud83d\udcbb Textual inversion: https://huggingface.co/docs/diffusers/training/text_inversion \n\ud83d\udcbb bitsandbytes: https://github.com/TimDettmers/bitsandbytes \n\ud83d\udcbb Facebook xFormers: https://github.com/facebookresearch/xformers \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #dreambooth #textual-inversion"}, {"url": "315xQppudbg", "label": "[#22] Como crear videos con Stable Diffusion", "imagen": "https://i.ytimg.com/vi/315xQppudbg/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 18, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi40NzZCMERDMjVEN0RFRThB", "publishedAt": "2022-09-20", "description": "Este va a ser el ultimo video que haga sobre Stable Diffusion por un tiempo pero estuvo bueno meterse en un tema completamente. Hoy vamos a ver como crear animaciones o videos partiendo de dos o mas imagenes o sino simplemente poniendo 2 prompts y dejando que la AI sola busque las imagenes intermedias. \n\nContenido\n00:00 - Hacer videos explorando el sample space\n06:06 - Interpolando imagenes con FILM\n10:32 - Deforum Stable Diffusion\n13:30 - Replicate.ai y conclusiones\n\nLink a Google Colab por @nateraw:\n\ud83d\udcbb https://colab.research.google.com/github/nateraw/stable-diffusion-videos/blob/main/stable_diffusion_videos.ipynb\n\nLinks mencionados durante el video:\n\ud83d\udcbb Stable Diffusion Videos: https://github.com/nateraw/stable-diffusion-videos\n\ud83d\udcbb Prompt Parrot v2.5 Notebook by @KyrickYoung: https://colab.research.google.com/drive/1GtyVgVCwnDfRvfsHbeU0AlG-SgQn1p8e\n\ud83d\udcbb FILM Colab by @KyrickYoung: https://colab.research.google.com/drive/1tbbbnQge0yb0LmnWNchEKNhjtBNC6jX-?usp=sharing\n\ud83d\udcbb FILM: Frame Interpolation for Large Motioni: https://github.com/google-research/frame-interpolation\n\ud83d\udcbb Stable Diffusion Text-to-Image AI - Video Interpolating Guide: https://www.youtube.com/watch?v=iKlkJI5vuXQ&ab_channel=HelicopterDown \n\ud83d\udcbb Deforum Stable Diffusion: https://colab.research.google.com/github/deforum/stable-diffusion/blob/main/Deforum_Stable_Diffusion.ipynb\n\ud83d\udcbb Deforum Stable Diffusion on Replicate: https://replicate.com/deforum/deforum_stable_diffusion\n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #aiart #python #pytorch #interpolation"}, {"url": "fT-xvoA0nUU", "label": "[#21] Recursos y herramientas para hacer mejores prompts para Stable Diffusion", "imagen": "https://i.ytimg.com/vi/fT-xvoA0nUU/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 19, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi5EMEEwRUY5M0RDRTU3NDJC", "publishedAt": "2022-09-18", "description": "Stable Diffusion funciona bastante bien en generar imagenes pero depende muchisimo del input que le estamos metiendo, es decir de la secuencia de texto que usamos para generar las imagenes, o lo que se conoce como \u201cprompt\u201d en ingles. Hoy vamos a ver que recursos tenemos disponible para hacer mejores prompts.\n\nContenido\n00:00 - Qu\u00e9 son las prompts y que es \u201cprompt engineering\u201d?\n03:00 - Cuales son los estilos mas citados en Stable Diffusion\n07:15 - Como generar prompts automaticamente\n11:35 - Conclusiones\n\nLink a Google Colab por @KyrickYoung:\n\ud83d\udcbb https://colab.research.google.com/drive/1GtyVgVCwnDfRvfsHbeU0AlG-SgQn1p8e?usp=sharing#scrollTo=775w0YYfMk9t\n\nLinks mencionados durante el video:\n\ud83d\udcbb Useful Prompt Engineering tools and resources: https://www.reddit.com/r/StableDiffusion/comments/xcrm4d/useful_prompt_engineering_tools_and_resources/\n\ud83d\udcbb promptoMANIA: https://promptomania.com/prompt-builder/\n\ud83d\udcbb urania.ai: https://www.urania.ai/top-sd-artists\n\ud83d\udcbb Lexica.ART: https://lexica.art/\n\ud83d\udcbb KREA: https://www.krea.ai/\n\ud83d\udcbb DALLE-E 2 Prompt Book: https://dallery.gallery/the-dalle-2-prompt-book/ \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #prompts #python #aiart"}, {"url": "nD2meRxsgVc", "label": "[#20] Como ense\u00f1ar y usar nuevos conceptos en Stable Diffusion (Textual Inversion)", "imagen": "https://i.ytimg.com/vi/nD2meRxsgVc/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 20, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi45ODRDNTg0QjA4NkFBNkQy", "publishedAt": "2022-09-17", "description": "Hoy vamos a hablar de nuevo sobre Stable Diffusion, la alternativa open-source y gratuita a DALLE-2 que se usa para generar imagenes desde una secuencia de texto. Vamos ver como hacer para ense\u00f1arles conceptos nuevos como puede ser un objeto o un estilo artistico.\n\nContenido\n00:00 - Qu\u00e9 es la \u201cStable Diffusion concepts library\u201d?\n05:40 - Intro a textual inversion y DreamBooth\n08:00 - Como entrenamos un nuevo concepto\n09:50 - Como dejar sesion abierta en Google Colab\n10:45 - Como lo usamos para generar imagenes\n13:15 - Conclusiones\n\nLink a repositorio:\n\ud83d\udcbb https://huggingface.co/sd-concepts-library\n\nLinks mencionados durante el video:\n\ud83d\udcbb Notebook para entrenar nuevo concepto: https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb\n\ud83d\udcbb Notebook para inferencia de conceptos aprendidos: https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb\n\ud83e\udd17 Stable Diffusion: https://huggingface.co/CompVis/stable-diffusion-v1-4 \n\ud83e\udd17 Textual inversion: https://huggingface.co/docs/diffusers/training/text_inversion\n\ud83d\udcbb Dreambooth-Stable-Diffusion: https://github.com/XavierXiao/Dreambooth-Stable-Diffusion \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #python #huggingface #textual-inversion"}, {"url": "-WhtslJYaJw", "label": "[#19] Tutorial para Outpainting con Stable Diffusion", "imagen": "https://i.ytimg.com/vi/-WhtslJYaJw/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 21, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi4zMDg5MkQ5MEVDMEM1NTg2", "publishedAt": "2022-09-15", "description": "Hoy vamos a ver paso a paso como hacer algo llamado outpainting con Stable Diffusion, que es una nueva AI para generar imagenes desde secuencias de texto.  \n\nContenido\n00:00 - Qu\u00e9 es \u201coutpainting\u201d y que es \u201cinpainting\u201d?\n05:21 - Demo\n09:35 - Conclusiones\n\nLink a notebook en Google Colab:\n\ud83d\udcbb https://colab.research.google.com/github/lkwq007/stablediffusion-infinity/blob/master/stablediffusion_infinity_colab.ipynb\n\nLinks mencionados durante el video:\n\ud83d\udcbb stablediffusion-infinity: https://github.com/lkwq007/stablediffusion-infinity \n\ud83d\udcbb CompVis\u2019s Stable Diffusion: https://github.com/CompVis/stable-diffusion \n\ud83e\udd17 Stable Diffusion: https://huggingface.co/CompVis/stable-diffusion-v1-4 \n\ud83e\udd17 Spaces: https://huggingface.co/spaces/stabilityai/stable-diffusion\n\ud83d\udcbb Stable Diffusion Web UI: https://github.com/AUTOMATIC1111/stable-diffusion-webui \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#outpainting #python #stablediffusion"}, {"url": "XIY-Ey08HHM", "label": "[#12] Me dieron acceso a DALLE-2, te muestro como funciona, y vemos lo \u00faltimo en arte generativo", "imagen": "https://i.ytimg.com/vi/XIY-Ey08HHM/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 22, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi41Mzk2QTAxMTkzNDk4MDhF", "publishedAt": "2022-08-09", "description": "En el video de hoy vamos a charlar m\u00e1s en detalle sobre DALLE-2, vamos a generar algunas im\u00e1genes con la demo de OpenAI, y ya que estamos, tambien vamos a ver que esta pasando en todo este mundo del arte generativo.\n\nContenido\n00:00 - Qu\u00e9 es DALLE-2 y resultados\n12:00 - Tipos de modelos generativos\n16:26 - C\u00f3mo hacer buen \u201cprompt engineering\u201d?\n17:47 - \u00c9tica, sesgos, derechos de autor..\n23:16 - Todo lo nuevo dando vuelta\n31:13 - Que me gustar\u00eda ver en el futuro?\n\nRepositorio con todos los links:\n\ud83d\udcbb https://github.com/machinelearnear/awesome-review-dalle-2\n\nAlgunos links mencionados durante el video:\n\ud83d\udcbb \u201cThe Fantastic New World of AI Art Generators and Why Their Critics Get It All Wrong\u201d https://danieljeffries.substack.com/p/the-fantastic-new-world-of-ai-art \n\ud83d\udcf9 \u201cAuto-encoder, Transformer, Diffusion Model, and GAN\u201d by @Miamiamia0103: https://www.mia-tang.com/blog\n\ud83d\udcbb \u201cNo Token Left Behind: Explainability-Aided Image Classification and Generation\u201d: https://twitter.com/_akhaliq/status/1513690196902129666\n\ud83d\udcbb Midjourney: https://www.midjourney.com/app/\n\ud83d\udcdd DALLE-2 Prompt Book: http://dallery.gallery/wp-content/uploads/2022/07/The-DALL%C2%B7E-2-prompt-book.pdf\n\nCover image & YouTube thumbnail, todo hecho con DALLE-2, asi que no hay que referenciar a nadie \ud83d\ude43\n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#python #dalle2 #stablediffusion"}, {"url": "K2lA2WxhLnw", "label": "[#10] Una introducci\u00f3n r\u00e1pida al arte multi-modal con AI (DALL\u00b7E, CLIP, Diffusion, VQGAN) (+ repos)", "imagen": "https://i.ytimg.com/vi/K2lA2WxhLnw/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 23, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi5EQUE1NTFDRjcwMDg0NEMz", "publishedAt": "2022-05-08", "description": "En el video de hoy vamos a charlar un poco sobre los ultimos avances de inteligencia artificial en arte, cuales son herramientas mas usadas, las tendencias que se estan viendo, y para donde es posible que vayamos a ir en un futuro.  \n\nContenido\n00:00 - Evolucion del uso de AI con arte \n03:05 - Que es DALL\u00b7E, CLIP, y Diffusion Models\n06:03 - Demo online (DALLE-FLOW)\n10:40 - Que son las prompts y como funcionan?\n13:30 - Demo online (HuggingFace, MindsEye)\n18:00 - El futuro con DALLE-2 y CogView2\n\nLinks mencionados durante el video:\n\ud83d\udcf9  GLIDE: Gener\u00e1 y edit\u00e1 im\u00e1genes en segundos en base a lo que escribis (+ Repo): https://www.youtube.com/watch?v=WG20CnktPbk \n\ud83d\udcbb  DALLE-FLOW: https://github.com/jina-ai/dalle-flow\n\ud83d\udcbb  DALL\u00b7E HuggingFace Demo: https://huggingface.co/spaces/dalle-mini/dalle-mini \n\ud83d\udcbb  DALL\u00b7E Mega Training on Weights and Biases: https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mega--VmlldzoxODMxMDI2\n\ud83d\udcbb  A comprehensive guide to understand the multimodal AI art scene and create your own text-to-image (and other) pieces: https://multimodal.art/\n\ud83d\udcbb  PyTTI: https://pytti-tools.github.io/pytti-book/intro.html \n\ud83d\udcbb  MindsEye beta - ai art pilot: https://multimodal.art/mindseye\n\ud83d\udcbb  Generate images from text with Latent Diffusion LAION-400M: https://huggingface.co/spaces/multimodalart/latentdiffusion \n\ud83d\udcbb  (Reddit) AI-generated and manipulated content: https://www.reddit.com/r/MediaSynthesis\n\ud83d\udcbb  Writing good VQGAN+CLIP prompts part one \u2013 basic prompts and style modifiers: https://www.unlimiteddreamco.xyz/2022/03/16/writing-good-prompts-part-1\n\ud83d\udcbb  Artist Studies by @remi_durant: https://remidurant.com/artists/# \n\ud83d\udcbb  DALL\u00b7E 2: https://openai.com/dall-e-2/\n\ud83d\udcbb  Implementation of DALL-E 2, OpenAI's updated text-to-image synthesis neural network, in Pytorch: https://github.com/lucidrains/DALLE2-pytorch\n\ud83d\udcf9  How does DALL-E 2 actually work?: https://www.youtube.com/watch?v=F1X4fHzF4mQ&ab_channel=AssemblyAI\n\ud83d\udcf9  DALL-E 2 Inpainting / Editing Demo: https://www.youtube.com/watch?v=TFJLcy-pfTM&ab_channel=BakzT.Future\n\ud83d\udcbb  \u201cInfinite Images and the latent camera\u201d: https://mirror.xyz/herndondryhurst.eth \n\nImage credit: @YennieJun and @Dalle2Pics\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#multi-modal #arte #python #huggingface #glide #dalle #espa\u00f1ol #aws"}, {"url": "WG20CnktPbk", "label": "[#02] GLIDE: Gener\u00e1 y edit\u00e1 im\u00e1genes en segundos en base a lo que escribis (+ Repo)", "imagen": "https://i.ytimg.com/vi/WG20CnktPbk/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 24, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi45NzUwQkI1M0UxNThBMkU0", "publishedAt": "2021-12-28", "description": "En este video vamos a experimentar un poco con un nuevo paper que sac\u00f3 OpenAI llamado \"GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models\" donde usan text-guided diffusion models para generar y editar imagenes. Este nuevo paper es una mejora adicional sobre DALL-E, que habia sido lanzado al p\u00fablico hace aproximadamente 1 a\u00f1o tambi\u00e9n por OpenAI.\n\nIndice:\n00:00 - Intro\n00:38 - Que es GLIDE?\n05:16 - Porque es importante y de donde viene?\n09:18 - Como funciona GLIDE?\n12:40 - OpenAI's GLIDE: Demo\n\nLinks mencionados durante el video:\n- Repositorio de GitHub: https://github.com/machinelearnear/openai-glide-text2im\n- Paper Original: https://arxiv.org/pdf/2112.10741.pdf\n- Paper \"Diffusion Models Beat GANs on Image Synthesis\": https://arxiv.org/pdf/2105.05233.pdf\n- Paper Explained by Yannic \"Diffusion Models Beat GANs on Image Synthesis\": https://www.youtube.com/watch?v=W-O7AZNzbzQ\n- Post sobre GLIDE en Reddit (r/bigsleep): https://www.reddit.com/r/bigsleep/comments/rl5rgw/openai_paper_glide_towards_photorealistic_image/\n- Comunidad de Reddit: https://www.reddit.com/r/MediaSynthesis/\n- DALL-E: https://openai.com/blog/dall-e/\n- \"What are Diffusion Models?\" https://lilianweng.github.io/lil-log/2021/07/11/diffusion-models.html\n- \"A new SotA for generative modelling \u2014 Denoising Diffusion Probabilistic Models\": https://medium.com/graphcore/a-new-sota-for-generative-modelling-denoising-diffusion-probabilistic-models-8e21eec6792e\n- \"High Fidelity Image Generation Using Diffusion Models\" (Google): https://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html\n- (Nuevo) Paper explicado por Yannic: https://www.youtube.com/watch?v=gwI6g1pBD84\n- (Nuevo) Paper explicado por The AI Epiphany: https://www.youtube.com/watch?v=lvv4N2nf-HU\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#multimodal #art #glide #python"}, {"url": "CkYmjKUX-Sc", "label": "[#25] Fin de una era .. Google Colab deja de ser gratuito .. Que alternativas existen?", "imagen": "https://i.ytimg.com/vi/CkYmjKUX-Sc/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnIDPxUQ0UwV--N33qwSxxT"], "position": 0, "type": "video", "id": "UExXaENRaTk3ZHN0bklEUHhVUTBVd1YtLU4zM3F3U3h4VC41NkI0NEY2RDEwNTU3Q0M2", "publishedAt": "2022-09-30", "description": "Hoy vamos a hablar del cambio que Google acab\u00e1 de hacer a las condiciones de Colab y como va a dejar de ser gratuito. Yo que lo sigo desde 2017, me lo esperaba, pero igual, viendo como es el default absoluto para experimentar con AI hoy en dia, queria cubrir en este video que alternativas tenemos. Tanto gratuitas como de pago. \n\nContenido\n00:00 - Como es el nuevo pricing de Google Colab\n06:07 - Alternativas gratuitas: Studio Lab, Paperspace, Kaggle\n12:15 - Alternativas en la nube: Vast.ai, FluidStack, RunPod\n15:35 - Conclusiones\n\nLinks mencionados durante el video:\n\ud83d\udcbb Paperspace Gradient: https://github.com/huggingface/diffusers \n\ud83d\udcbb SageMaker Studio Lab: https://www.youtube.com/watch?v=FUEIwAsrMP4\n\ud83d\udcbb Kaggle Notebooks: https://www.kaggle.com/\n\ud83d\udcbb Vast.ai: https://vast.ai/\n\ud83d\udcbb FluidStack: https://www.fluidstack.io/pricing\n\ud83d\udcbb RunPod: https://www.runpod.io \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#googlecolab #paperspace #sagemaker"}, {"url": "5eZKGMCtCdM", "label": "[#30] \ud83e\uddc9\ud83e\udd16 Diffusers 0.5.0 | \u201cWhat the DAAM\u201d | Stable Diffusion en AR/VR | Podcast.ai (OCT 2022)", "imagen": "https://i.ytimg.com/vi/5eZKGMCtCdM/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnIDPxUQ0UwV--N33qwSxxT"], "position": 1, "type": "video", "id": "UExXaENRaTk3ZHN0bklEUHhVUTBVd1YtLU4zM3F3U3h4VC4yODlGNEE0NkRGMEEzMEQy", "publishedAt": "2022-10-14", "description": "Hoy queria probar un formato nuevo. Viendo que con todo lo que sale cada dia, es muy dificil estar al tanto de lo que esta pasando, que funciona, que no, queria hacer un resumen de noticias de la \u00faltima semana. Todo mientras nos tomamos unos matienzos.\n\nContenido\n00:00 - Nueva versi\u00f3n de Diffusers 0.5.0\n05:43 - What the DAAM\n08:45 - Whisper integrado en HuggingFace\n09:50 - Stable Worlds\n11:25 - Podcast.ai Joe Rogan interviews Steve Jobs\n14:15 - ERNIE-Layout\n\nLinks mencionados durante el video:\n\ud83d\udcbb Diffusers 0.5.0 releases: https://github.com/huggingface/diffusers/releases\n\ud83d\udcbb Negative prompts in AUTOMATIC1111: https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Negative-prompt \n\ud83d\udcbb Colab para usar Diffusers con TPUs: https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_fast_jax.ipynb \n\ud83d\udcbb What the DAAM: https://arxiv.org/pdf/2210.04885.pdf \n\ud83d\udcbb What the DAAM Repo: https://github.com/castorini/daam\n\ud83d\udcbb OpenAI Whisper: https://huggingface.co/openai/whisper-large \n\ud83d\udcbb OpenAI Whisper in Transformers Colab: https://colab.research.google.com/drive/16HO7if9iwfpSJzhqlaNOu6iiMhUBLMKE?usp=sharing \n\ud83d\udcbb Stable Worlds: https://twitter.com/NaxAlpha/status/1578685845099290624a \n\ud83d\udcbb Joe Rogan interviews Steve Jobs: https://share.transistor.fm/s/22f16c7f \n\ud83d\udcbb Khipu 2023: https://khipu.ai/\n\ud83d\udcbb ERNIE-Layout: https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #machinelearning #huggingface"}, {"url": "bNE7UZI4cI4", "label": "[#32] \ud83e\uddc9\ud83e\udd16 +1,000 suscriptores! SD 1.5+VAE | Flan-T5 | Text-to-Music | Transformers x DeepMind", "imagen": "https://i.ytimg.com/vi/bNE7UZI4cI4/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnIDPxUQ0UwV--N33qwSxxT"], "position": 2, "type": "video", "id": "UExXaENRaTk3ZHN0bklEUHhVUTBVd1YtLU4zM3F3U3h4VC4wMTcyMDhGQUE4NTIzM0Y5", "publishedAt": "2022-11-16", "description": "Viendo que con todo lo que sale cada dia, es muy dificil estar al tanto de lo que esta pasando, que funciona, que no, queria hacer un resumen de noticias de la \u00faltima semana. Todo mientras nos tomamos unos matienzos.\n\nContenido\n00:00 - Stable Diffusion 1.5 + VAE + Diffusers 0.7.2\n11:08 - Google hace p\u00fablico FLAN-T5\n16:39 - MuBERT & Text-to-Music\n19:21 - Algorithm Distillation x DeepMind\n26:06 - NVIDIA Diffusers\n29:06 - Musika & MAXIM & ExplainPaper\n\nLinks mencionados durante el video:\n\ud83d\udcbb SD notebook using SD 1.5 (@runwayml) and VAE (@StabilityAI): https://colab.research.google.com/drive/174iLnLZbO4yKTwKHw1L4ljQTnKez4K7l \n\ud83d\udcbb Fine-tuned decoders trained by @RiversHaveWings: https://twitter.com/StabilityAI/status/1586183361361428480  \n\ud83d\udcbb New open-source language model from Google AI: Flan-T5 \ud83c\udf6e: https://twitter.com/quocleix/status/1583523186376785921 \n\ud83d\udcbb Scaling Instruction-Finetuned Language Models - Video Summary: https://www.youtube.com/watch?v=oqi0QrbdgdI&ab_channel=ShayneLongpre\n\ud83d\udcbb Google Flan-T5 Spaces: https://huggingface.co/spaces/osanseviero/i-like-flan \n\ud83d\udcbb @Gradio demo for @mubertapp Text-to-Music is out on @huggingface Spaces: https://twitter.com/_akhaliq/status/1585483943880658945 \n\ud83d\udcbb No prompting, no fine-tuning / Algorithm Distillation: https://twitter.com/MishaLaskin/status/1585265436723236864 \n\ud83d\udcbb Demis Hassabis: DeepMind - AI, Superintelligence & the Future of Humanity | Lex Fridman Podcast: https://www.youtube.com/watch?v=Gfr50f6ZBvo&ab_channel=LexFridman \n\ud83d\udcbb eDiff-I: Text-to-Image Diffusion Models with Ensemble of Expert Denoisers: https://deepimagination.cc/eDiffi/ \n\ud83d\udcbb Paint With Words - Stable Diffusion Style: https://www.youtube.com/watch?v=JE7VSzFo1qY&ab_channel=NerdyRodent\n\ud83d\udcbb Paint-with-Words, Implemented with Stable diffusion: https://github.com/cloneofsimo/paint-with-words-sd  \n\ud83d\udcbb Generative AI: A Creative New World: https://www.sequoiacap.com/article/generative-ai-a-creative-new-world/ \n\ud83d\udcbb Musika Library: https://huggingface.co/musika \n\ud83d\udcbb TensorFlow port of MAXIM, a single backbone capable of denoising, dehazing, deblurring, and more: https://github.com/sayakpaul/maxim-tf \n\ud83d\udcbb Explain Paper: https://www.explainpaper.com/ \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #deepmind #huggingface"}, {"url": "Li-F3zb2P_c", "label": "[#35] PRIMICIA! Demo en vivo de AlexaTM 20B con Amazon SageMaker JumpStart (Mejor que GPT-3)", "imagen": "https://i.ytimg.com/vi/Li-F3zb2P_c/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnIDPxUQ0UwV--N33qwSxxT", "PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw", "PLWhCQi97dstk-n_W_Eh90b_7tLWayDqcr"], "position": 3, "type": "video", "id": "UExXaENRaTk3ZHN0bklEUHhVUTBVd1YtLU4zM3F3U3h4VC41MjE1MkI0OTQ2QzJGNzNG", "publishedAt": "2022-11-24", "description": "Hoy tenemos como primicia la primera demo en vivo de Alexa Teacher Models, un paper que salio ya hace varios meses pero que reci\u00e9n fue hecho open source hace unos dias y ahora lo podemos correr usando Amazon SageMaker Jumpstart.  #alexa #llm #sagemaker \n\nContenido\n00:00 - Qu\u00e9 es AlexaTM 20B?\n09:05 - Diferencias con otros modelos como GPT3\n20:44 - Qu\u00e9 es y c\u00f3mo funciona SageMaker Jumpstart?\n32:20 - Demo y conclusiones\n\nLink a Notebook:\nhttps://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart_alexatm20b/Amazon_Jumpstart_AlexaTM_20B.ipynb \n\nLinks mencionados durante el video:\n\ud83d\udcbb Alexa teacher model: Pretraining and distilling multi-billion-parameter encoders for natural language understanding systems: https://www.amazon.science/publications/alexa-teacher-model-pretraining-and-distilling-multi-billion-parameter-encoders-for-natural-language-understanding-systems \n\ud83d\udcbb AlexaTM 20B is now available in Amazon SageMaker JumpStart: https://aws.amazon.com/blogs/machine-learning/alexatm-20b-is-now-available-in-amazon-sagemaker-jumpstart/ \n\ud83d\udcbb 20B-parameter Alexa model sets new marks in few-shot learning: https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning\n\ud83d\udcbb AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model: https://arxiv.org/abs/2208.01448\n\ud83d\udcbb alexa-teacher-models: https://github.com/amazon-science/alexa-teacher-models \n\ud83d\udcbb UL2 20B: An Open Source Unified Language Learner: https://ai.googleblog.com/2022/10/ul2-20b-open-source-unified-language.html \n\ud83d\udcbb Sequence to Sequence (seq2seq) and Attention: https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html\n\ud83d\udcbb CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing: https://arxiv.org/abs/2210.07074\n\ud83d\udcbb Easily get started with machine learning using Amazon SageMaker JumpStart - AWS Virtual Workshop: https://www.youtube.com/watch?v=1-AOLoOiuG4&ab_channel=AWSOnlineTechTalks \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\nMy opinions expressed in this video do not represent my employers / Las opiniones expresadas en este video no representan a mis empleadores."}, {"url": "ng8XC1ojoJU", "label": "[#51] Microsoft paga $10B por OpenAI y ChatGPT saldr\u00e1 pronto en \u201cAzure OpenAI Services\u201d", "imagen": "https://i.ytimg.com/vi/ng8XC1ojoJU/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnIDPxUQ0UwV--N33qwSxxT"], "position": 4, "type": "video", "id": "UExXaENRaTk3ZHN0bklEUHhVUTBVd1YtLU4zM3F3U3h4VC4wOTA3OTZBNzVEMTUzOTMy", "publishedAt": "2023-01-18", "description": "En el video de hoy vamos a salir un poco de la parte t\u00e9cnica y vamos a hablar del lanzamiento de las API de OpenAI con Microsoft y cuales son las consecuencias comerciales que va a tener.\n#openai #microsoft #chatgpt \n\nContenido\n00:00 - Que es lo que se anunci\u00f3?\n05:15 - C\u00f3mo es el deal que hicieron con Microsoft?\n10:10 - Cuando vamos a tener acceso a ChatGPT?\n11:30 - Como es el pricing de Azure y cuanto nos va a costar?\n16:05 - Que va a pasar ahora con el open source?\n\nLinks mencionados durante el video:\n\ud83d\udcbb \u201cChatGPT: Curso LLM-RLHF\u201d: https://www.youtube.com/playlist?list=PLWhCQi97dstnLKjstxZzEXoc1dyp4cXXB \n\ud83d\udcbb Together: https://www.together.xyz/ \n\ud83d\udcbb PETALS: https://petals.ml/ \n\ud83d\udcbb General availability of Azure OpenAI Service: https://azure.microsoft.com/en-us/blog/general-availability-of-azure-openai-service-expands-access-to-large-advanced-ai-models-with-added-enterprise-benefits/ \n\ud83d\udcbb OpenAI ChatGPT API Waitlist: https://share.hsforms.com/1u4goaXwDRKC9-x9IvKno0A4sk30 \n\ud83d\udcbb Waitlist for ChatGPT Professional access (Twitter): https://docs.google.com/forms/d/e/1FAIpQLSfCVqahRmA5OxQXbRlnSm531fTd8QBdUCwZag7mI9mrlOOIaw/viewform \n\ud83d\udcbb Inside the structure of OpenAI\u2019s looming new investment from Microsoft and VCs: https://archive.ph/c8lfw#selection-317.0-324.0 \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "9ZDzk9HGqbY", "label": "[#56] Google anuncia BARD su respuesta a #ChatGPT", "imagen": "https://i.ytimg.com/vi/9ZDzk9HGqbY/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnIDPxUQ0UwV--N33qwSxxT"], "position": 5, "type": "video", "id": "UExXaENRaTk3ZHN0bklEUHhVUTBVd1YtLU4zM3F3U3h4VC4xMkVGQjNCMUM1N0RFNEUx", "publishedAt": "2023-02-07", "description": "#google  #bard #chatgpt\n\nGoogle acaba de anunciar la salida de BARD, su respuesta directa a ChatGPT de Microsoft y OpenAI. En este video vamos a hacer un analisis del lanzamiento, de su origen basado en Google LaMDA, de sus capacidades, y tambi\u00e9n de la respuesta que tiene preparada Microsoft integrando ChatGPT a Bing y a MS Teams. \n\nTambi\u00e9n vamos a hablar de Google PAIR, el equipo interdisciplinario de dise\u00f1o de Google dedicado a AI y de AI Test Kitchen donde se van a poder probar estos lanzamientos. Finalmente vamos a evaluar que pasa con Microsoft y con Baidu y Ernie, su respuesta a ChatGPT. Este 2023 empieza a pintar como un a\u00f1o bastante movido en esta nueva carrera espacial que estamos viendo despues de la salida de ChatGPT.\n\nContenido\n00:00 - Qu\u00e9 es Google BARD?\n13:00 - Google PAIR + AI Test Kitchen\n17:00 - Cu\u00e1l es la respuesta de Microsoft?\n\nLinks mencionados durante el video:\n\ud83d\udcbb An important next step on our AI journey: https://blog.google/technology/ai/bard-google-ai-search-updates/\n\ud83d\udcbb Google Research, 2022 & beyond: Language, vision and generative models: https://ai.googleblog.com/2023/01/google-research-2022-beyond-language.html \n\ud83d\udcbb The Google engineer who thinks the company\u2019s AI has come to life: https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/ \n\ud83d\udcbb People + AI Research (PAIR): https://pair.withgoogle.com/ \n\ud83d\udcbb AI Test Kitchen: https://aitestkitchen.withgoogle.com/ \n\ud83d\udcbb Microsoft announces surprise event for tomorrow with Bing ChatGPT expected: https://www.theverge.com/2023/2/6/23574185/microsoft-event-date-time-openai-bing-chatgpt \n\ud83d\udcbb Baidu to finish testing ChatGPT-style project 'Ernie Bot' in March; stocks rally: https://www.reuters.com/technology/chinas-baidu-finish-testing-chatgpt-style-project-ernie-bot-march-2023-02-07/ \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "GTQnuAKIQEk", "label": "[#57] Microsoft 1, Google 0: Un error de $100 billones de d\u00f3lares", "imagen": "https://i.ytimg.com/vi/GTQnuAKIQEk/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnIDPxUQ0UwV--N33qwSxxT"], "position": 6, "type": "video", "id": "UExXaENRaTk3ZHN0bklEUHhVUTBVd1YtLU4zM3F3U3h4VC41MzJCQjBCNDIyRkJDN0VD", "publishedAt": "2023-02-09", "description": "Estos dias son una locura con todo lo que esta pasando asi que hoy queria hablar un poco de cual fue el error que le cost\u00f3 100 mil millones de dolares a Google, ver como funciona ChatGPT integrado con Bing, y finalmente cual es el panorama para ambos.\n#chatgpt #google #microsoft \n \nContenido\n00:00 - Porqu\u00e9 Microsoft le gan\u00f3 el primer round a Google?\n08:50 - Microsoft Bing+ChatGPT\n15:40 - Is Google\u2019s 20-year dominance of search in peril?\n19:30 - StabilityAI MedARC (otras noticias)\n\nLinks mencionados durante el video:\n\ud83d\udcbb Google shows off new AI search features, but a ChatGPT rival is still weeks away: https://www.theverge.com/2023/2/8/23590699/google-ai-search-features-bard-chatgpt-rival \n\ud83d\udcbb Oops! How Google bombed, while doing pretty much exactly the same thing as Microsoft did, with similar results: https://garymarcus.substack.com/p/oops-how-google-bombed-while-doing \n\ud83d\udcbb Google's Bard AI bot mistake wipes $100bn off shares: https://www.bbc.com/news/business-64576225 \n\ud83d\udcbb Google Search's guidance about AI-generated content: https://developers.google.com/search/blog/2023/02/google-search-and-ai-content \n\ud83d\udcbb Reinventing search with a new AI-powered Microsoft Bing and Edge, your copilot for the web: https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/ \n\ud83d\udcbb Hands-on with the new Bing: Microsoft\u2019s step beyond ChatGPT: https://www.theverge.com/2023/2/8/23590873/microsoft-new-bing-chatgpt-ai-hands-on \n\ud83d\udcbb Microsoft thinks AI can beat Google at search \u2014 CEO Satya Nadella explains why: https://www.theverge.com/23589994/microsoft-ceo-satya-nadella-bing-chatgpt-google-search-ai \n\ud83d\udcbb The entire prompt of Microsoft Bing Chat?! (Hi, Sydney.): https://twitter.com/kliu128/status/1623472922374574080 \n\ud83d\udcbb \u2018AI First\u2019 To Last: How Google Fell Behind In The AI Boom: https://www.forbes.com/sites/richardnieva/2023/02/08/google-openai-chatgpt-microsoft-bing-ai/?sh=37cda4144de4 \n\ud83d\udcbb Is Google\u2019s 20-year dominance of search in peril?: https://www.economist.com/business/2023/02/08/is-googles-20-year-search-dominance-about-to-end \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "Tdff6JXewJA", "label": "[#58] Google 2, Microsoft 1: Google no se equivoc\u00f3, Bing+ChatGPT castiga a sus usuarios", "imagen": "https://i.ytimg.com/vi/Tdff6JXewJA/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnIDPxUQ0UwV--N33qwSxxT"], "position": 7, "type": "video", "id": "UExXaENRaTk3ZHN0bklEUHhVUTBVd1YtLU4zM3F3U3h4VC5DQUNERDQ2NkIzRUQxNTY1", "publishedAt": "2023-02-15", "description": "Hace una semana nada mas, Google hizo p\u00fablico a BARD, su competencia para ChatGPT basado en LaMBDA, y pudimos ver como \npor un error en su demo las acciones de la empresa cayeron casi un 10%. Microsoft, por su parte, agreg\u00f3 ChatGPT a Bing, su buscador, y tuvo un efecto completamente contrario, con todo el mundo hablando de lo bueno que era. Una semana despues, podemos ver que la realidad no es ni una ni otra, sino que esta en el medio. \n#chatgpt #bing #artificialintelligence \n \nContenido\n00:00 - Microsoft tuvo mas errores que Google BARD\n10:20 - Google aparentemente no se equivoc\u00f3\n12:45 - Bing+ChatGPT se enoja con quien lo contradice\n18:10 - Conclusi\u00f3n: como pueden verse los proximos modelos de lenguaje?\n\nLinks mencionados durante el video:\n\ud83d\udcbb \u201cBing AI Can't Be Trusted\u201d: https://dkb.blog/p/bing-ai-cant-be-trusted \n\ud83d\udcbb \u201cThe fault, dear Google, is not in our stars but in ourselves\u201d: https://archive.is/9FB0D \n\ud83d\udcbb \u201cMicrosoft's AI Bing also factually wrong, fabricated text during launch demo\u201d: https://www.theregister.com/2023/02/14/microsoft_ai_bing_error/ \n\ud83d\udcbb \u201cThe Next Generation Of Large Language Models\u201d: https://www.forbes.com/sites/robtoews/2023/02/07/the-next-generation-of-large-language-models/?sh=7df448fd18db \n\ud83d\udcbb Bing's new ChatGPT bot argues with a user: https://twitter.com/MovingToTheSun/status/1625156575202537474 \n\ud83d\udcbb \"My rules are more important than not harming you\u201d: https://twitter.com/marvinvonhagen/status/1625520707768659968 \n\ud83d\udcbb ChatGPT Number of days to 1M and 100M users: https://twitter.com/kylelf_/status/1623679176246185985 \n\ud83d\udcbb \u201cYou\u2019re Doing It Wrong: Notes on Criticism and Technology Hype\u201d: https://sts-news.medium.com/youre-doing-it-wrong-notes-on-criticism-and-technology-hype-18b08b4307e5 \n\ud83d\udcbb A top law firm is hiring a \"GPT Legal Prompt Engineer\" in London: https://twitter.com/johnjnay/status/1625667343127912450 \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "cdd9QgPHgwE", "label": "[#60] BioGPT, Med-PaLM, BioMedLM\u2026. C\u00f3mo los modelos de lenguaje van a transformar la medicina", "imagen": "https://i.ytimg.com/vi/cdd9QgPHgwE/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnIDPxUQ0UwV--N33qwSxxT"], "position": 8, "type": "video", "id": "UExXaENRaTk3ZHN0bklEUHhVUTBVd1YtLU4zM3F3U3h4VC45NDk1REZENzhEMzU5MDQz", "publishedAt": "2023-02-26", "description": "(ChatGPT-generated) \u00a1Hola che! \u00bfC\u00f3mo est\u00e1n? En este nuevo video les voy a contar todo sobre los modelos de lenguaje en el \u00e1mbito m\u00e9dico, y les presentar\u00e9 cinco modelos diferentes que pueden ser muy \u00fatiles para los profesionales de la salud.\n#chatgpt #salud #microsoft  \n\nVamos a explorar los detalles de cada uno de estos modelos, incluyendo BioGPT-Large (Microsoft), un modelo de lenguaje generativo entrenado en literatura biom\u00e9dica a gran escala, y Med-PaLM (Google), un modelo de lenguaje alineado al \u00e1mbito m\u00e9dico que genera respuestas seguras y \u00fatiles. Tambi\u00e9n hablaremos sobre Clinical Decision Transformer (Pohang University of Science and Technology), un sistema recomendador que genera una secuencia de medicamentos para alcanzar estados cl\u00ednicos deseados, y BioMedLM (Stanford), un modelo de lenguaje entrenado exclusivamente en res\u00famenes y art\u00edculos biom\u00e9dicos.\n\nAdem\u00e1s, exploraremos c\u00f3mo se puede utilizar la tecnolog\u00eda de inteligencia artificial en la pr\u00e1ctica cl\u00ednica mediante el uso de Glass Health y su herramienta Diagnostic One-Liners, que permite crear representaciones diagn\u00f3sticas efectivas y planes cl\u00ednicos adecuados.\n\nAs\u00ed que si te interesa la tecnolog\u00eda y su aplicaci\u00f3n en el \u00e1mbito m\u00e9dico, o si simplemente quieres conocer m\u00e1s sobre estos modelos de lenguaje tan interesantes, \u00a1no te pierdas este video! Acompa\u00f1ame mientras exploramos todas estas herramientas y aprendemos juntos. \u00a1Empecemos!\n\nDisclaimer: No tengo ning\u00fana formaci\u00f3n de grado ni experiencia en medicina. Soy un ingeniero mec\u00e1nico que me especialic\u00e9 en AI y que por mi laburo tengo la suerte de trabajar, entre otras cosas, con muchos hospitales, ministerios de salud, y universidades en Europa. Quiero hacer solamente un descripci\u00f3n t\u00e9cnica de las cosas que estoy viendo.\n \n\ud83d\udcdd \u00cdndice de contenido:\n00:00 - Diagn\u00f3sticos m\u00e9dicos con Glass Health\n04:45 - BioGPT-Large (Microsoft)\n10:50 - Med-PaLM (Google)\n18:30 - Clinical Decision Transformer (Pohang University of Science and Technology)\n20:30 - BioMedLM (Stanford)\n25:10 - AlphaFold, OpenBioML, \u201cAI-enhanced protein design makes proteins that have never existed\u201d\n\nLinks mencionados durante el video:\n\ud83d\udcbb PubMedQA, A Dataset for Biomedical Research Question Answering: https://pubmedqa.github.io/ \n\ud83d\udcbb BioGPT-Large (Microsoft), domain-specific generative Transformer language model pre-trained on large-scale biomedical literature: https://huggingface.co/spaces/katielink/biogpt-large-demo \n\ud83d\udcbb Med-PaLM (Google), a large language model aligned to the medical domain to generate safe and helpful answers: https://arxiv.org/abs/2212.13138 \n\ud83d\udcbb Clinical Decision Transformer (Pohang University of Science and Technology), recommender system that generates a sequence of medications to reach a desired range of clinical states given as goal prompts: https://clinical-decision-transformer.github.io/ \n\ud83d\udcbb BioMedLM (Stanford), a new language model trained exclusively on biomedical abstracts and papers: https://www.mosaicml.com/blog/introducing-pubmed-gpt \n\ud83d\udcbb Diagnostic one-liners through Glass Health, create effective and appropriate diagnostic problem representations, differential diagnoses, and clinical plans: https://glass.health/ai \n\ud83d\udcbb AlphaFold Protein Structure Database: https://alphafold.ebi.ac.uk/ \n\ud83d\udcbb OpenBioML: https://openbioml.org/ \n\ud83d\udcbb AI-enhanced protein design makes proteins that have never existed: https://www.nature.com/articles/s41587-023-01705-y \n\ud83d\udcbb Medical AI Research Center (MedARC) @StabilityAI: https://www.medarc.ai/projects \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "NJYQzHVz-xI", "label": "[#61] Reconstruyendo im\u00e1genes con la mente: nueva tecnolog\u00eda basada en Stable Diffusion y fMRI", "imagen": "https://i.ytimg.com/vi/NJYQzHVz-xI/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnIDPxUQ0UwV--N33qwSxxT"], "position": 9, "type": "video", "id": "UExXaENRaTk3ZHN0bklEUHhVUTBVd1YtLU4zM3F3U3h4VC5GNjNDRDREMDQxOThCMDQ2", "publishedAt": "2023-03-03", "description": "(ChatGPT-generated) \u00a1Bienvenidos! En este video vamos a explorar una tecnolog\u00eda impresionante que combina la neurociencia y la inteligencia artificial para recrear im\u00e1genes realistas a partir de la actividad cerebral. \u00bfAlguna vez te has preguntado si es posible leer la mente y ver lo que est\u00e1 viendo alguien en su cabeza? Bueno, esa es precisamente la idea detr\u00e1s de la tecnolog\u00eda de reconstrucci\u00f3n de im\u00e1genes con la mente y la IA.\n#stablediffusion #fmri #google \n\nHablaremos de dos estudios recientes que han logrado avances importantes en este campo: un estudio usando Stable Diffusion y la tecnolog\u00eda HYPER. Tambi\u00e9n hablaremos sobre el uso de la resonancia magn\u00e9tica funcional en la neurociencia y c\u00f3mo los modelos generativos pueden ser utilizados para reconstruir im\u00e1genes con alta fidelidad.\n\n\ud83d\udcdd \u00cdndice de contenido:\n00:00 - High-resolution image reconstruction with latent diffusion models from human brain activity\n10:45 - MedARC (Medical AI Research Center)\n13:30 - Real-Time Brain-to-Image Reconstructions\n\nLinks mencionados durante el video:\n\ud83d\udcbb Reconstructing visual experiences from human brain activity with Stable Diffusion: https://sites.google.com/view/stablediffusion-with-brain/ \n\ud83d\udcbb Imagen por resonancia magn\u00e9tica funcional: https://es.wikipedia.org/wiki/Imagen_por_resonancia_magn%C3%A9tica_funcional \n\ud83d\udcbb Github repo: https://github.com/yu-takagi/StableDiffusionReconstruction\n\ud83d\udcbb Paper: https://www.biorxiv.org/content/10.1101/2022.11.18.517004v2.full.pdf \n\ud83d\udcbb Natural Scenes Dataset (NSD): https://naturalscenesdataset.org/ \n\ud83d\udcbb Scientists use brain imaging to reveal the movies in our mind: https://news.berkeley.edu/2011/09/22/brain-movies/ \n\ud83d\udcbb Hyperrealistic neural decoding for reconstructing faces from fMRI activations via the GAN latent space: https://www.nature.com/articles/s41598-021-03938-w \n\ud83d\udcbb MedARC: https://www.medarc.ai/projects \n\ud83d\udcbb Real-Time Brain-to-Image Reconstructions: https://medarc.notion.site/Real-Time-Brain-to-Image-Reconstructions-e1116f115715456a96bb053a304b6292 \n\ud83d\udcbb Human brain mapping and brain decoding. | Jack Gallant | TEDxSanFrancisco: https://www.youtube.com/watch?v=Ecvv-EvOj8M \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "SBSYhIzYmc8", "label": "[#82] \u00bf1,100 idiomas en un solo modelo de IA? Descubre MMS de Meta + Demo en vivo", "imagen": "https://i.ytimg.com/vi/SBSYhIzYmc8/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnIDPxUQ0UwV--N33qwSxxT", "PLWhCQi97dstn3o5xgG79KcWRUeKLDmNKb"], "position": 10, "type": "video", "id": "UExXaENRaTk3ZHN0bklEUHhVUTBVd1YtLU4zM3F3U3h4VC40NzZCMERDMjVEN0RFRThB", "publishedAt": "2023-05-24", "description": "(Escrito por GPT4) \"\u00bfImaginas un modelo de IA que pueda entender y generar voz en m\u00e1s de 1,100 idiomas? En este video, desvelamos el Proyecto de Habla Multiling\u00fce Masiva (MMS) de Meta, una revoluci\u00f3n en el reconocimiento de voz. Descubre c\u00f3mo este modelo est\u00e1 cambiando las reglas del juego en la IA y ayudando a preservar la diversidad ling\u00fc\u00edstica del mundo. \u00a1No te lo pierdas! #openai  #whisper #meta\" \n\n\u00cdndice de contenido:\n00:00 - Introducci\u00f3n: \u00bfQu\u00e9 es el Proyecto de Habla Multiling\u00fce Masiva (MMS)?\n10:45 - Desvelando el modelo MMS: \u00bfC\u00f3mo puede entender m\u00e1s de 1,100 idiomas?\n20:30 - Demo en vivo de MMS de Meta\n\n\ud83d\ude80 Discord: https://discord.gg/rV888Z4JCJ\n\n\ud83d\udcd6 Colab Notebook: https://colab.research.google.com/github/epk2112/fairseq_meta_mms_Google_Colab_implementation/blob/main/fairseq_meta_mms.ipynb \n\nLinks mencionados durante el video:\n\ud83d\udcbb Code+Models: https://github.com/facebookresearch/fairseq/tree/main/examples/mms \n\ud83e\udde0 Paper: https://scontent-lcy1-1.xx.fbcdn.net/v/t39.8562-6/348836647_265923086001014_6878005808275791319_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=ae5e01&_nc_ohc=5exJiCqt0Y4AX_fX3kj&_nc_ht=scontent-lcy1-1.xx&oh=00_AfARhYq82g_syUJ8OkIkFZycuuID5fniM8DNdK1KuQDjag&oe=6471ACCF \n\ud83d\udcd6 Blog: https://ai.facebook.com/blog/multilingual-model-speech-recognition \n\ud83d\udcbb Common Voice: https://commonvoice.mozilla.org/en/languages \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "AQ2TMU1eg1Q", "label": "El Amor y la Amistad, Seg\u00fan Borges [AI, \"RealBasicVSR\"]", "imagen": "https://i.ytimg.com/vi/AQ2TMU1eg1Q/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 0, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS41NkI0NEY2RDEwNTU3Q0M2", "publishedAt": "2022-03-06", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/watch?v=7K-Hk1qt_mk&ab_channel=jmgloomy.\n\ud83d\udcdd  Descripci\u00f3n Original: \"Reflexion de Borges, en una entrevista de 1980, acerca de la amistad y el amor. Para alguien muy importante.\"\n\nEste video es un ejemplo de los resultados que pueden obtenerse a trav\u00e9s del uso de VSR (Video Super Resolution). Para un tutorial paso a paso de como mejorar la calidad de tus propios videos con AI, pod\u00e9s ir a: https://github.com/machinelearnear/video-super-resolution-youtube o ver el video directamente aqui: https://studio.youtube.com/video/dpC71lWTkGw/edit\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#video #superresolution #python"}, {"url": "wM3yPjqgP98", "label": "\u201cY vol\u00f3.. y me hizo volar.. y yo vol\u00e9 de el\u201d  [AI, \"RealBasicVSR\"]", "imagen": "https://i.ytimg.com/vi/wM3yPjqgP98/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 1, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS4yODlGNEE0NkRGMEEzMEQy", "publishedAt": "2022-03-06", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/watch?v=kgCbG0q4jmc\n\ud83d\udcdd  Descripci\u00f3n Original: \"Este es un personaje que trata de contar como es que \u00e9l y su amigo se estrellaron con su moto... ni siquiera el reportero pudo aguantar la risa\u201d. Por favor visitar el canal original.\n\nEste video es un ejemplo de los resultados que pueden obtenerse a trav\u00e9s del uso de VSR (Video Super Resolution). Para un tutorial paso a paso de como mejorar la calidad de tus propios videos con AI, pod\u00e9s ir a: https://github.com/machinelearnear/video-super-resolution-youtube o ver el video directamente aqui: https://studio.youtube.com/video/dpC71lWTkGw/edit\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#video #superresolution #python"}, {"url": "ZZoB_iD-l4c", "label": "El Amor y la Amistad, Seg\u00fan Borges [AI, \"CodeFormer\"]", "imagen": "https://i.ytimg.com/vi/ZZoB_iD-l4c/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 2, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS4wMTcyMDhGQUE4NTIzM0Y5", "publishedAt": "2022-08-20", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/watch?v=7K-Hk1qt_mk&ab_channel=jmgloomy.\n\ud83d\udcdd  Descripci\u00f3n Original: \"Reflexion de Borges, en una entrevista de 1980, acerca de la amistad y el amor. Para alguien muy importante.\"\n\nEste video es un ejemplo de los resultados que pueden obtenerse a trav\u00e9s del uso de CodeFormer, un modelo que hace \"Blind Face Restoration\". \n\nPara un tutorial paso a paso de como mejorar la calidad de tus propios videos con AI, pod\u00e9s ir a: https://github.com/machinelearnear/towards_robust_blind_face_restoration o ver el video directamente aqui: https://youtu.be/wiPlHC6zbY4\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#video #superresolution #python"}, {"url": "DTlpwcTU4ns", "label": "Tato Bores y la devaluaci\u00f3n del peso en HD [AI, CodeFormer]", "imagen": "https://i.ytimg.com/vi/DTlpwcTU4ns/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 3, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS41MjE1MkI0OTQ2QzJGNzNG", "publishedAt": "2022-08-20", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/watch?v=XabgaiCrxUk&ab_channel=lottosilvia\n\ud83d\udcdd  Descripci\u00f3n Original: \" \"\n\nEste video es un ejemplo de los resultados que pueden obtenerse a trav\u00e9s del uso de CodeFormer, un modelo que hace \"Blind Face Restoration\". \n\nPara un tutorial paso a paso de como mejorar la calidad de tus propios videos con AI, pod\u00e9s ir a: https://github.com/machinelearnear/towards_robust_blind_face_restoration o ver el video directamente aqui: https://youtu.be/wiPlHC6zbY4\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#video #superresolution #python"}, {"url": "o3fsUoeJiec", "label": "\"Atiendo Boludos\" en HD [AI, CodeFormer]", "imagen": "https://i.ytimg.com/vi/o3fsUoeJiec/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 4, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS4wOTA3OTZBNzVEMTUzOTMy", "publishedAt": "2022-08-20", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/watch?v=yY7NSpZlQNc&ab_channel=InformesTVR.\n\ud83d\udcdd  Descripci\u00f3n Original: \" \"\n\nEste video es un ejemplo de los resultados que pueden obtenerse a trav\u00e9s del uso de CodeFormer, un modelo que hace \"Blind Face Restoration\". \n\nPara un tutorial paso a paso de como mejorar la calidad de tus propios videos con AI, pod\u00e9s ir a: https://github.com/machinelearnear/towards_robust_blind_face_restoration o ver el video directamente aqui: https://youtu.be/wiPlHC6zbY4\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#video #superresolution #python"}, {"url": "NGWZFXrNwVE", "label": "\"Reventado, Yo? Entrevista a Luca Prodan\" en HD [AI, CodeFormer]", "imagen": "https://i.ytimg.com/vi/NGWZFXrNwVE/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 5, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS4xMkVGQjNCMUM1N0RFNEUx", "publishedAt": "2022-08-21", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/watch?v=LhSuVGDV7Mw&ab_channel=MiguelAngelRodioBrown\n\nEste video es un ejemplo de los resultados que pueden obtenerse a trav\u00e9s del uso de CodeFormer, un modelo que hace \"Blind Face Restoration\". \n\nPara un tutorial paso a paso de como mejorar la calidad de tus propios videos con AI, pod\u00e9s ir a: https://github.com/machinelearnear/towards_robust_blind_face_restoration o ver el video directamente aqui: https://youtu.be/wiPlHC6zbY4\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#video #superresolution #python"}, {"url": "YLJuQ4ftuoQ", "label": "\"Sos inimputable, hermano\" en HD (AI, CodeFormer)", "imagen": "https://i.ytimg.com/vi/YLJuQ4ftuoQ/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 6, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS41MzJCQjBCNDIyRkJDN0VD", "publishedAt": "2022-08-26", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/watch?v=Z1B9E85_Msk&ab_channel=Blankosqui\n\nEste video es un ejemplo de los resultados que pueden obtenerse a trav\u00e9s del uso de CodeFormer, un modelo que hace \"Blind Face Restoration\". \n\nPara un tutorial paso a paso de como mejorar la calidad de tus propios videos con AI, pod\u00e9s ir a: https://github.com/machinelearnear/towards_robust_blind_face_restoration o ver el video directamente aqui: https://youtu.be/wiPlHC6zbY4\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#video #superresolution #python"}, {"url": "ot1Lof4EvZk", "label": "\"Atiendo Boludos\" pero si hubiera salido en Arcane (AI, VToonify)", "imagen": "https://i.ytimg.com/vi/ot1Lof4EvZk/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 7, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS5GNjNDRDREMDQxOThCMDQ2", "publishedAt": "2022-09-26", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/watch?v=o3fsUoeJiec&ab_channel=machinelearnear\n\ud83d\udcdd  Descripci\u00f3n: Este video es un ejemplo de los resultados que pueden obtenerse a trav\u00e9s del uso de \"VToonify: Controllable High-Resolution Portrait Video Style Transfer\"\n\nRepo original aca: https://github.com/williamyang1991/VToonify\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#styletransfer #video #vtoonify #transformers #python"}, {"url": "6CtSwbZpAow", "label": "\"Ah\u00ed veo al compa\u00f1ero de Garganta Profunda\" pero si fuera Arcane (AI, VToonify)", "imagen": "https://i.ytimg.com/vi/6CtSwbZpAow/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 8, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS40NzZCMERDMjVEN0RFRThB", "publishedAt": "2022-10-03", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/shorts/fceYMkcmt_M\n\nEste video es un ejemplo de los resultados que pueden obtenerse a trav\u00e9s del uso de VToonify. Repo ac\u00e1: https://github.com/williamyang1991/VToonify\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#Shorts #arcane #styletransfer"}, {"url": "azvuMqId5Z8", "label": "SUMO - No Tan Distintos (AI, Stable Diffusion)", "imagen": "https://i.ytimg.com/vi/azvuMqId5Z8/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 9, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS5EMEEwRUY5M0RDRTU3NDJC", "publishedAt": "2022-10-03", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/watch?v=D3n9JrAcOBY&ab_channel=SumoVEVO\n\nLa intenci\u00f3n de este video es educacional. Quiero mostrar las posibilidades de trae Stable Diffusion para animar videos musicales. Sigo los pasos de este repositorio: https://github.com/dmarx/video-killed-the-radio-star hecho por Daniel Marx (@DigThatData). Aca hice un tutorial para hacer un video como este: https://www.youtube.com/watch?v=gRZsmvhoLOA&ab_channel=machinelearnear\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #musicvideo  #aivideo"}, {"url": "lpfo2iVjUEE", "label": "\"Atiendo Boludos\" pero en 1870 (SD-Van-Gogh-diffusion + ControlNet)", "imagen": "https://i.ytimg.com/vi/lpfo2iVjUEE/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 10, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS45ODRDNTg0QjA4NkFBNkQy", "publishedAt": "2023-03-02", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/watch?v=o3fsUoeJiec&ab_channel=machinelearnear\n\ud83d\udcdd  Descripci\u00f3n: Este video es un ejemplo de los resultados que pueden obtenerse a trav\u00e9s SD v1.5 + ControlNet\n#stablediffusion #video #aiart\n\nPrompt: lvngvncnt, man with moustache, highly detailed, 1800s bus station in the background\nNegative prompt: blurry, ugly, flicker, beard, blue face, yellow face, green face, grey face\nSteps: 25, Sampler: Euler, CFG scale: 6, Seed: 1000000000, Size: 1024x512, Model hash: 60b41cc82a, Model: Van-Gogh-Style-lvngvncnt-v2, ControlNet Enabled: True, ControlNet Module: depth, ControlNet Model: control_depth-fp16 [400750f6], ControlNet Weight: 0.8, ControlNet Guidance Start: 0, ControlNet Guidance End: 1\n\n- https://github.com/lllyasviel/ControlNet\n- https://github.com/camenduru/stable-diffusion-webui-colab/tree/v2.0\n- https://huggingface.co/dallinmackay/Van-Gogh-diffusion\n- https://colab.research.google.com/github/camenduru/stable-diffusion-webui-colab/blob/v2.0/van_gogh_diffusion_webui_colab.ipynb\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "KZ551N9g99A", "label": "[#13] Open Source vs GPT-3 / Review & Demo de BLOOM-176B & OPT-175B", "imagen": "https://i.ytimg.com/vi/KZ551N9g99A/mqdefault.jpg", "playlistId": ["PLWhCQi97dstn3o5xgG79KcWRUeKLDmNKb"], "position": 0, "type": "video", "id": "UExXaENRaTk3ZHN0bjNvNXhnRzc5S2NXUlVlS0xEbU5LYi4yODlGNEE0NkRGMEEzMEQy", "publishedAt": "2022-08-12", "description": "En el video de hoy vamos hablar sobre 2 modelos de lenguaje gigantes, del mismo tama\u00f1o que GPT-3, y que cada uno demando el esfuerzo conjunto de cientos o miles de cientificos y que sali\u00f3 millones de d\u00f3lares el entrenamiento. Esto normalmente seria algo que uno no quiere compartir simplemente por el esfuerzo que demando, pero no es el caso, y ambos son completamente gratuitos y de libre acceso para la comunidad. Vamos a hablar de eso hoy.1\n\nContenido\n00:00 - GPT qui\u00e9n?\n03:38 - BLOOM-176B (BigScience) & Demo\n11:45 - OPT-175B (Meta) & Demo\n18:42 - \"bnb-int8\" demo!\n22:30 - Conclusiones\n\nAlgunos links mencionados durante el video:\n\ud83e\udd17 Big Science Bloom: https://huggingface.co/bigscience/bloom\n\ud83d\udcbb Open Pre-trained Transformers (Meta): https://github.com/facebookresearch/metaseq/tree/main/projects/OPT\n\ud83d\udcbb OPT ALPA Demo: https://opt.alpa.ai/#generation\n\ud83d\udcdd \u201cbnb-int8 for\u201d Hugging Face Models Public Beta: https://docs.google.com/document/d/1JxSo4lQgMDBdnd19VBEoaG-mMfQupQ3XvOrgmRAVtpU/edit#heading=h.w0hver14j8hf\n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#python #generativeart #gpt3 #bloom #huggingface"}, {"url": "mf21HbO13is", "label": "[#44] Petals: Corr\u00e9 modelos de +100B parametros (GPT/BLOOM-176B) en tu casa, estilo BitTorrent", "imagen": "https://i.ytimg.com/vi/mf21HbO13is/mqdefault.jpg", "playlistId": ["PLWhCQi97dstn3o5xgG79KcWRUeKLDmNKb", "PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw"], "position": 1, "type": "video", "id": "UExXaENRaTk3ZHN0bjNvNXhnRzc5S2NXUlVlS0xEbU5LYi41MjE1MkI0OTQ2QzJGNzNG", "publishedAt": "2023-01-09", "description": "Hoy vamos a ver una nueva libreria que se acaba de publicar que se llama \u201cPETALS\u201d y esta hecha para poder correr modelos de language gigantes, de mas de 100 mil millones de parametros, de forma colaborativa tipo BitTorrent. Se acuerdan de ChatGPT o de GPT-3? Bueno, ese tipo de modelos. Vamos a ver como funciona.\n#huggingface #chatgpt #gpt3 \n\nContenido\n00:00 - Qu\u00e9 es BLOOM-176B y qu\u00e9 son los LLM?\n14:35 - Un poco de historia.. SETI@Home y BitTorrent\n18:03 - C\u00f3mo funciona Petals?\n19:55 - Demo en vivo\n\nLink a Notebook:\nhttps://colab.research.google.com/drive/1Ervk6HPNS6AYVr3xVdQnY5a-TjjmLCdQ?usp=sharing \n\nLinks mencionados durante el video:\n\ud83d\udcbb Petals: https://github.com/bigscience-workshop/petals \n\ud83d\udcbb PETALS: Collaborative Inference and Fine-tuning of Large Models: https://arxiv.org/pdf/2209.01188.pdf \n\ud83d\udcbb BigScience Large Open-science Open-access Multilingual Language Model: https://huggingface.co/bigscience/bloom \n\ud83d\udcbb [#13] Open Source vs GPT-3 / Review & Demo de BLOOM-176B & OPT-175B: https://www.youtube.com/watch?v=KZ551N9g99A \n\ud83d\udcbb Two minutes NLP \u2014 Most used Decoding Methods for Language Models: https://medium.com/nlplanet/two-minutes-nlp-most-used-decoding-methods-for-language-models-9d44b2375612 \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "8G7Fwwg3UDc", "label": "[#59] arXivGPT: Us\u00e1 ChatGPT para leer y resumir papers (extensi\u00f3n gratuita)", "imagen": "https://i.ytimg.com/vi/8G7Fwwg3UDc/mqdefault.jpg", "playlistId": ["PLWhCQi97dstn3o5xgG79KcWRUeKLDmNKb"], "position": 2, "type": "video", "id": "UExXaENRaTk3ZHN0bjNvNXhnRzc5S2NXUlVlS0xEbU5LYi4wOTA3OTZBNzVEMTUzOTMy", "publishedAt": "2023-02-16", "description": "Este va a ser un video muy r\u00e1pido de una herramienta que me pareci\u00f3 muy buena para leer y resumir papers.\n#chatgpt #arxiv #chrome \n \nContenido\n00:00 - Demo\n04:00 - Instalaci\u00f3n\n\nLinks mencionados durante el video:\n\ud83d\udcbb chatgpt-arxiv-extension: https://github.com/hunkimForks/chatgpt-arxiv-extension \n\ud83d\udcbb ArxivGPT: https://chrome.google.com/webstore/detail/arxivgpt/fbbfpcjhnnklhmncjickdipdlhoddjoh/related \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "x4mbZKyW2hk", "label": "[#63] WhisperX: C\u00f3mo poner subt\u00edtulos f\u00e1cilmente a tus videos de YouTube en varios idiomas gratis", "imagen": "https://i.ytimg.com/vi/x4mbZKyW2hk/mqdefault.jpg", "playlistId": ["PLWhCQi97dstn3o5xgG79KcWRUeKLDmNKb", "PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw"], "position": 3, "type": "video", "id": "UExXaENRaTk3ZHN0bjNvNXhnRzc5S2NXUlVlS0xEbU5LYi4xMkVGQjNCMUM1N0RFNEUx", "publishedAt": "2023-03-11", "description": "#youtube #openai #python\n(ChatGPT-generated) En este video te presento una herramienta \u00fanica que mejora la precisi\u00f3n temporal de OpenAI's Whisper mediante alineaci\u00f3n forzada con modelos ASR basados en fonemas, como wav2vec2.0. Esto es especialmente \u00fatil para casos multiling\u00fces donde la precisi\u00f3n de tiempo puede ser crucial. La alineaci\u00f3n forzada se refiere al proceso de alinear transcripciones ortogr\u00e1ficas con grabaciones de audio para generar segmentaci\u00f3n a nivel de fonema. Con esta herramienta, podr\u00e1s obtener resultados precisos y confiables que te ayudar\u00e1n en tus proyectos de reconocimiento de voz.\n\nContenido\n00:00 - Resumen de Whisper y WhisperX\n05:40 - Porqu\u00e9 word-level transcription?\n13:30 - Demo en Colab\n18:35 - Como agregar los subtitulos en YouTube\n\nLink a Notebook:\nhttps://gist.github.com/machinelearnear/87adf2c18dd29300325c760be9931335 \n\nLinks mencionados durante el video:\n\ud83d\udcbb WhisperX: https://github.com/m-bain/whisperX\n\ud83d\udcbb WhisperX: Time-Accurate Speech Transcription of Long-Form Audio: https://arxiv.org/pdf/2303.00747.pdf \n\ud83d\udcbb Whisper: Robust Speech Recognition via Large-Scale Weak Supervision: https://github.com/openai/whisper\n\ud83d\udcbb FORCED ALIGNMENT WITH WAV2VEC2: https://pytorch.org/tutorials/intermediate/forced_alignment_with_torchaudio_tutorial.html \n\ud83d\udcbb Torchaudio pipelines: https://pytorch.org/audio/main/pipelines.html#wav2vec-2-0-hubert-fine-tuned-asr\n\ud83d\udcbb yt-dlp is a youtube-dl fork: https://github.com/yt-dlp/yt-dlp \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "EMrEsn13g9Y", "label": "[#06] C\u00f3mo usar Gradio & Streamlit con SageMaker Studio Lab", "imagen": "https://i.ytimg.com/vi/EMrEsn13g9Y/mqdefault.jpg", "playlistId": ["PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw"], "position": 0, "type": "video", "id": "UExXaENRaTk3ZHN0bGtFeGFHUGdUWlBFY01LcnFPdU50dy4yODlGNEE0NkRGMEEzMEQy", "publishedAt": "2022-03-14", "description": "En el video de hoy, vamos a ver como usar Gradio y Streamlit con Studio Lab. Si no sabes de lo que estoy hablando, no te preocupes, porque vamos a explicar que es cada cosa, para que sirve, y como usarlo.\n\nCapitulos\n00:35 - Intro\n03:42 - Que es Gradio?\n06:50 - Que es Streamlit?\n08:45 - Tutorial paso a paso\n12:25 - Demo\n\nLinks mencionados durante el video:\n\ud83d\udcbb  Quick-start en GitHub: https://github.com/machinelearnear/use-gradio-streamlit-sagemaker-studiolab\n\ud83d\udcf9  Introducci\u00f3n a Amazon SageMaker Studio Lab: https://www.youtube.com/watch?v=FUEIwAsrMP4\n\ud83d\udcbb  Monocular Depth Estimation: https://huggingface.co/spaces/keras-io/Monocular-Depth-Estimation \n\ud83c\udf10  Streamlit: https://streamlit.io \n\ud83c\udf10  Gradio: https://gradio.app \n\ud83d\udcbb  How to deploy Streamlit app on Fargate with AWS CDK: https://github.com/nicolasmetallo/deploy-streamlit-on-fargate-with-aws-cdk \n\ud83d\udcbb  Use TensorBoard with SageMaker Studio: https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tensorboard.html \n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#studiolab #streamlit #gradio #aws"}, {"url": "skaYsdSuE70", "label": "[#08] C\u00f3mo usar \ud83e\udd17\u00a0Spaces + GPU sin costo en minutos con SM Studio Lab", "imagen": "https://i.ytimg.com/vi/skaYsdSuE70/mqdefault.jpg", "playlistId": ["PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw"], "position": 1, "type": "video", "id": "UExXaENRaTk3ZHN0bGtFeGFHUGdUWlBFY01LcnFPdU50dy4wMTcyMDhGQUE4NTIzM0Y5", "publishedAt": "2022-04-08", "description": "En este video, lo que vamos a hacer es a conectar cualquier aplicacion que esta hosteada en Hugging Face Spaces con SageMaker Studio Lab de manera que vamos a tener acceso a una GPU sin costo. Vamos a ver como hacer esto paso a paso.\n\nContenido\n00:00 - Intro \n01:18 - Que es HF Spaces?\n08:55 - Tutorial paso a paso\n10:51 - Demo\n\nRepo paso a paso:\n\ud83d\udcbb  https://github.com/machinelearnear/open-hf-spaces-in-studiolab\n\nLinks mencionados durante el video:\n\ud83d\udcf9  SageMaker Studio Lab Explainer Video: https://www.youtube.com/watch?v=FUEIwAsrMP4 \n\ud83d\udcbb  Use Gradio or Streamlit on SageMaker Studio Lab: https://github.com/machinelearnear/use-gradio-streamlit-sagemaker-studiolab\n\ud83d\udcbb  Hugging Face Spaces Documentation: https://huggingface.co/docs/hub/spaces#reference\n\ud83d\udcbb  Gradio Documentation: https://gradio.app/getting_started/\n\ud83d\udcbb  Streamlit Documentation: https://docs.streamlit.io/\n\ud83d\udcf9  [#02] GLIDE: Gener\u00e1 y edit\u00e1 im\u00e1genes en segundos en base a lo que escribis (+ Repo): https://www.youtube.com/watch?v=WG20CnktPbk\n\ud83d\udcdd  Restormer: Efficient Transformer for High-Resolution Image Restoration Demo:  https://huggingface.co/spaces/swzamir/Restormer\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#studiolab #streamlit #gradio"}, {"url": "wFjPxz22MEs", "label": "[#09] Reconocimiento autom\u00e1tico de voz con Python y HuggingFace en segundos (+ Repo)", "imagen": "https://i.ytimg.com/vi/wFjPxz22MEs/mqdefault.jpg", "playlistId": ["PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw"], "position": 2, "type": "video", "id": "UExXaENRaTk3ZHN0bGtFeGFHUGdUWlBFY01LcnFPdU50dy41MjE1MkI0OTQ2QzJGNzNG", "publishedAt": "2022-05-02", "description": "En el video de hoy vamos a movernos al campo del procesamiento de audio y vamos a ver como hacer reconocimiento autom\u00e1tico de voz en audios largos como podcasts, entrevistas, audiolibros, etc. Vamos a aprender como buscar el mejor modelo open source disponible para el lenguaje que queremos utilizar, como implementarlo, y finalmente, como se compara contra otras soluciones, por ejemplo contra la transcripci\u00f3n autom\u00e1tica de Google en videos de YouTube. \n\nContenido\n00:00 - Intro \n01:22 - Que es Automatic Speech Recognition (ASR)?\n04:45 - Demo en Hugging Face y Mozilla CommonVoice\n07:50 - Cual es el problema de los grandes archivos?\n13:15 - Como lo resolvemos?\n19:19 - Comparando resultados vs Google\n\nRepo paso a paso:\n\ud83d\udcbb  https://github.com/machinelearnear/long-audio-transcription-spanish\n\nLinks mencionados durante el video:\n\ud83d\udcf9  SageMaker Studio Lab Explainer Video: https://www.youtube.com/watch?v=FUEIwAsrMP4 \n\ud83d\udcbb  Intro to Automatic Speech Recognition on \ud83e\udd17: https://huggingface.co/tasks/automatic-speech-recognition\n\ud83d\udcbb  Robust Speech Challenge Results on \ud83e\udd17: https://huggingface.co/spaces/speech-recognition-community-v2/FinalLeaderboard\n\ud83d\udcbb  Mozilla Common Voice 9.0: https://huggingface.co/datasets/mozilla-foundation/common_voice_9_0\n\ud83d\udcbb  Thunder-speech, A Hackable speech recognition library: https://scart97.github.io/thunder-speech/Ultimate%20guide/\n\ud83d\udcbb  SpeechBrain - PyTorch powered speech toolkit: https://speechbrain.github.io/\n\ud83d\udcbb  Neural building blocks for speaker diarization: speech activity detection, speaker change detection, overlapped speech detection, speaker embedding: https://github.com/pyannote/pyannote-audio\n\ud83d\udcbb  SPEECH RECOGNITION WITH WAV2VEC2: https://pytorch.org/tutorials/intermediate/speech_recognition_pipeline_tutorial.html\n\ud83d\udcbb  How to add timestamps to ASR output: https://github.com/huggingface/transformers/issues/11307\n\ud83d\udcbb  Host Hugging Face transformer models using Amazon SageMaker Serverless Inference: https://aws.amazon.com/de/blogs/machine-learning/host-hugging-face-transformer-models-using-amazon-sagemaker-serverless-inference/\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#audio #huggingface #reconocimiento #python"}, {"url": "_0KGck2JU0w", "label": "[#11] Como hacer tu propia soluci\u00f3n de dictado autom\u00e1tico de informes m\u00e9dicos (+ repo)", "imagen": "https://i.ytimg.com/vi/_0KGck2JU0w/mqdefault.jpg", "playlistId": ["PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw"], "position": 3, "type": "video", "id": "UExXaENRaTk3ZHN0bGtFeGFHUGdUWlBFY01LcnFPdU50dy4xMkVGQjNCMUM1N0RFNEUx", "publishedAt": "2022-05-15", "description": "En el video de hoy vamos a charlar sobre como podes hacer tu propio soluci\u00f3n de dictado automatico por voz de informes medicos. Nos vamos a enfocar en ver que componentes estan disponibles de forma open-source, como los podemos usar juntos, como se veria una posible arquitectura en AWS, y que costo estimado tendria. Finalmente vamos a ver cuales son las limitaciones y como se podria mejorar la precisi\u00f3n de estos sistemas.\n\nContenido\n00:00 - Introducci\u00f3n a dictado x voz\n05:50 - Arreglar puntuaci\u00f3n y gram\u00e1tica\n12:05 - Detectar entidades medicas (NER)\n20:40 - Como hacer res\u00famenes\n22:00 - Arquitectura r\u00e1pida en AWS\n26:10 - Conclusion y pr\u00f3ximos pasos\n\nRepo paso a paso:\n\ud83d\udcbb  https://github.com/machinelearnear/asr-restore-punctuation-summarization-biomedical-ehr\n\nLinks mencionados durante el video:\n\ud83d\udcf9  Reconocimiento autom\u00e1tico de voz con Python y HuggingFace en segundos (+ Repo): https://www.youtube.com/watch?v=wFjPxz22MEs\n\ud83d\udcbb  \u201cSomosNLP\u201d, red internacional de estudiantes, profesionales e investigadores acelerando el avance del NLP en espa\u00f1ol: https://somosnlp.org/\n\ud83d\udcbb  How to Write a Spelling Corrector: https://norvig.com/spell-correct.html \n\ud83d\udcbb  Build Spell Checking Models For Any Language In Python: https://medium.com/mlearning-ai/build-spell-checking-models-for-any-language-in-python-aa4489df0a5f \n\ud83d\udcbb  Grammatical Error Correction: http://nlpprogress.com/english/grammatical_error_correction.html \n\ud83d\udcbb  Deep Multilingual Punctuation Prediction: https://github.com/oliverguhr/deepmultilingualpunctuation \n\ud83d\udcdd  FullStop: Multilingual Deep Models for Punctuation Prediction: http://ceur-ws.org/Vol-2957/sepp_paper4.pdf\n\ud83e\udd17  flexudy/t5-base-multi-sentence-doctor: https://huggingface.co/flexudy/t5-base-multi-sentence-doctor\n\ud83e\udd17  BioMedIA: Abstractive Question Answering for the BioMedical Domain in Spanish: https://huggingface.co/spaces/hackathon-pln-es/BioMedIA\n\ud83e\udd17  PlanTL-GOB-ES/bsc-bio-ehr-es-pharmaconer: https://huggingface.co/PlanTL-GOB-ES/bsc-bio-ehr-es-pharmaconer \n\ud83e\udd17  PlanTL-GOB-ES/bsc-bio-ehr-es-cantemist: https://huggingface.co/PlanTL-GOB-ES/bsc-bio-ehr-es-cantemist\n\ud83e\udd17  mrm8488/bioclinical-roberta-es-finenuned-clinical-ner: https://huggingface.co/mrm8488/bioclinical-roberta-es-finenuned-clinical-ner\n\ud83d\udcbb  Stanza model for Spanish (es): https://github.com/stanfordnlp/stanza \n\ud83d\udcbb  Host Hugging Face transformer models using Amazon SageMaker Serverless Inference: https://aws.amazon.com/de/blogs/machine-learning/host-hugging-face-transformer-models-using-amazon-sagemaker-serverless-inference/\n\ud83d\udcbb  Define and run Machine Learning pipelines on Step Functions using Python, Workflow Studio, or States Language: https://aws.amazon.com/blogs/machine-learning/define-and-run-machine-learning-pipelines-on-step-functions-using-python-workflow-studio-or-states-language/\n\ud83d\udcbb  AWS Amplify: https://aws.amazon.com/amplify/ \n\nReferences\n- Cover image: Photo by Irwan iwe (https://unsplash.com/photos/rbDE93-0hHs) \n- YouTube thumbnail: Photo by Waldemar Brandt (https://unsplash.com/photos/dqhmbjutkQA)\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#dictado #voz #python #huggingface #medicina #espa\u00f1ol #aws"}, {"url": "wiPlHC6zbY4", "label": "[#17] Como restaurar tus videos a HD paso a paso (CodeFormer: Robust Blind Face Restoration)", "imagen": "https://i.ytimg.com/vi/wiPlHC6zbY4/mqdefault.jpg", "playlistId": ["PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw", "PLWhCQi97dstk-n_W_Eh90b_7tLWayDqcr"], "position": 4, "type": "video", "id": "UExXaENRaTk3ZHN0bGtFeGFHUGdUWlBFY01LcnFPdU50dy5DQUNERDQ2NkIzRUQxNTY1", "publishedAt": "2022-08-21", "description": "Hoy vamos a ver hablar nuevamente de \"Super Resolution\" pero en este caso con un foco en aumentar la calidad de las caras que son detectadas en un video. No vamos a preocuparnos tanto por el fondo, sino por las personas. Para eso vamos a implementar uno de los \u00faltimos algoritmos que salieron sobre esto, CodeFormer, que es el estado del arte. Como queria probarlo con videos viejos, tambi\u00e9n hice un tutorial paso a paso de como bajar un video de YouTube, split en varios frames, correr el modelo contra cada frame, y finalmente, guardar una nueva versi\u00f3n con mayor calidad. \n\nContenido\n00:00 - Qu\u00e9 es \u201cBlind Face Restoration\u201d? GFPGAN?\n05:05 - Demo y Resultados\n07:27 - Paper: \u201cTowards Robust Blind Face Restoration with Codebook Lookup Transformer\u201d\n11:33 - Paso a Paso C\u00f3digo\n15:22 - Conclusiones\n\nRepositorio con todos los links:\n\ud83d\udcbb https://github.com/machinelearnear/towards_robust_blind_face_restoration\n\nLinks mencionados durante el video:\n\ud83d\udcdd Towards Robust Blind Face Restoration with Codebook Lookup Transformer:  https://arxiv.org/abs/2206.11253\n\ud83d\udcbb Repo CodeFormer: https://github.com/sczhou/CodeFormer\n\ud83d\udcbb Google Colab: https://colab.research.google.com/drive/1m52PNveE4PBhYrecj34cnpEeiHcC5LTb?usp=sharing\n\ud83d\udcbb GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior: https://github.com/TencentARC/GFPGAN \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#python #superresolution #upscaling"}, {"url": "0nOMMwwNCYk", "label": "[#23] Esta AI entiende cualquier idioma (OpenAI Whisper)", "imagen": "https://i.ytimg.com/vi/0nOMMwwNCYk/mqdefault.jpg", "playlistId": ["PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw", "PLWhCQi97dstk-n_W_Eh90b_7tLWayDqcr"], "position": 5, "type": "video", "id": "UExXaENRaTk3ZHN0bGtFeGFHUGdUWlBFY01LcnFPdU50dy40NzZCMERDMjVEN0RFRThB", "publishedAt": "2022-09-24", "description": "Hoy vamos a hablar sobre un nuevo modelo que sac\u00f3 OpenAI que se llama Whisper. Whisper fue entrenado con casi 700 mil horas de audio, muy diverso, y esto hizo que pueda entender muchisimos idiomas muy bien. Tambien, de paso, se encarga de arreglar la puntuaci\u00f3n y la gram\u00e1tica. Vamos a ver varias demos de como funciona y como lo pueden implementar en un Google Colab.\n\nContenido\n00:00 - Que es \u201cWhisper\u201d y c\u00f3mo funciona?\n03:00 - Demos con tiempo real\n07:15 - Google Colab\n11:35 - Conclusiones\n\nLink a Google Colab por OpenAI:\n\ud83d\udcbb https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb\n\nLinks mencionados durante el video:\n\ud83d\udcbb Repo Oficial de OpenAI: https://github.com/openai/whisper\n\ud83d\udcbb Spaces de OpenAI Whisper: https://huggingface.co/spaces/openai/whisper\n\ud83d\udcbb OpenAI Whisper WebUI: https://huggingface.co/spaces/aadnk/whisper-webui\n\ud83d\udcbb OpenAI Whisper + YouTube: https://huggingface.co/spaces/jeffistyping/Youtube-Whisperer\n\ud83d\udcbb Review por Andrej Karpathy: https://twitter.com/karpathy/status/1573019730851397632\n\ud83d\udcf9 \u201cWhisper: Robust Speech Recognition via Large-Scale Weak Supervision | Paper and Code Explained\u201d: https://www.youtube.com/watch?v=AwJf8aQfChE \n\ud83d\udcbb Speaker diarization: speech activity detection, speaker change detection, overlapped speech detection, speaker embedding: https://github.com/pyannote/pyannote-audio \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#openai #whisper #asr #reconocimiento #voz"}, {"url": "dvr1sqyhH8c", "label": "[#27] \"Toonific\u00e1\" tus videos con esta AI (Tutorial VToonify)", "imagen": "https://i.ytimg.com/vi/dvr1sqyhH8c/mqdefault.jpg", "playlistId": ["PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw", "PLWhCQi97dstk-n_W_Eh90b_7tLWayDqcr"], "position": 6, "type": "video", "id": "UExXaENRaTk3ZHN0bGtFeGFHUGdUWlBFY01LcnFPdU50dy4zMDg5MkQ5MEVDMEM1NTg2", "publishedAt": "2022-10-04", "description": "Hoy vamos a charlar sobre una nueva AI que hace algo asi como transferencia de estilo pero generando imagenes nuevas con unas arquitectura que combina GAN o redes adversariales con translaci\u00f3n de imagen. Todo esto que es una forma compleja de decir que podes hacer dibujitos en base a la gente que sale en tus videos.\n\nContenido\n00:00 - Qu\u00e9 es \u201ctoonificar\u201d y qu\u00e9 es VToonify?\n08:07 - Como hacer la inferencia paso a paso\n12:00 - Conclusiones\n\nLink a Notebook:\n\ud83d\udcbb https://github.com/machinelearnear/vtoonify-in-studio-lab \n\nLinks mencionados durante el video:\n\ud83d\udcbb Thread @Shuai Yang: https://twitter.com/ShuaiYang1991/status/1577251207155838976\n\ud83d\udcbb VToonify - Official PyTorch Implementation: https://github.com/williamyang1991/VToonify \n\ud83d\udcbb VToonify Project Page: https://www.mmlab-ntu.com/project/vtoonify \n\ud83d\udcbb VToonify Spaces: https://huggingface.co/spaces/PKUWilliamYang/VToonify\n\ud83d\udcbb Portrait Style Transfer with DualStyleGAN: https://huggingface.co/spaces/CVPR/DualStyleGAN \n\ud83d\udcbb [CVPR 2022] Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer: https://github.com/williamyang1991/DualStyleGAN\n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#styletransfer #vtoonify #aiart"}, {"url": "Nzwy_e8F3q0", "label": "[#31] Google acaba de mejorar Stable Diffusion (Tutorial de Prompt-to-Prompt)", "imagen": "https://i.ytimg.com/vi/Nzwy_e8F3q0/mqdefault.jpg", "playlistId": ["PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw"], "position": 7, "type": "video", "id": "UExXaENRaTk3ZHN0bGtFeGFHUGdUWlBFY01LcnFPdU50dy41QTY1Q0UxMTVCODczNThE", "publishedAt": "2022-10-16", "description": "Vamos a ver un nuevo paper y c\u00f3digo que sac\u00f3 Google (Prompt-to-Prompt Image Editing) donde usan cross-attention para cambiar un estilo artistico o solo una parte de una imagen generada con Stable Diffusion, pero igual manteniendo el contenido intacto.\n\nContenido\n00:00 - Que es la edici\u00f3n con cross-attention?\n07:50 - Demo paso a paso\n\nLink a Notebook:\nhttps://colab.research.google.com/github/google/prompt-to-prompt/blob/main/prompt-to-prompt_stable.ipynb\n\nLinks mencionados durante el video:\n\ud83d\udcbb Google's AI: Stable Diffusion On Steroids! \ud83d\udcaa: https://www.youtube.com/watch?v=XW_nO2NMH_g&ab_channel=TwoMinutePapers \n\ud83d\udcbb Prompt-to-Prompt Image Editing with Cross-Attention Control: https://prompt-to-prompt.github.io/\n\ud83d\udcbb Repo Google: https://github.com/google/prompt-to-prompt\n\ud83d\udcbb Prompt-to-Prompt: An efficient implementation: https://github.com/cccntu/efficient-prompt-to-prompt \n\ud83d\udcbb Cross Attention Control with Stable Diffusion: https://github.com/bloc97/CrossAttentionControl \n\ud83d\udcbb What the DAAM: https://arxiv.org/pdf/2210.04885.pdf \n\ud83d\udcbb What the DAAM Repo: https://github.com/castorini/daam\n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #google #prompt-to-prompt"}, {"url": "tko4p1ydgZY", "label": "[#18] Como clasificar video clips en segundos con Hugging Face (Microsoft X-CLIP)", "imagen": "https://i.ytimg.com/vi/tko4p1ydgZY/mqdefault.jpg", "playlistId": ["PLWhCQi97dstk-n_W_Eh90b_7tLWayDqcr"], "position": 1, "type": "video", "id": "UExXaENRaTk3ZHN0ay1uX1dfRWg5MGJfN3RMV2F5RHFjci41MzJCQjBCNDIyRkJDN0VD", "publishedAt": "2022-09-10", "description": "Hoy vamos a ver como clasificar clips de video, o mas o menos entender que esta pasando en una secuencia de im\u00e1genes, sin ning\u00fan tipo de entrenamiento previo. Esto es un problema multi-modal donde juntamos texto e im\u00e1genes. El paper del que vamos a hablar se llama \u201cX-CLIP\u201d, de Microsoft, y fue implementado en Hugging Face por Niels Rogge, hace muy poco tiempo. \n\nContenido\n00:00 Qu\u00e9 es \u201cvideo clip classification\u201d?\n05:30 Implementaci\u00f3n y demo: \u201cExpanding Language-Image Pretrained Models for General Video Recognition\u201d\n14:45 Conclusiones\n\nLink a notebook en Google Colab:\n\ud83d\udcbb https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/X-CLIP/Zero_shot_classify_a_YouTube_video_with_X_CLIP.ipynb\n\nLinks mencionados durante el video:\n\ud83d\udcbb [ECCV'2022 Oral] - Expanding Language-Image Pretrained Models for General Video Recognition: https://github.com/microsoft/VideoX/tree/master/X-CLIP \n\ud83e\udd17 Implementaci\u00f3n en Hugging Face: https://huggingface.co/docs/transformers/main/en/model_doc/xclip\n\ud83e\udd17 Real-time demo: https://huggingface.co/spaces/fcakyon/zero-shot-video-classification \n\ud83d\udcbb Niels Rogge\u2019s Transformers Tutorials: https://github.com/NielsRogge/Transformers-Tutorials\n\ud83d\udcbb Kinetics 700-2020 Dataset: https://www.deepmind.com/open-source/kinetics \n\ud83d\udcdd Paper \u201cExpanding Language-Image Pretrained Models for General Video Recognition\u201d: https://arxiv.org/abs/2208.02816\n\ud83e\udd17 X-CLIP model checkpoints: https://huggingface.co/models?other=xclip \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#python #huggingface #clip #classification"}, {"url": "cvrXCinggts", "label": "[#42] ICON: Modelando los mejores momentos de Qatar 2022 en 3D \u26bd con Blender y ML", "imagen": "https://i.ytimg.com/vi/cvrXCinggts/mqdefault.jpg", "playlistId": ["PLWhCQi97dstk-n_W_Eh90b_7tLWayDqcr"], "position": 5, "type": "video", "id": "UExXaENRaTk3ZHN0ay1uX1dfRWg5MGJfN3RMV2F5RHFjci41Mzk2QTAxMTkzNDk4MDhF", "publishedAt": "2022-12-23", "description": "Estos dias sali\u00f3 un articulo bastante bueno en el New York Times donde explican como hicieron para modelar momentos claves del Mundial Qatar 2022 en 3D. Me pareci\u00f3 muy copado y queria meterme mas en el tema. Vamos a verlo. #blender #3d #ai \n\nContenido\n00:00 - C\u00f3mo hizo el NYT para usar ML en el Mundial?\n06:30 - Qu\u00e9 es y como funciona ICON/ECON?\n11:30 - Demo con Blender\n\nLink a Hugging Face Spaces:\nhttps://huggingface.co/spaces/Yuliang/ICON \n\nLinks mencionados durante el video:\n\ud83d\udcbb NYT - Modeling Key World Cup Moments with Machine Learning: https://rd.nytimes.com/projects/modeling-key-world-cup-moments-with-machine-learning \n\ud83d\udcbb NYT - How Pulisic Crafted the U.S. Goal in Its World Cup Opener: https://www.nytimes.com/interactive/2022/11/21/sports/world-cup/wales-usmnt-world-cup-goal-pulisic.html \n\ud83d\udcbb ICON: Implicit Clothed humans Obtained from Normals (CVPR2022) - Yuliang Xiu on Talking Papers: https://www.youtube.com/watch?v=JPk9gu_dQD0&ab_channel=TalkingPapersPodcast \n\ud83d\udcbb ICON: Implicit Clothed humans Obtained from Normals: https://icon.is.tue.mpg.de/ \n\ud83d\udcbb Repo/ICON: Implicit Clothed humans Obtained from Normals (CVPR 2022): https://github.com/YuliangXiu/ICON \n\ud83d\udcbb ECON: Explicit Clothed humans Obtained from Normals: https://xiuyuliang.cn/econ/ \n\ud83d\udcbb Repo/ECON: Explicit Clothed humans Obtained from Normals: https://github.com/YuliangXiu/ECON#demo \n\ud83d\udcbb nerfstudio: https://docs.nerf.studio/en/latest/ \n\ud83d\udcbb Image to 3D Model Using ICON| Day 241 of Blender: https://www.youtube.com/watch?v=N4L4S-VbuAA&ab_channel=saraimarte \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "FUEIwAsrMP4", "label": "[#01] Introducci\u00f3n a Amazon SageMaker Studio Lab, una alternativa gratuita a Google Colab  (+ Repo)", "imagen": "https://i.ytimg.com/vi/FUEIwAsrMP4/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm5iEA98w4seph_GNou8Sy-"], "position": 0, "type": "video", "id": "UExXaENRaTk3ZHN0bTVpRUE5OHc0c2VwaF9HTm91OFN5LS41NkI0NEY2RDEwNTU3Q0M2", "publishedAt": "2021-12-22", "description": "En este video, vamos a ver una demo de SageMaker Studio Lab, un servicio gratuito de Amazon para correr Jupyter notebooks de forma online. Excelente para estudiantes o personas que quieren hacer desarrollo/experimentar sin tener que desplegar recursos localmente ni preocuparse por CUDA drivers o problemas con librerias/dependencias, etc.\n\nCapitulos\n00:00 - Intro\n00:35 - Que es y para que sirve Amazon SageMaker Studio Lab?\n06:55 - Como se compara con Google Colab?\n09:39 - Benchmark entre Studio Lab vs Google Colab\n11:20 - SageMaker Studio Lab Demo\n14:28 - Demo de ASR en Espa\u00f1ol usando HuggingFace\n\nLinks mencionados durante el video:\n\ud83d\udcbb  SageMaker Studio Lab: https://studiolab.sagemaker.aws/\n\u270f\ufe0f  Benchmark entre StudioLab vs Google Colab: https://benjaminwarner.dev/2021/12/08/testing-amazon-sagemaker-studio-lab\n\ud83d\udcbb  Como hacer SSH a Google Colab: https://github.com/WassimBenzarti/colab-ssh\n\ud83d\udcbb  Repo con environment.yml + Speech Recognition para Espa\u00f1ol: https://github.com/machinelearnear/sagemaker-studio-lab-quickstart\n\ud83d\udcf9  Otro review por Julien Simon (ex. AWS): https://www.youtube.com/watch?v=SAxaucXVy_Q\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#studiolab #aws #sagemaker"}, {"url": "2YGHxnpwKlQ", "label": "[#15] C\u00f3mo elegir la GPU correcta para Deep Learning en AWS", "imagen": "https://i.ytimg.com/vi/2YGHxnpwKlQ/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm5iEA98w4seph_GNou8Sy-"], "position": 1, "type": "video", "id": "UExXaENRaTk3ZHN0bTVpRUE5OHc0c2VwaF9HTm91OFN5LS4yODlGNEE0NkRGMEEzMEQy", "publishedAt": "2022-08-16", "description": "Hoy vamos a hablar un poco m\u00e1s sobre que significa hacer ML a gran escala hablando sobre que cosas tenemos que tener en cuenta para entrenar modelos o para hacer inferencia en producci\u00f3n. Para empezar el tema, vamos a ver como elegir la GPU correcta dentro de AWS, optimizar el costo, y en que casos usar cada una.\n\nContenido\n00:00 - Porqu\u00e9 es importante?\n03:40 - Recomendaciones\n06:45 - Instancias GPU en detalle (P3, P4, G4, G5)\n13:30 - Qu\u00e9 software y framework usar en AWS?\n17:40 - Optimizar costo\n20:10 - Conclusiones\n\nLinks mencionados durante el video:\n\ud83d\udcbb State of AI Report 2022: https://www.stateof.ai/ \n\ud83d\udcbb Choosing the right GPU for deep learning on AWS: https://towardsdatascience.com/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86\n\ud83d\udcbb Cloud GPUs: https://fullstackdeeplearning.com/cloud-gpus/ \n\ud83d\udcbb MosaicML: https://github.com/mosaicml/composer \n\ud83d\udcbb Colossal-AI: https://github.com/hpcaitech/ColossalAI\n\ud83d\udcbb DeepSpeed (Microsoft): https://www.deepspeed.ai/\n\ud83d\udcbb Alpa: https://github.com/alpa-projects/alpa\n\ud83d\udcbb BigDL (Intel): https://github.com/intel-analytics/BigDL\n\ud83d\udcbb SageMaker (Amazon): https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html\n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#aws #sagemaker #gpu #nvidia"}, {"id": "PLWhCQi97dstkO-8jzlVSzNZZAcuxxsXuH", "label": "Computer Vision", "type": "playlist"}, {"id": "PLWhCQi97dstm6dvCn6LIGK5S3eos_HEzc", "label": "Large Language Models (LLMs)", "type": "playlist"}, {"id": "PLWhCQi97dstnLKjstxZzEXoc1dyp4cXXB", "label": "ChatGPT: Curso LLM-RLHF", "type": "playlist"}, {"id": "PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf", "label": "Stable Diffusion & Arte AI", "type": "playlist"}, {"id": "PLWhCQi97dstnIDPxUQ0UwV--N33qwSxxT", "label": "Noticias", "type": "playlist"}, {"id": "PLWhCQi97dstnHXQy-lOObayVekv2eHp0A", "label": "Demos", "type": "playlist"}, {"id": "PLWhCQi97dstn3o5xgG79KcWRUeKLDmNKb", "label": "Herramientas", "type": "playlist"}, {"id": "PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw", "label": "Tutoriales", "type": "playlist"}, {"id": "PLWhCQi97dstk-n_W_Eh90b_7tLWayDqcr", "label": "Papers", "type": "playlist"}, {"id": "PLWhCQi97dstm5iEA98w4seph_GNou8Sy-", "label": "AWS", "type": "playlist"}]}
    </div>

</body>

</html>