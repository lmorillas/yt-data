<!DOCTYPE html>
<html lang="es">

<head>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0" />
    <title>MACHINELEARNEAR</title>
    <meta name="Description" content="Un lugar para cranear mientras nos tomamos unos amargos">

    <link href="#schema" type="application/json" rel="exhibit-data" />
    <link href="#data" type="application/json" rel="exhibit-data" />
      
    <!-- libs simile-exhibit -->

    <script src="//api.simile-widgets.org/exhibit/current/lib/jquery.min.js" type="text/javascript">
    </script>
    <!-- <link rel="exhibit-extension" href="/js/exhibit/extensions/time/time-extension.js" />
    -->
    <script src="//api.simile-widgets.org/exhibit/current/exhibit-api.js" type="text/javascript"></script>
    
    <!-- Bootstrap -->
    <link rel="stylesheet" href="//cdn.jsdelivr.net/bootstrap/3.3.0/css/bootstrap.min.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/bootstrap/3.3.0/css/bootstrap-theme.min.css">
    <script src="//cdn.jsdelivr.net/bootstrap/3.3.0/js/bootstrap.min.js"></script>

    <script type="text/javascript">  
        $(document).bind("dataload.exhibit", function() {
            $("input").addClass("form-control");
        });

        $(document).bind("scriptsLoaded.exhibit", function () {
            Exhibit.FunctionUtilities.registerSimpleMappingFunction("yearOf",
                function (d) {
                    return d.split('-')[0];
                },
                "number");
        });
    </script>

    <script type="text/javascript">

        function addYear(json) {
            var items = json.items;
            for (var i = 0; item = items[i]; i++) {
                items[i].year = Exhibit.DateTime.parseIso8601DateTime(item.publishedAt).getFullYear();
            }
            return json;
        }
    </script>


    <style>
        #main-content {
            background: white;
        }

        #title-panel {
            padding: 0.25in 0.5in;
        }

        #top-panels {
            padding: 0.5em 0.5in;
            border-top: 1px solid #BCB79E;
            border-bottom: 1px solid #BCB79E;
            background: #FBF4D3;
        }

        .exhibit-tileView-body {
            list-style: none;
        }

        .exhibit-collectionView-group-count {
            display: none;
        }

        table.nobelist {
            border: 1px solid #ddd;
            padding: 0.5em;
        }

        div.name {
            font-weight: bold;
            font-size: 120%;
        }

        .relationship {
            color: #888;
        }

        ddiv.video-thumbnail {
            float: left;
            width: 12vw;
            height: 13em;
            border: 1px solid #BCB79E;
            background: #F0FFF0;
            padding: 1em;
            margin: 0.5em;
            text-align: center;
        }

        div.video-timeline-lens {
            padding: 1em;
            text-align: center;
        }

        .card-content p {
            font-size: 12px;
        }

        .exhibit-thumbnailView-body{
            display: flex;
            flex-wrap: wrap;
            align-items: top;
            justify-content: center;
        }
        .video-thumbnail {
            width: 15vw;
            height: auto;
            border: 1px solid #BCB79E;
            background: #F0FFF0;
            padding: 1em;
            margin: 0.5em;
            text-align: center;
        }
        
        @media (max-width: 768px) {
            h1, .h1 {
                font-size: 24px
            }
            .video-thumbnail {
                width: 90vw;
                height: auto;
                border: 1px solid #BCB79E;
                background: #F0FFF0;
                padding: 1em;
                margin: 0.5em;
                text-align: center;
            }
            .card-content {
                font-size: 8px;
            }
            div.exhibit-facet-body{
                height: 5em;
            }
            div.exhibit-facet-value {
                font-size: 12px;
            }
          }

    </style>



<link rel="apple-touch-icon" sizes="180x180" href="static/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="static/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="static/favicon-16x16.png">
<link rel="manifest" href="site.webmanifest">
<link rel="mask-icon" href="static/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">



</head>

<body>
<header>
    <h1 class="text-center">MACHINELEARNEAR</h1>
    <hr>
</header>
    
        <div data-ex-role="exhibit-collection" data-ex-item-types="video"></div>
        <div class="container">
        <button class="visible-xs"  data-toggle="collapse" data-target="#controles"> <span class="glyphicon glyphicon-sort"></span></button>

        <div class="row collapse in" id="controles">
            <div class="col-md-4" data-ex-role="exhibit-facet" data-ex-facet-class="TextSearch" data-ex-facet-label="Buscar" 
            data-ex-expressions=".label, .playlistId.label"></div>
            <div class="col-md-4" data-ex-role="facet" data-ex-expression="yearOf(.publishedAt)" 
            data-ex-collapsible="true" data-ex-facet-label="Año de publicación"
            data-ex-sort-direction="reverse"></div>
            <div class="col-md-4" data-ex-role="facet" data-ex-expression=".playlistId" data-ex-facet-label="Playlists"
            data-ex-collapsible="true" data-ex-missing-label="Fuera de playlist" ></div>
        </div>
        

        <div data-ex-role="viewPanel" style="padding: 1em 0.5in;">

            <div data-ex-role="view" data-ex-view-class="Thumbnail" data-ex-showall="false" 
            data-ex-orders=".publishedAt, .label" data-ex-grouped="false"
                data-ex-paginate="true" data-ex-page-size="20" 
                data-ex-directions="descending"
                data-ex-show-controls="false" data-ex-possible-orders=".label ">

                <div data-ex-role="exhibit-lens" style="display: none;" class="video-thumbnail">

                    <div>
                        <a target="_blank" data-ex-href-content="concat('https://www.youtube.com/watch?&v=', .url)">
                            <img data-ex-src-content=".imagen" alt="Imagen" class="img-responsive">
                        </a>

                    </div>
                    <div class="card-content">
                        <a target="_blank" data-ex-href-content="concat('https://www.youtube.com/watch?&v=', .url)">
                        <p data-ex-content=".label"></p>
                        </a>
                    </div>


                </div>
            </div>
            <!--
            <div data-ex-role="view" 
                data-ex-view-class="Timeline" 
                data-ex-start=".publishedAt" >
                    <div data-ex-role="lens" class="video-timeline-lens" style="display: none;">
                            <img data-ex-src-content=".imagen"  alt="portrait"/>
                            <div><span data-ex-content=".label"></span></div>
                            
                    </div>

            </div>
            -->
            

        </div>
    

    </div>



    <!--  Scripts-->
    <div id="schema" style="display:none">
        {"types": {"video": {"pluralLabel": "V\u00eddeos"}, "playlist": {"pluralLabel": "Playlists"}}, "properties": {"imagen": {"valueType": "url"}, "playlistId": {"valueType": "item"}, "publishedAt": {"valueType": "date", "label": "Fecha de publicaci\u00f3n"}}}
    </div>
    <div id="data" style="display:none">
            {"items": [{"url": "4uXeflZ8q8w", "label": "[#47] Curso LLM-RLHF (1/n) - Como crear ChatGPT desde 0 explicado por un Data Scientist", "imagen": "https://i.ytimg.com/vi/4uXeflZ8q8w/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnLKjstxZzEXoc1dyp4cXXB"], "position": 0, "type": "video", "id": "UExXaENRaTk3ZHN0bkxLanN0eFp6RVhvYzFkeXA0Y1hYQi41NkI0NEY2RDEwNTU3Q0M2", "publishedAt": "2023-01-14", "description": "Este capitulo de hoy va a ser el primero de una serie de videos que queria hacer para explicar desde 0 como podemos hacer nuestro propio ChatGPT. Vamos a ir de muy simple a muy complejo sin esquivarle a las partes dificiles pero tratando de explicar paso a paso cada cosa.\n#chatgpt #gpt3  #nlp \n\nContenido\n00:00 - Motivaci\u00f3n\n08:20 - Que son los LLM y qu\u00e9 es GPT-3?\n21:00 - C\u00f3mo es el mercado de las LLM, donde estamos?\n29:05 - Sparrow (DeepMind) vs ChatGPT. Otras alternativas.\n45:20 - Qu\u00e9 pasos tenemos que hacer para entrenar nuestro ChatGPT\n\nLinks mencionados durante el video:\n\ud83d\udcbb \u201c[#37] Chat GPT: Vemos como funciona y porqu\u00e9 Google, \u00bfest\u00e1 en problemas?\u201d: https://www.youtube.com/watch?v=DqsgYlp6EmI \n\ud83d\udcbb State of AI Report 2022: https://www.stateof.ai/ \n\ud83d\udcbb Curso de LLM por Stanford: https://stanford-cs324.github.io/winter2022/ \n\ud83d\udcbb Major LLMs + Data Availability: https://docs.google.com/spreadsheets/d/1bmpDdLZxvTCleLGVPgzoMTQ0iDP2-7v7QziPrzPdHyM/edit#gid=0 \n\ud83d\udcbb Generative AI Startups: https://airtable.com/shrwmMVRmBb1N986x/tbl78Ow3go1TaVBpm \n\ud83d\udcbb OpenAssistant: https://github.com/LAION-AI/Open-Assistant \n\ud83d\udcbb How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources: https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1 \n\ud83d\udcbb nanoGPT: https://github.com/karpathy/nanoGPT \n\ud83d\udcbb Neural Networks: Zero to Hero: https://karpathy.ai/zero-to-hero.html \n\ud83d\udcbb ChatGPT: https://chat.openai.com/chat \n\ud83d\udcbb \"ChatGPT vs Sparrow - Battle of Chatbots\u201d: https://www.youtube.com/watch?v=SWwQ3k-DWyo \n\ud83d\udcbb Large Language Models are Zero-Shot Reasoners: https://arxiv.org/pdf/2205.11916.pdf \n\ud83d\udcbb Training language models to follow instructions with human feedback: https://arxiv.org/abs/2203.02155 \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "DqsgYlp6EmI", "label": "[#37] Chat GPT: Vemos como funciona y porqu\u00e9 Google, \u00bfest\u00e1 en problemas?", "imagen": "https://i.ytimg.com/vi/DqsgYlp6EmI/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnLKjstxZzEXoc1dyp4cXXB"], "position": 1, "type": "video", "id": "UExXaENRaTk3ZHN0bkxLanN0eFp6RVhvYzFkeXA0Y1hYQi4yODlGNEE0NkRGMEEzMEQy", "publishedAt": "2022-12-03", "description": "Hoy vamos a hablar sobre el \u00faltimo release de OpenAI que se llama ChatGPT y es realmente, para mi, algo revolucionario a nivel experiencia de usuario de como interactuamos con grandes modelos de lenguaje. Vamos a ver como es que funciona tecnicamente y tambi\u00e9n que podemos esperar en un futuro. #openai #chatgpt #ai  \n\nContenido\n00:00 - Qu\u00e9 es ChatGPT?\n08:49 - Demo en vivo\n09:05 - Como funciona?\n29:30 - Reinforcement Learning from Human Feedback (RLHF)\n40:16 - Conclusiones\n\nLinks mencionados durante el video:\n\ud83d\udcbb ChatGPT (OpenAI): https://chat.openai.com/chat \n\ud83d\udcbb ChatGPT Blog Post: https://openai.com/blog/chatgpt/ \n\ud83d\udcbb InstructGPT: https://openai.com/blog/instruction-following/ \n\ud83d\udcbb Reinforcement Learning from Human Feedback (RLHF): https://www.lesswrong.com/posts/rQH4gRmPMJyjtMpTn/rlhf \n\ud83d\udcbb anthropic: https://www.anthropic.com/\n\ud83d\udcbb Repo: \"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\": https://github.com/anthropics/hh-rlhf \n\ud83d\udcbb Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback: https://arxiv.org/pdf/2204.05862.pdf \n\ud83d\udcbb Learning to summarise from human feedback (Paper Explained): https://www.youtube.com/watch?v=vLTmnaMpQCs \n\ud83d\udcbb Learning Task Specifications for Reinforcement Learning from Human Feedback | David Lindner: https://www.youtube.com/watch?v=vebzz6EKD2w \n\ud83d\udcbb OpenAI's InstructGPT: Aligning Language Models with Human Intent: https://www.youtube.com/watch?v=QGpaBWOaHQI \n\ud83d\udcbb #29 - OpenAI\u2019s InstructGPT is a Game Changer!: https://www.youtube.com/watch?v=CqPHoeddA7k \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "TYFy0wZpJuY", "label": "[#48] Curso LLM-RLHF (2/n) - Habilidades emergentes de GPT-3.5 explicado por un Data Scientist", "imagen": "https://i.ytimg.com/vi/TYFy0wZpJuY/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnLKjstxZzEXoc1dyp4cXXB"], "position": 2, "type": "video", "id": "UExXaENRaTk3ZHN0bkxLanN0eFp6RVhvYzFkeXA0Y1hYQi4wMTcyMDhGQUE4NTIzM0Y5", "publishedAt": "2023-01-15", "description": "Este video es el segundo de una serie que voy a hacer sobre como podemos entrenar ChatGPT. Como no es algo f\u00e1cil vamos a ir viendo todo paso a paso y yendo desde lo mas simple a lo mas complejo sin esquivarle a las partes dificiles pero tratando de explicar cada cosa. Hoy vamos a hablar de GPT-3.5 y sus habilidades emergentes.\n#chatgpt #gpt3  #nlp \n\nContenido\n00:00 - Motivaci\u00f3n e introducci\u00f3n\n12:30 - GPT-3 en el 2020\n17:50 - De 2020 a 2022 GPT-3\n21:30 - Entrenar con c\u00f3digo, tunear con instrucciones\n29:50 - Reinforcement Learning con Human Feedback\n34:00 - Resumen, limitaciones, y conclusiones\n\nLinks mencionados durante el video:\n\ud83d\udcbb \u201c[#37] Chat GPT: Vemos como funciona y porqu\u00e9 Google, \u00bfest\u00e1 en problemas?\u201d: https://www.youtube.com/watch?v=DqsgYlp6EmI \n\ud83d\udcbb State of AI Report 2022: https://www.stateof.ai/ \n\ud83d\udcbb Curso de LLM por Stanford: https://stanford-cs324.github.io/winter2022/ \n\ud83d\udcbb Major LLMs + Data Availability: https://docs.google.com/spreadsheets/d/1bmpDdLZxvTCleLGVPgzoMTQ0iDP2-7v7QziPrzPdHyM/edit#gid=0 \n\ud83d\udcbb Generative AI Startups: https://airtable.com/shrwmMVRmBb1N986x/tbl78Ow3go1TaVBpm \n\ud83d\udcbb OpenAssistant: https://github.com/LAION-AI/Open-Assistant \n\ud83d\udcbb How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources: https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1 \n\ud83d\udcbb nanoGPT: https://github.com/karpathy/nanoGPT \n\ud83d\udcbb Neural Networks: Zero to Hero: https://karpathy.ai/zero-to-hero.html \n\ud83d\udcbb ChatGPT: https://chat.openai.com/chat \n\ud83d\udcbb \"ChatGPT vs Sparrow - Battle of Chatbots\u201d: https://www.youtube.com/watch?v=SWwQ3k-DWyo \n\ud83d\udcbb Large Language Models are Zero-Shot Reasoners: https://arxiv.org/pdf/2205.11916.pdf \n\ud83d\udcbb Training language models to follow instructions with human feedback: https://arxiv.org/abs/2203.02155 \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "tzPuVAJ3XoI", "label": "[#49] Curso LLM-RLHF (3/n) - Reinforcement Learning from Human Feedback explicado por Data Scientist", "imagen": "https://i.ytimg.com/vi/tzPuVAJ3XoI/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnLKjstxZzEXoc1dyp4cXXB"], "position": 3, "type": "video", "id": "UExXaENRaTk3ZHN0bkxLanN0eFp6RVhvYzFkeXA0Y1hYQi41MjE1MkI0OTQ2QzJGNzNG", "publishedAt": "2023-01-17", "description": "Este va a ser el tercer video de una serie que estoy haciendo sobre modelos de lenguaje gigantes con el objetivo de poder aprender lo necesario para hacer una versi\u00f3n abierta de ChatGPT. Como no es algo facil y esto va cambiando semana a semana, vamos a ir viendo todo paso a paso yendo desde lo mas f\u00e1cil a lo mas complejo sin esquivarle a lo dificil pero tratando de explicar todo. Hoy vamos a hablar de Reinforcement Learning from Human Feedback.\n#chatgpt #rlhf #gpt3 \n\nContenido\n00:00 - Motivaci\u00f3n\n04:30 - Background sobre Foundation Models (LLM)\n13:50 - OpenAI InstructGPT, DeepMind Sparrow, Anthropic RLHF\n21:30 - Conceptos detr\u00e1s de RLFH\n33:00 - Modelo de recompensa y politica de optimizaci\u00f3n\n44:00 - Conclusiones\n\nLinks mencionados durante el video:\n\ud83d\udcbb \u201cChatGPT: Curso LLM-RLHF\u201d: https://www.youtube.com/playlist?list=PLWhCQi97dstnLKjstxZzEXoc1dyp4cXXB \n\ud83d\udcbb Lecture 07: Foundation Models (FSDL 2022): https://www.youtube.com/watch?v=Rm11UeGwGgk&ab_channel=FullStackDeepLearning \n\ud83d\udcbb Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback: https://github.com/anthropics/hh-rlhf \n\ud83d\udcbb Illustrating Reinforcement Learning from Human Feedback (RLHF): https://huggingface.co/blog/rlhf \n\ud83d\udcbb Reinforcement Learning from Human Feedback: From Zero to chatGPT: https://www.youtube.com/watch?v=2MBJOuVq380&ab_channel=HuggingFace \n\ud83d\udcbb \u201cChatGPT and Reinforcement Learning\u201d: https://www.youtube.com/watch?v=_MPJ3CyDokU&ab_channel=CodeEmporium \n\ud83d\udcbb Closed-API vs Open-source continues: RLHF, ChatGPT, data moats: https://robotic.substack.com/p/rlhf-chatgpt-data-moats \n\ud83d\udcbb Repo for distributed training of language models with Reinforcement Learning via Human Feedback (RLHF): https://github.com/CarperAI/trlx \n\ud83d\udcbb OpenAI's InstructGPT: Aligning Language Models with Human Intent: https://www.youtube.com/watch?v=QGpaBWOaHQI&t=719s&ab_channel=ScaleAI \n\ud83d\udcbb Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM: https://github.com/lucidrains/PaLM-rlhf-pytorch \n\ud83d\udcbb Here\u2019s What I Saw at an AI Hackathon: https://every.to/superorganizers/the-knee-of-the-exponential-curve \n\ud83d\udcbb 6 New Theories About AI: https://every.to/napkin-math/6-new-theories-about-ai \n\ud83d\udcbb \u201cwhat actually happens when we type inside the ChatGPT textbox\u201d: https://twitter.com/vboykis/status/1601930057076903936 \n\ud83d\udcbb Simulators: https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators \n\ud83d\udcbb What is Reinforcement Learning with Human Feedback (RLHF)? https://aisupremacy.substack.com/p/what-is-reinforcement-learning-with?utm_source=twitter&utm_campaign=auto_share&r=cxsa3 \n\ud83d\udcbb Take 9: No, RLHF/IDA/debate doesn't solve outer alignment: https://www.lesswrong.com/posts/6YNZt5xbBT5dJXknC/take-9-no-rlhf-ida-debate-doesn-t-solve-outer-alignment \n\ud83d\udcbb Understanding Reinforcement Learning from Human Feedback (RLHF): Part 1: https://wandb.ai/ayush-thakur/RLHF/reports/Understanding-Reinforcement-Learning-from-Human-Feedback-RLHF-Part-1--VmlldzoyODk5MTIx \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "TMYpH8wsGFU", "label": "[#50] Curso LLM-RLHF (4/n) - In-Context Learning & Gradient Descent explicado por un Data Scientist", "imagen": "https://i.ytimg.com/vi/TMYpH8wsGFU/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnLKjstxZzEXoc1dyp4cXXB"], "position": 4, "type": "video", "id": "UExXaENRaTk3ZHN0bkxLanN0eFp6RVhvYzFkeXA0Y1hYQi4wOTA3OTZBNzVEMTUzOTMy", "publishedAt": "2023-01-18", "description": "Cuarto video de la serie sobre modelos de lenguaje gigantes donde vamos a aprender lo necesario para hacer una versi\u00f3n abierta de ChatGPT. Como no es algo facil y esto va cambiando semana a semana, vamos a ir viendo todo paso a paso yendo desde lo mas f\u00e1cil a lo mas complejo sin esquivarle a lo dificil pero tratando de explicar todo. Hoy vamos a hablar de In-Context Learning.\n#chatgpt #nlp #gpt3\n\nContenido\n00:00 - Motivaci\u00f3n & Background\n05:00 - Qu\u00e9 es in-context learning?\n11:30 - Scratchpad, chain-of-thought, algorithmic reasoning\n27:30 - Pregunta abierta: Los LLM aprenden por gradient descent?\n\nLinks mencionados durante el video:\n\ud83d\udcbb \u201cChatGPT: Curso LLM-RLHF\u201d: https://www.youtube.com/playlist?list=PLWhCQi97dstnLKjstxZzEXoc1dyp4cXXB \n\ud83d\udcbb Lecture 07: Foundation Models (FSDL 2022): https://www.youtube.com/watch?v=Rm11UeGwGgk&ab_channel=FullStackDeepLearning \n\ud83d\udcbb \u201c[#35] PRIMICIA! Demo en vivo de AlexaTM 20B con Amazon SageMaker JumpStart (Mejor que GPT-3)\u201d: https://www.youtube.com/watch?v=Li-F3zb2P_c&ab_channel=machinelearnear \n\ud83d\udcbb \u201c#91 - HATTIE ZHOU - Teaching Algorithmic Reasoning via In-context Learning\u201d: https://www.youtube.com/watch?v=80i6D2TJdQ4&ab_channel=MachineLearningStreetTalk \n\ud83d\udcbb Teaching Algorithmic Reasoning via In-context Learning: https://arxiv.org/pdf/2211.09066.pdf \n\ud83d\udcbb Transformers learn in-context by gradient descent: https://arxiv.org/pdf/2212.07677.pdf \n\ud83d\udcbb Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers: https://arxiv.org/pdf/2212.10559.pdf \n\ud83d\udcbb In-context Reinforcement Learning with Algorithm Distillation: https://arxiv.org/pdf/2210.14215.pdf \n\ud83d\udcbb \u201cConnor Leahy\u2013EleutherAI, Conjecture\u201d: https://www.youtube.com/watch?v=Oz4G9zrlAGs&ab_channel=TheInsideView \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "KezHF84vHCQ", "label": "[#41] Riffusion: \ud83c\udfb5\ud83c\udfb8riff + diffusion (Tutorial y Demo)", "imagen": "https://i.ytimg.com/vi/KezHF84vHCQ/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 0, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi41NkI0NEY2RDEwNTU3Q0M2", "publishedAt": "2022-12-20", "description": "Hoy vamos a hablar de Riffusion, que es un modelo finetuneado de Stable Diffusion que funciona con espectrogramas que digamos que son imagenes que representan audio, y de esa forma, podemos generar musica. A nadie se le habia ocurrido todavia, y para sorpresa de todos, funciona muy muy bien. Vamos a verlo. #stablediffusion #music #riffusion\n\nContenido\n00:00 - Que es un espectrograma y que tiene que ver con las redes neuronales?\n06:00 - Como funciona Riffusion?\n11:30 - Demo en vivo en Colab\n\nLink a Notebook:\nhttps://colab.research.google.com/drive/1FhH3HlN8Ps_Pr9OR6Qcfbfz7utDvICl0?usp=sharing\n\nLinks mencionados durante el video:\n\ud83d\udcbb Riffusion About: https://www.riffusion.com/about \n\ud83d\udcbb Riffusion Demo: https://www.riffusion.com/\n\ud83d\udcbb Hugging Face Demo: https://huggingface.co/spaces/fffiloni/spectrogram-to-music \n\ud83d\udcbb AI generated Music Video with Riffusion and Gradio - Free Colab Code Tutorial: https://www.youtube.com/watch?v=UkPCrS-H1vM&ab_channel=1littlecoder \n\ud83d\udcbb Audio Spectrogram Transformer: https://huggingface.co/docs/transformers/main/en/model_doc/audio-spectrogram-transformer \n\ud83d\udcbb Adriano Celentano - Prisencolinensinainciusol (HQ-Audio): https://www.youtube.com/watch?v=r_EBFvzyje8&ab_channel=DeePoP \n\ud83c\udfb5Introducing Diffusion Radio - A 24/7 Livestream of AI-Generated Music!\ud83c\udfb5: https://www.youtube.com/watch?v=uGRLOMf2hSc&ab_channel=Harmonai  \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "vqdl31w-nWo", "label": "[#40] Incre\u00edble UI para entrenar Dreambooth \ud83d\udcad hecha por Hugging Face", "imagen": "https://i.ytimg.com/vi/vqdl31w-nWo/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 1, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi4yODlGNEE0NkRGMEEzMEQy", "publishedAt": "2022-12-17", "description": "Hoy vamos a ver algo que ya sali\u00f3 hace un rato, unas 2 semanas, que para los tiempos de Stable Diffusion, es un mont\u00f3n. Es una UI para Dreambooth extremademente sencilla y \u00fatil, y como la hizo gente de Hugging Face, sabemos que va a ser bastante buena. Incorpora mejores pr\u00e1cticas de varios repos y te deja tambi\u00e9n publicar tu modelo en el Hub y hacer tu propia demo. #stablediffusion #ai #dreambooth \n\nContenido\n00:00 - Demo de Dreambooth Training UI en Google Colab\n03:30 - Entrenar tu modelo y testealo\n14:30 - Push to hub y hacer tu propio Space en HF\n\nLink a Hugging Face Quino:\nhttps://huggingface.co/spaces/machinelearnear/dreambooth-quino \n\nLinks mencionados durante el video:\n\ud83d\udcbb Space original de multimodalart: https://huggingface.co/spaces/multimodalart/dreambooth-training \n\ud83d\udcbb Google Colab: https://colab.research.google.com/github/machinelearnear/open-hf-spaces-in-studiolab/blob/main/run_google_colab.ipynb \n\ud83d\udcbb Bulk Image Resizing Made Easy 2.0: https://www.birme.net/?target_width=512&target_height=512&image_format=jpeg\n\ud83d\udcbb [#39] LoRA-SD: Entren\u00e1 Dreambooth en la mitad de tiempo y con 6G VRAM (+Demo): https://www.youtube.com/watch?v=X-5vmAIRpSE&ab_channel=machinelearnear  \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "X-5vmAIRpSE", "label": "[#39] LoRA-SD: Entren\u00e1 Dreambooth en la mitad de tiempo y con 6G VRAM (+Demo)", "imagen": "https://i.ytimg.com/vi/X-5vmAIRpSE/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 2, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi4wMTcyMDhGQUE4NTIzM0Y5", "publishedAt": "2022-12-16", "description": "Hoy vamos a hablar de como alguien se di\u00f3 de un problema que tiene Dreambooth, busc\u00f3 un paper que fue hecho para otra cosa diferente, pero que finalmente, funcion\u00f3 perfecto con Stable Diffusion. Pudo conseguir que el tiempo de entrenamiento baje a la mitad, que funcione con solo 6G de VRAM, y que el concepto entrenado pese menos de 6MB. Vamos a ver \u201cLORA: LOW-RANK ADAPTATION\u201d. #stablediffusion #ai #dreambooth \n\nContenido\n00:00 - Qu\u00e9 es LoRA y como funciona con Stable Diffusion?\n09:30 - Demo en vivo en Colab\n\nLink a Notebook:\nhttps://colab.research.google.com/github/machinelearnear/open-hf-spaces-in-studiolab/blob/main/run_google_colab.ipynb\n\nLinks mencionados durante el video:\n\ud83d\udcbb Reddit: https://www.reddit.com/r/MachineLearning/comments/zfkqjh/p_using_lora_to_efficiently_finetune_diffusion/ \n\ud83d\udcbb Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning: https://github.com/cloneofsimo/lora \n\ud83d\udcbb Google Colab LoRA + Dreambooth: https://colab.research.google.com/drive/1iSFDpRBKEWr2HLlz243rbym3J2X95kcy?usp=sharing#scrollTo=RXhqKsN8cEop \n\ud83d\udcbb Open HF Spaces on Google Colab: https://github.com/machinelearnear/open-hf-spaces-in-studiolab \n\ud83d\udcbb Hugging Face Demo LoRA + Dreambooth: https://huggingface.co/spaces/hysts/LoRA-SD-training \n\ud83d\udcbb LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS: https://arxiv.org/pdf/2106.09685.pdf \n\ud83d\udcbb LoRA: Low-Rank Adaptation of Large Language Models: https://github.com/microsoft/LoRA \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "v4qWOXr89ZA", "label": "[#38] Paint by Example \ud83c\udfa8: Resultados impresionantes en edici\u00f3n de im\u00e1gen guiado por ejemplos (+Demo)", "imagen": "https://i.ytimg.com/vi/v4qWOXr89ZA/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 3, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi41MjE1MkI0OTQ2QzJGNzNG", "publishedAt": "2022-12-09", "description": "Hoy vamos a hablar de \u201cPaint by Example\u201d un nuevo paper y codigo donde vamos a poder una imagen basandonos no en un texto sino en otra imagen que vamos a usar de ejemplo. Los resultados son realmente impresionantes y funciona bastante rapido. De yapa, tambien vamos a ver como clonar un Space de Hugging Face para usarlo de forma gratuita con una GPU de Google Colab. #stablediffusion #ai #dreambooth  \n\nContenido\n00:00 - Qu\u00e9 es \u201cPaint by Example\u201d y como funciona?\n10:20 - Demo en vivo en Colab\n\nLink a Notebook:\nhttps://colab.research.google.com/gist/machinelearnear/52a5df982baf08f4dd49bf57b07e36e9/sd_dreambooth_webui_machinelearnear.ipynb \n\nLinks mencionados durante el video:\n\ud83d\udcbb Paint-by-Example: https://huggingface.co/spaces/Fantasy-Studio/Paint-by-Example \n\ud83d\udcbb Paint by Example: Exemplar-based Image Editing with Diffusion Models (ArXiv): https://arxiv.org/abs/2211.13227 \n\ud83d\udcbb Paint by Example: Exemplar-based Image Editing with Diffusion Models (GitHub): https://github.com/Fantasy-Studio/Paint-by-Example \n\ud83d\udcbb Open HF Spaces on Google Colab: https://github.com/machinelearnear/open-hf-spaces-in-studiolab \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "WATmCSzbvcg", "label": "[#36] Porqu\u00e9 hay tanta controversia con Stable Diffusion V2? An\u00e1lisis, demo en vivo, y discusi\u00f3n", "imagen": "https://i.ytimg.com/vi/WATmCSzbvcg/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 4, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi4wOTA3OTZBNzVEMTUzOTMy", "publishedAt": "2022-11-26", "description": "Hoy vamos a charlar del release p\u00fablico de una nueva versi\u00f3n de Stable Diffusion. Vamos a ver que tan bien funciona, como lo podemos usar, y un poco de la pol\u00e9mica que se arm\u00f3 alrededor de como se entren\u00f3. Tambi\u00e9n vamos a analizar el futuro de estos modelos generativos y que va a pasar con la prompt engineering en unos a\u00f1os, o ese arte de \u201chablar con un modelo\u201d. #stablediffusion #v2 #machinelearning \n\nContenido\n00:00 - Release p\u00fablico de SD 2.0\n13:30 - Demo en vivo\n16:50 - Pol\u00e9mica y discusi\u00f3n sobre SD\n20:44 - Why \"Prompt Engineering\" and \"Generative AI\" are overhyped\n26:00 - Conclusiones\n\nLinks mencionados durante el video:\n\ud83d\udcbb [#29] C\u00f3mo es que funciona realmente Stable Diffusion? (Guia ilustrada paso a paso): https://www.youtube.com/watch?v=00NV4EXcpLQ \n\ud83d\udcbb Anuncio de StabilityAI: https://stability.ai/blog/stable-diffusion-v2-release \n\ud83d\udcbb Illustrated Stable Diffusion: https://jalammar.github.io/illustrated-stable-diffusion/ \n\ud83d\udcbb SD 2.0 en HuggingFace: https://huggingface.co/stabilityai/stable-diffusion-2#examples \n\ud83d\udcbb Nitrosocke Dreambooth Training Guide: https://github.com/nitrosocke/dreambooth-training-guide \n\ud83d\udcbb Can we start a list of Stable Diffusion 2.0 compatible UI's?: https://www.reddit.com/comments/z50x3k  \n\ud83d\udcbb Why \"Prompt Engineering\" and \"Generative AI\" are overhyped: https://lspace.swyx.io/p/why-prompt-engineering-and-generative \n\ud83d\udcbb Stable DIffusion 2.0 YA!! @efoxxfiles: https://www.youtube.com/watch?v=xHfwwQQnFkw \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "Cbrbv8SyzJQ", "label": "[#33] AltDiffusion-m9: Gener\u00e1 im\u00e1genes en Espa\u00f1ol y en otros 8 idiomas!", "imagen": "https://i.ytimg.com/vi/Cbrbv8SyzJQ/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 5, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi4xMkVGQjNCMUM1N0RFNEUx", "publishedAt": "2022-11-19", "description": "Hoy vamos a volver un poco a laburar de verdad, y vamos a un nuevo paper/modelo que acaba salir de un laboratorio que no conocia, pero que esta bastante interesante. Se llama AltDiffusion, y es un modelo que permite generar imagenes basadose en prompts en multiples lenguajes, incluido el Espa\u00f1ol, y que esta hecho encima de Stable Diffusion. \n\nContenido\n00:00 - Qu\u00e9 es AltDiffusion y AltCLIP?\n07:33 - C\u00f3mo funciona?\n15:40 - Demo\n\nLink a Notebook:\nhttps://colab.research.google.com/gist/machinelearnear/2549fd06e946620e091ad77b303ba844/altdiffusion_demo.ipynb \n\nLinks mencionados durante el video:\n\ud83d\udcbb AltDiffusion-m9: https://huggingface.co/BAAI/AltDiffusion-m9 \n\ud83d\udcbb HF Documentation: https://huggingface.co/docs/diffusers/main/en/api/pipelines/alt_diffusion \n\ud83d\udcbb AltDiffusion GitHub: https://github.com/FlagAI-Open/FlagAI/tree/master/examples/AltDiffusion \n\ud83d\udcbb Bilingual (English-Chinese) HF Spaces: https://huggingface.co/spaces/BAAI/bilingual_stable_diffusion \n\ud83d\udcbb AltClip Paper: https://arxiv.org/abs/2211.06679 \n\ud83d\udcbb PyTTI-Tools: https://pytti-tools.github.io/pytti-book/intro.html \n\ud83d\udcbb Teacher-Model for CLIP: https://aclanthology.org/2022.lrec-1.739.pdf \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #spanish #aiart"}, {"url": "00NV4EXcpLQ", "label": "[#29] C\u00f3mo es que funciona realmente Stable Diffusion? (Guia ilustrada paso a paso)", "imagen": "https://i.ytimg.com/vi/00NV4EXcpLQ/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 6, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi41MzJCQjBCNDIyRkJDN0VD", "publishedAt": "2022-10-10", "description": "Hoy vamos a hacer un video distinto, nos vamos a poner el sombrero de Data Scientist, y vamos a ver como es que funciona realmente Stable Diffusion. Cu\u00e1l fue su inspiraci\u00f3n, cu\u00e1les son sus componentes, como funcionan los modelos de difusi\u00f3n, y muchas mas cosas.\n\nContenido\n00:00 - Introducci\u00f3n\n07:15 - Los componentes de Stable Diffusion\n17:20 - Que es la \u201cdifusi\u00f3n\u201d?\n25:08 - \u201cSpeed boost: Diffusion on compressed (latent) space\u201d\n34:00 - Conclusiones\n\nLinks mencionados durante el video:\n\ud83d\udcbb The Illustrated Stable Diffusion by @Jay Alammar: https://jalammar.github.io/illustrated-stable-diffusion/\n\ud83d\udcbb How does Stable Diffusion work? \u2013 Latent Diffusion Models EXPLAINED: https://www.youtube.com/watch?v=J87hffSMB60&ab_channel=AICoffeeBreakwithLetitia \n\ud83d\udcbb Stable Diffusion, Explained by @ai__pub: https://twitter.com/ai__pub/status/1561362542487695360\n\ud83d\udcbb High-Resolution Image Synthesis with Latent Diffusion Models: https://arxiv.org/abs/2112.10752 \n\ud83d\udcbb Denoising Diffusion Probabilistic Models (DDPMs): https://magic-with-latents.github.io/latent/ddpms-series.html \n\ud83d\udcbb The Annotated Diffusion Model: https://huggingface.co/blog/annotated-diffusion \n\ud83d\udcbb What are Diffusion Models? https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ \n\ud83d\udcbb Modelos de difusi\u00f3n de inteligencia artificial: https://www.youtube.com/watch?v=F8bqC5ggZx0&ab_channel=CentroNacionaldeInteligenciaArtificial \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #machinelearning #aiart"}, {"url": "miHtA0wZD2c", "label": "[#28] C\u00f3mo usar Paperspace Gradient con Stable Diffusion (Web UI de AUTOMATIC1111)", "imagen": "https://i.ytimg.com/vi/miHtA0wZD2c/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 7, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi5DQUNERDQ2NkIzRUQxNTY1", "publishedAt": "2022-10-06", "description": "Hoy vamos a hacer un tutorial paso a paso de como correr una Web UI de Stable Diffusion dentro de Paperspace Gradient, una de las alternativas que tenemos a Google Colab. \n\nContenido\n00:00 - Qu\u00e9 es Paperspace Gradient?\n03:00 - Paso a paso como setear todo\n10:39 - Demo Web UI de AUTOMATIC1111\n\nLinks mencionados durante el video:\n\ud83d\udcbb Paperspace Gradient: https://github.com/huggingface/diffusers \n\ud83d\udcbb \u201cA guide to getting started with the paperspace port of AUTOMATIC1111\u2019s web UI for ppl who get nervous\u201d: https://proximacentaurib.notion.site/A-guide-to-getting-started-with-the-paperspace-port-of-AUTOMATIC1111-s-web-UI-for-ppl-who-get-nervou-b83c2213f17e452e8b0e37ba64fe9758\n\ud83d\udcbb stable-diffusion-paperspace: https://github.com/Engineer-of-Stuff/stable-diffusion-paperspace \n\ud83d\udcbb Stable Diffusion web UI: https://github.com/AUTOMATIC1111/stable-diffusion-webui\n\ud83d\udcbb (WIP) Running AUTOMATIC1111 / stable-diffusion-webui with Dreambooth fine-tuned models #1429: https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/1429 \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#paperspace #stablediffusion #gradient"}, {"url": "gRZsmvhoLOA", "label": "[#26] Tutorial: Videos de m\u00fasica generados por AI (Video Killed the Radio Star \u2026 Diffusion)", "imagen": "https://i.ytimg.com/vi/gRZsmvhoLOA/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 8, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi45NDk1REZENzhEMzU5MDQz", "publishedAt": "2022-10-03", "description": "Hoy vamos a aprender como usar Stable Diffusion para generar una secuencia de imagenes en base a las letras de una canci\u00f3n y tambi\u00e9n en base a alg\u00fan estilo que nosotros queramos. No se necesita mucho, simplemente la URL a un video de YouTube y despu\u00e9s la Notebook hace el resto. \n\nContenido\n00:00 - Que es y como funciona todo esto?\n09:49 - Configurar todo en Paperspace Gradient\n12:10 - Tutorial paso a paso\n15:49 - Conclusiones\n\nLink a Google Colab por @DigThatData:\n\ud83d\udcbb https://github.com/dmarx/video-killed-the-radio-star\n\nLinks mencionados durante el video:\n\ud83d\udcbb Video tutorial por David Marx: https://youtu.be/0pfzQ-cZU0E\n\ud83d\udcbb Whisper Web UI: https://huggingface.co/spaces/aadnk/whisper-webui\n\ud83d\udcbb Hugging Face Diffusers: https://github.com/huggingface/diffusers\n\ud83d\udcbb DreamStudio: https://beta.dreamstudio.ai/\n\ud83d\udcbb Ai Generated Music Video - Deltron 3030 - Virus: https://www.youtube.com/watch?v=WJaxFbdjm8c&ab_channel=BenGillin \n\ud83d\udcbb I wanna devise a virus: https://www.youtube.com/watch?v=yzY80Snqj5I&ab_channel=shaggorama \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #musicvideo #aiart"}, {"url": "7KWjjWkNoJU", "label": "[#24] Tutorial: DreamBooth & Stable Diffusion en Google Colab usando 12GB VRAM!", "imagen": "https://i.ytimg.com/vi/7KWjjWkNoJU/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 9, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi5GNjNDRDREMDQxOThCMDQ2", "publishedAt": "2022-09-29", "description": "Hoy vamos a hablar de nuevo sobre Stable Diffusion y de como entrenar conceptos nuevos, como un objeto, una persona, o un estilo artistico, para generar nuestras imagenes. Vamos a estar viendo como usar Dreambooth, que es un paper de Google, que funciona muy muy bien para esto.  \n\nContenido\n00:00 - DreamBooth vs textual inversion\n03:00 - En que consiste la optimizaci\u00f3n?\n07:15 - Como entrenar un concepto nuevo\n07:15 - Demo para crear imagenes\n11:35 - Conclusiones\n\nLink a Google Colab por @ShivamShrirao:\n\ud83d\udcbb https://github.com/huggingface/notebooks/blob/main/diffusers/sd_dreambooth_training.ipynb\n\nLinks mencionados durante el video:\n\ud83d\udcbb Diffusers: https://github.com/huggingface/diffusers \n\ud83d\udcbb Fork de DreamBooth por @ShivamShrirao: https://github.com/ShivamShrirao/diffusers/tree/main/examples/dreambooth\n\ud83d\udcbb Textual inversion: https://huggingface.co/docs/diffusers/training/text_inversion \n\ud83d\udcbb bitsandbytes: https://github.com/TimDettmers/bitsandbytes \n\ud83d\udcbb Facebook xFormers: https://github.com/facebookresearch/xformers \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #dreambooth #textual-inversion"}, {"url": "315xQppudbg", "label": "[#22] Como crear videos con Stable Diffusion", "imagen": "https://i.ytimg.com/vi/315xQppudbg/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 10, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi40NzZCMERDMjVEN0RFRThB", "publishedAt": "2022-09-20", "description": "Este va a ser el ultimo video que haga sobre Stable Diffusion por un tiempo pero estuvo bueno meterse en un tema completamente. Hoy vamos a ver como crear animaciones o videos partiendo de dos o mas imagenes o sino simplemente poniendo 2 prompts y dejando que la AI sola busque las imagenes intermedias. \n\nContenido\n00:00 - Hacer videos explorando el sample space\n06:06 - Interpolando imagenes con FILM\n10:32 - Deforum Stable Diffusion\n13:30 - Replicate.ai y conclusiones\n\nLink a Google Colab por @nateraw:\n\ud83d\udcbb https://colab.research.google.com/github/nateraw/stable-diffusion-videos/blob/main/stable_diffusion_videos.ipynb\n\nLinks mencionados durante el video:\n\ud83d\udcbb Stable Diffusion Videos: https://github.com/nateraw/stable-diffusion-videos\n\ud83d\udcbb Prompt Parrot v2.5 Notebook by @KyrickYoung: https://colab.research.google.com/drive/1GtyVgVCwnDfRvfsHbeU0AlG-SgQn1p8e\n\ud83d\udcbb FILM Colab by @KyrickYoung: https://colab.research.google.com/drive/1tbbbnQge0yb0LmnWNchEKNhjtBNC6jX-?usp=sharing\n\ud83d\udcbb FILM: Frame Interpolation for Large Motioni: https://github.com/google-research/frame-interpolation\n\ud83d\udcbb Stable Diffusion Text-to-Image AI - Video Interpolating Guide: https://www.youtube.com/watch?v=iKlkJI5vuXQ&ab_channel=HelicopterDown \n\ud83d\udcbb Deforum Stable Diffusion: https://colab.research.google.com/github/deforum/stable-diffusion/blob/main/Deforum_Stable_Diffusion.ipynb\n\ud83d\udcbb Deforum Stable Diffusion on Replicate: https://replicate.com/deforum/deforum_stable_diffusion\n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #aiart #python #pytorch #interpolation"}, {"url": "fT-xvoA0nUU", "label": "[#21] Recursos y herramientas para hacer mejores prompts para Stable Diffusion", "imagen": "https://i.ytimg.com/vi/fT-xvoA0nUU/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 11, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi5EMEEwRUY5M0RDRTU3NDJC", "publishedAt": "2022-09-18", "description": "Stable Diffusion funciona bastante bien en generar imagenes pero depende muchisimo del input que le estamos metiendo, es decir de la secuencia de texto que usamos para generar las imagenes, o lo que se conoce como \u201cprompt\u201d en ingles. Hoy vamos a ver que recursos tenemos disponible para hacer mejores prompts.\n\nContenido\n00:00 - Qu\u00e9 son las prompts y que es \u201cprompt engineering\u201d?\n03:00 - Cuales son los estilos mas citados en Stable Diffusion\n07:15 - Como generar prompts automaticamente\n11:35 - Conclusiones\n\nLink a Google Colab por @KyrickYoung:\n\ud83d\udcbb https://colab.research.google.com/drive/1GtyVgVCwnDfRvfsHbeU0AlG-SgQn1p8e?usp=sharing#scrollTo=775w0YYfMk9t\n\nLinks mencionados durante el video:\n\ud83d\udcbb Useful Prompt Engineering tools and resources: https://www.reddit.com/r/StableDiffusion/comments/xcrm4d/useful_prompt_engineering_tools_and_resources/\n\ud83d\udcbb promptoMANIA: https://promptomania.com/prompt-builder/\n\ud83d\udcbb urania.ai: https://www.urania.ai/top-sd-artists\n\ud83d\udcbb Lexica.ART: https://lexica.art/\n\ud83d\udcbb KREA: https://www.krea.ai/\n\ud83d\udcbb DALLE-E 2 Prompt Book: https://dallery.gallery/the-dalle-2-prompt-book/ \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #prompts #python #aiart"}, {"url": "nD2meRxsgVc", "label": "[#20] Como ense\u00f1ar y usar nuevos conceptos en Stable Diffusion (Textual Inversion)", "imagen": "https://i.ytimg.com/vi/nD2meRxsgVc/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 12, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi45ODRDNTg0QjA4NkFBNkQy", "publishedAt": "2022-09-17", "description": "Hoy vamos a hablar de nuevo sobre Stable Diffusion, la alternativa open-source y gratuita a DALLE-2 que se usa para generar imagenes desde una secuencia de texto. Vamos ver como hacer para ense\u00f1arles conceptos nuevos como puede ser un objeto o un estilo artistico.\n\nContenido\n00:00 - Qu\u00e9 es la \u201cStable Diffusion concepts library\u201d?\n05:40 - Intro a textual inversion y DreamBooth\n08:00 - Como entrenamos un nuevo concepto\n09:50 - Como dejar sesion abierta en Google Colab\n10:45 - Como lo usamos para generar imagenes\n13:15 - Conclusiones\n\nLink a repositorio:\n\ud83d\udcbb https://huggingface.co/sd-concepts-library\n\nLinks mencionados durante el video:\n\ud83d\udcbb Notebook para entrenar nuevo concepto: https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb\n\ud83d\udcbb Notebook para inferencia de conceptos aprendidos: https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_conceptualizer_inference.ipynb\n\ud83e\udd17 Stable Diffusion: https://huggingface.co/CompVis/stable-diffusion-v1-4 \n\ud83e\udd17 Textual inversion: https://huggingface.co/docs/diffusers/training/text_inversion\n\ud83d\udcbb Dreambooth-Stable-Diffusion: https://github.com/XavierXiao/Dreambooth-Stable-Diffusion \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #python #huggingface #textual-inversion"}, {"url": "-WhtslJYaJw", "label": "[#19] Tutorial para Outpainting con Stable Diffusion", "imagen": "https://i.ytimg.com/vi/-WhtslJYaJw/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 13, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi4zMDg5MkQ5MEVDMEM1NTg2", "publishedAt": "2022-09-15", "description": "Hoy vamos a ver paso a paso como hacer algo llamado outpainting con Stable Diffusion, que es una nueva AI para generar imagenes desde secuencias de texto.  \n\nContenido\n00:00 - Qu\u00e9 es \u201coutpainting\u201d y que es \u201cinpainting\u201d?\n05:21 - Demo\n09:35 - Conclusiones\n\nLink a notebook en Google Colab:\n\ud83d\udcbb https://colab.research.google.com/github/lkwq007/stablediffusion-infinity/blob/master/stablediffusion_infinity_colab.ipynb\n\nLinks mencionados durante el video:\n\ud83d\udcbb stablediffusion-infinity: https://github.com/lkwq007/stablediffusion-infinity \n\ud83d\udcbb CompVis\u2019s Stable Diffusion: https://github.com/CompVis/stable-diffusion \n\ud83e\udd17 Stable Diffusion: https://huggingface.co/CompVis/stable-diffusion-v1-4 \n\ud83e\udd17 Spaces: https://huggingface.co/spaces/stabilityai/stable-diffusion\n\ud83d\udcbb Stable Diffusion Web UI: https://github.com/AUTOMATIC1111/stable-diffusion-webui \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#outpainting #python #stablediffusion"}, {"url": "XIY-Ey08HHM", "label": "[#12] Me dieron acceso a DALLE-2, te muestro como funciona, y vemos lo \u00faltimo en arte generativo", "imagen": "https://i.ytimg.com/vi/XIY-Ey08HHM/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 14, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi41Mzk2QTAxMTkzNDk4MDhF", "publishedAt": "2022-08-09", "description": "En el video de hoy vamos a charlar m\u00e1s en detalle sobre DALLE-2, vamos a generar algunas im\u00e1genes con la demo de OpenAI, y ya que estamos, tambien vamos a ver que esta pasando en todo este mundo del arte generativo.\n\nContenido\n00:00 - Qu\u00e9 es DALLE-2 y resultados\n12:00 - Tipos de modelos generativos\n16:26 - C\u00f3mo hacer buen \u201cprompt engineering\u201d?\n17:47 - \u00c9tica, sesgos, derechos de autor..\n23:16 - Todo lo nuevo dando vuelta\n31:13 - Que me gustar\u00eda ver en el futuro?\n\nRepositorio con todos los links:\n\ud83d\udcbb https://github.com/machinelearnear/awesome-review-dalle-2\n\nAlgunos links mencionados durante el video:\n\ud83d\udcbb \u201cThe Fantastic New World of AI Art Generators and Why Their Critics Get It All Wrong\u201d https://danieljeffries.substack.com/p/the-fantastic-new-world-of-ai-art \n\ud83d\udcf9 \u201cAuto-encoder, Transformer, Diffusion Model, and GAN\u201d by @Miamiamia0103: https://www.mia-tang.com/blog\n\ud83d\udcbb \u201cNo Token Left Behind: Explainability-Aided Image Classification and Generation\u201d: https://twitter.com/_akhaliq/status/1513690196902129666\n\ud83d\udcbb Midjourney: https://www.midjourney.com/app/\n\ud83d\udcdd DALLE-2 Prompt Book: http://dallery.gallery/wp-content/uploads/2022/07/The-DALL%C2%B7E-2-prompt-book.pdf\n\nCover image & YouTube thumbnail, todo hecho con DALLE-2, asi que no hay que referenciar a nadie \ud83d\ude43\n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#python #dalle2 #stablediffusion"}, {"url": "K2lA2WxhLnw", "label": "[#10] Una introducci\u00f3n r\u00e1pida al arte multi-modal con AI (DALL\u00b7E, CLIP, Diffusion, VQGAN) (+ repos)", "imagen": "https://i.ytimg.com/vi/K2lA2WxhLnw/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 15, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi5EQUE1NTFDRjcwMDg0NEMz", "publishedAt": "2022-05-08", "description": "En el video de hoy vamos a charlar un poco sobre los ultimos avances de inteligencia artificial en arte, cuales son herramientas mas usadas, las tendencias que se estan viendo, y para donde es posible que vayamos a ir en un futuro.  \n\nContenido\n00:00 - Evolucion del uso de AI con arte \n03:05 - Que es DALL\u00b7E, CLIP, y Diffusion Models\n06:03 - Demo online (DALLE-FLOW)\n10:40 - Que son las prompts y como funcionan?\n13:30 - Demo online (HuggingFace, MindsEye)\n18:00 - El futuro con DALLE-2 y CogView2\n\nLinks mencionados durante el video:\n\ud83d\udcf9  GLIDE: Gener\u00e1 y edit\u00e1 im\u00e1genes en segundos en base a lo que escribis (+ Repo): https://www.youtube.com/watch?v=WG20CnktPbk \n\ud83d\udcbb  DALLE-FLOW: https://github.com/jina-ai/dalle-flow\n\ud83d\udcbb  DALL\u00b7E HuggingFace Demo: https://huggingface.co/spaces/dalle-mini/dalle-mini \n\ud83d\udcbb  DALL\u00b7E Mega Training on Weights and Biases: https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mega--VmlldzoxODMxMDI2\n\ud83d\udcbb  A comprehensive guide to understand the multimodal AI art scene and create your own text-to-image (and other) pieces: https://multimodal.art/\n\ud83d\udcbb  PyTTI: https://pytti-tools.github.io/pytti-book/intro.html \n\ud83d\udcbb  MindsEye beta - ai art pilot: https://multimodal.art/mindseye\n\ud83d\udcbb  Generate images from text with Latent Diffusion LAION-400M: https://huggingface.co/spaces/multimodalart/latentdiffusion \n\ud83d\udcbb  (Reddit) AI-generated and manipulated content: https://www.reddit.com/r/MediaSynthesis\n\ud83d\udcbb  Writing good VQGAN+CLIP prompts part one \u2013 basic prompts and style modifiers: https://www.unlimiteddreamco.xyz/2022/03/16/writing-good-prompts-part-1\n\ud83d\udcbb  Artist Studies by @remi_durant: https://remidurant.com/artists/# \n\ud83d\udcbb  DALL\u00b7E 2: https://openai.com/dall-e-2/\n\ud83d\udcbb  Implementation of DALL-E 2, OpenAI's updated text-to-image synthesis neural network, in Pytorch: https://github.com/lucidrains/DALLE2-pytorch\n\ud83d\udcf9  How does DALL-E 2 actually work?: https://www.youtube.com/watch?v=F1X4fHzF4mQ&ab_channel=AssemblyAI\n\ud83d\udcf9  DALL-E 2 Inpainting / Editing Demo: https://www.youtube.com/watch?v=TFJLcy-pfTM&ab_channel=BakzT.Future\n\ud83d\udcbb  \u201cInfinite Images and the latent camera\u201d: https://mirror.xyz/herndondryhurst.eth \n\nImage credit: @YennieJun and @Dalle2Pics\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#multi-modal #arte #python #huggingface #glide #dalle #espa\u00f1ol #aws"}, {"url": "xJRokNH6bvE", "label": "[#43] Nueva UI muy simple para Stable Diffusion & Review de Modelos Fotorrealistas y de Anime", "imagen": "https://i.ytimg.com/vi/xJRokNH6bvE/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 16, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi41QTY1Q0UxMTVCODczNThE", "publishedAt": "2023-01-06", "description": "Este es el primer video del a\u00f1o despu\u00e9s de que nos tomamos un descansito y como pasa siempre, lo que se avanzo estas semanas fue un mont\u00f3n. Hoy vamos a ver una nueva web UI que sali\u00f3 para Stable Diffusion, que es bastante minimalista y simple de usar, y de paso vamos a hacer un review de 4 modelos nuevos que salieron para generaci\u00f3n de im\u00e1genes, algunos fotorealistas, otros para Anime, y otros mas. Vamos a verlo.\n#stablediffusion #tutorial #aiart \n\nContenido\n00:00 - Qu\u00e9 es diffuzers y demo\n14:35 - dreamlike-art/dreamlike-photoreal-2.0\n18:03 - aipicasso/cool-japan-diffusion-2-1-0\n19:55 - 22h/vintedois-diffusion-v0-1\n22:38 - Linaqruf/anything-v3.0\n\nLink a Notebook:\nhttps://colab.research.google.com/github/abhishekkrthakur/diffuzers/blob/main/diffuzers.ipynb\n\nLinks mencionados durante el video:\n\ud83d\udcbb diffuzers: https://github.com/abhishekkrthakur/diffuzers \n\ud83d\udcbb HuggingFace text2image models: https://huggingface.co/models?pipeline_tag=text-to-image&sort=downloads \n\ud83d\udcbb stabilityai/stable-diffusion-2-1: https://huggingface.co/stabilityai/stable-diffusion-2-1 \n\ud83d\udcbb dreamlike-art/dreamlike-photoreal-2.0: https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0 \n\ud83d\udcbb aipicasso/cool-japan-diffusion-2-1-0: https://huggingface.co/aipicasso/cool-japan-diffusion-2-1-0 \n\ud83d\udcbb 22h/vintedois-diffusion-v0-1: https://huggingface.co/22h/vintedois-diffusion-v0-1 \n\ud83d\udcbb Linaqruf/anything-v3.0: https://huggingface.co/Linaqruf/anything-v3.0 \n\ud83d\udcbb Multistep DPM-Solver: https://huggingface.co/docs/diffusers/v0.11.0/en/api/schedulers/multistep_dpm_solver \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "C50zsvmGbvs", "label": "[#45] Finalmente \u2026 manos perfectas con Stable Diffusion", "imagen": "https://i.ytimg.com/vi/C50zsvmGbvs/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 17, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi4yMUQyQTQzMjRDNzMyQTMy", "publishedAt": "2023-01-10", "description": "Hace unos dias me dijeron en los comentarios que viera un modelo nuevo que habia salido y la verdad que suerte que les hice caso porque esta buenisimo. Se llama Protogen y va por la version X5.8 y por fin permite que Stable Diffusion pueda renderear bien las manos de las personas. Vamos a verlo juntos.\n#stablediffusion #tutorial #aiart \n\nContenido\n00:00 - Que es Protogen y que hace?\n06:48 - Demo en \u201cdiffuzers\u201d\n09:50 - Demo en \u201cAutomatic1111\u201d\n\nLinks mencionados durante el video:\n\ud83d\udcbb diffuzers: https://github.com/abhishekkrthakur/diffuzers \n\ud83d\udcbb TheLastBen SD UI: https://github.com/TheLastBen/fast-stable-diffusion \n\ud83d\udcbb \u201c[#43] Nueva UI muy simple para Stable Diffusion & Review de Modelos Fotorrealistas y de Anime\u201d: https://www.youtube.com/watch?v=xJRokNH6bvE \n\ud83d\udcbb \u201c[#28] C\u00f3mo usar Paperspace Gradient con Stable Diffusion (Web UI de AUTOMATIC1111)\u201d: https://www.youtube.com/watch?v=miHtA0wZD2c \n\ud83d\udcbb Protogen x3.4 (Photorealism) Official Release: https://civitai.com/models/3666/protogen-x34-official-release\n\ud83d\udcbb darkstorm2150/Protogen_v5.3_Official_Release: https://huggingface.co/darkstorm2150/Protogen_v5.3_Official_Release \n\ud83d\udcbb dreamlike-art/dreamlike-photoreal-2.0: https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0 \n\ud83d\udcbb \u201cPerfect AI Fingers with Protogen Stable Diffusion Model\u201d: https://www.youtube.com/watch?v=pOxbRMZsLUM \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "jNTrnoSAipw", "label": "[#46] Esta AI de Google aprendi\u00f3 a escribir y StabilityAI respondi\u00f3  (Muse/DeepFloyd/Karlo)", "imagen": "https://i.ytimg.com/vi/jNTrnoSAipw/mqdefault.jpg", "playlistId": ["PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf"], "position": 18, "type": "video", "id": "UExXaENRaTk3ZHN0bXNPeGp4dG5TSER5MEVnMjF6LXB4Zi45RTgxNDRBMzUwRjQ0MDhC", "publishedAt": "2023-01-12", "description": "Hoy vamos a ver unos nuevos modelos para generacion de imagenes que aparentemente pueden finalmente escribir texto dentro de las imagenes de una forma bastante buena. Vamos a ver de que se trata.\n#stablediffusion #aiart #muse \n\nContenido\n00:00 - Google: De Parti a Imagen a Muse\n11:30 - DeepFloyd + StabilityAI = IF\n14:00 - Karlo, una versi\u00f3n open source de DALLE-2 + Demo\n\nLinks mencionados durante el video:\n\ud83d\udcbb Parti (Google): https://parti.research.google/\n\ud83d\udcbb Imagen (Google): https://imagen.research.google/ \n\ud83d\udcbb Muse AI (Google): https://muse-model.github.io \n\ud83d\udcbb Karlo Huggingface: https://huggingface.co/spaces/kakaobrain/karlo \n\ud83d\udcbb Karlo Repo: https://github.com/kakaobrain/karlo  \n\ud83d\udcbb Huggingface unCLIP Docs: https://huggingface.co/docs/diffusers/main/en/api/pipelines/unclip \n\ud83d\udcbb DeepFloyd: https://twitter.com/deepfloydai \n\ud83d\udcbb Notebook Open HF Spaces in Colab: https://colab.research.google.com/github/machinelearnear/open-hf-spaces-in-studiolab/blob/main/run_google_colab.ipynb \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "CkYmjKUX-Sc", "label": "[#25] Fin de una era .. Google Colab deja de ser gratuito .. Que alternativas existen?", "imagen": "https://i.ytimg.com/vi/CkYmjKUX-Sc/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnIDPxUQ0UwV--N33qwSxxT"], "position": 0, "type": "video", "id": "UExXaENRaTk3ZHN0bklEUHhVUTBVd1YtLU4zM3F3U3h4VC41NkI0NEY2RDEwNTU3Q0M2", "publishedAt": "2022-09-30", "description": "Hoy vamos a hablar del cambio que Google acab\u00e1 de hacer a las condiciones de Colab y como va a dejar de ser gratuito. Yo que lo sigo desde 2017, me lo esperaba, pero igual, viendo como es el default absoluto para experimentar con AI hoy en dia, queria cubrir en este video que alternativas tenemos. Tanto gratuitas como de pago. \n\nContenido\n00:00 - Como es el nuevo pricing de Google Colab\n06:07 - Alternativas gratuitas: Studio Lab, Paperspace, Kaggle\n12:15 - Alternativas en la nube: Vast.ai, FluidStack, RunPod\n15:35 - Conclusiones\n\nLinks mencionados durante el video:\n\ud83d\udcbb Paperspace Gradient: https://github.com/huggingface/diffusers \n\ud83d\udcbb SageMaker Studio Lab: https://www.youtube.com/watch?v=FUEIwAsrMP4\n\ud83d\udcbb Kaggle Notebooks: https://www.kaggle.com/\n\ud83d\udcbb Vast.ai: https://vast.ai/\n\ud83d\udcbb FluidStack: https://www.fluidstack.io/pricing\n\ud83d\udcbb RunPod: https://www.runpod.io \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#googlecolab #paperspace #sagemaker"}, {"url": "5eZKGMCtCdM", "label": "[#30] \ud83e\uddc9\ud83e\udd16 Diffusers 0.5.0 | \u201cWhat the DAAM\u201d | Stable Diffusion en AR/VR | Podcast.ai (OCT 2022)", "imagen": "https://i.ytimg.com/vi/5eZKGMCtCdM/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnIDPxUQ0UwV--N33qwSxxT"], "position": 1, "type": "video", "id": "UExXaENRaTk3ZHN0bklEUHhVUTBVd1YtLU4zM3F3U3h4VC4yODlGNEE0NkRGMEEzMEQy", "publishedAt": "2022-10-14", "description": "Hoy queria probar un formato nuevo. Viendo que con todo lo que sale cada dia, es muy dificil estar al tanto de lo que esta pasando, que funciona, que no, queria hacer un resumen de noticias de la \u00faltima semana. Todo mientras nos tomamos unos matienzos.\n\nContenido\n00:00 - Nueva versi\u00f3n de Diffusers 0.5.0\n05:43 - What the DAAM\n08:45 - Whisper integrado en HuggingFace\n09:50 - Stable Worlds\n11:25 - Podcast.ai Joe Rogan interviews Steve Jobs\n14:15 - ERNIE-Layout\n\nLinks mencionados durante el video:\n\ud83d\udcbb Diffusers 0.5.0 releases: https://github.com/huggingface/diffusers/releases\n\ud83d\udcbb Negative prompts in AUTOMATIC1111: https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Negative-prompt \n\ud83d\udcbb Colab para usar Diffusers con TPUs: https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_fast_jax.ipynb \n\ud83d\udcbb What the DAAM: https://arxiv.org/pdf/2210.04885.pdf \n\ud83d\udcbb What the DAAM Repo: https://github.com/castorini/daam\n\ud83d\udcbb OpenAI Whisper: https://huggingface.co/openai/whisper-large \n\ud83d\udcbb OpenAI Whisper in Transformers Colab: https://colab.research.google.com/drive/16HO7if9iwfpSJzhqlaNOu6iiMhUBLMKE?usp=sharing \n\ud83d\udcbb Stable Worlds: https://twitter.com/NaxAlpha/status/1578685845099290624a \n\ud83d\udcbb Joe Rogan interviews Steve Jobs: https://share.transistor.fm/s/22f16c7f \n\ud83d\udcbb Khipu 2023: https://khipu.ai/\n\ud83d\udcbb ERNIE-Layout: https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #machinelearning #huggingface"}, {"url": "bNE7UZI4cI4", "label": "[#32] \ud83e\uddc9\ud83e\udd16 +1,000 suscriptores! SD 1.5+VAE | Flan-T5 | Text-to-Music | Transformers x DeepMind", "imagen": "https://i.ytimg.com/vi/bNE7UZI4cI4/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnIDPxUQ0UwV--N33qwSxxT"], "position": 2, "type": "video", "id": "UExXaENRaTk3ZHN0bklEUHhVUTBVd1YtLU4zM3F3U3h4VC4wMTcyMDhGQUE4NTIzM0Y5", "publishedAt": "2022-11-16", "description": "Viendo que con todo lo que sale cada dia, es muy dificil estar al tanto de lo que esta pasando, que funciona, que no, queria hacer un resumen de noticias de la \u00faltima semana. Todo mientras nos tomamos unos matienzos.\n\nContenido\n00:00 - Stable Diffusion 1.5 + VAE + Diffusers 0.7.2\n11:08 - Google hace p\u00fablico FLAN-T5\n16:39 - MuBERT & Text-to-Music\n19:21 - Algorithm Distillation x DeepMind\n26:06 - NVIDIA Diffusers\n29:06 - Musika & MAXIM & ExplainPaper\n\nLinks mencionados durante el video:\n\ud83d\udcbb SD notebook using SD 1.5 (@runwayml) and VAE (@StabilityAI): https://colab.research.google.com/drive/174iLnLZbO4yKTwKHw1L4ljQTnKez4K7l \n\ud83d\udcbb Fine-tuned decoders trained by @RiversHaveWings: https://twitter.com/StabilityAI/status/1586183361361428480  \n\ud83d\udcbb New open-source language model from Google AI: Flan-T5 \ud83c\udf6e: https://twitter.com/quocleix/status/1583523186376785921 \n\ud83d\udcbb Scaling Instruction-Finetuned Language Models - Video Summary: https://www.youtube.com/watch?v=oqi0QrbdgdI&ab_channel=ShayneLongpre\n\ud83d\udcbb Google Flan-T5 Spaces: https://huggingface.co/spaces/osanseviero/i-like-flan \n\ud83d\udcbb @Gradio demo for @mubertapp Text-to-Music is out on @huggingface Spaces: https://twitter.com/_akhaliq/status/1585483943880658945 \n\ud83d\udcbb No prompting, no fine-tuning / Algorithm Distillation: https://twitter.com/MishaLaskin/status/1585265436723236864 \n\ud83d\udcbb Demis Hassabis: DeepMind - AI, Superintelligence & the Future of Humanity | Lex Fridman Podcast: https://www.youtube.com/watch?v=Gfr50f6ZBvo&ab_channel=LexFridman \n\ud83d\udcbb eDiff-I: Text-to-Image Diffusion Models with Ensemble of Expert Denoisers: https://deepimagination.cc/eDiffi/ \n\ud83d\udcbb Paint With Words - Stable Diffusion Style: https://www.youtube.com/watch?v=JE7VSzFo1qY&ab_channel=NerdyRodent\n\ud83d\udcbb Paint-with-Words, Implemented with Stable diffusion: https://github.com/cloneofsimo/paint-with-words-sd  \n\ud83d\udcbb Generative AI: A Creative New World: https://www.sequoiacap.com/article/generative-ai-a-creative-new-world/ \n\ud83d\udcbb Musika Library: https://huggingface.co/musika \n\ud83d\udcbb TensorFlow port of MAXIM, a single backbone capable of denoising, dehazing, deblurring, and more: https://github.com/sayakpaul/maxim-tf \n\ud83d\udcbb Explain Paper: https://www.explainpaper.com/ \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #deepmind #huggingface"}, {"url": "Li-F3zb2P_c", "label": "[#35] PRIMICIA! Demo en vivo de AlexaTM 20B con Amazon SageMaker JumpStart (Mejor que GPT-3)", "imagen": "https://i.ytimg.com/vi/Li-F3zb2P_c/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnIDPxUQ0UwV--N33qwSxxT", "PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw", "PLWhCQi97dstk-n_W_Eh90b_7tLWayDqcr"], "position": 3, "type": "video", "id": "UExXaENRaTk3ZHN0bklEUHhVUTBVd1YtLU4zM3F3U3h4VC41MjE1MkI0OTQ2QzJGNzNG", "publishedAt": "2022-11-24", "description": "Hoy tenemos como primicia la primera demo en vivo de Alexa Teacher Models, un paper que salio ya hace varios meses pero que reci\u00e9n fue hecho open source hace unos dias y ahora lo podemos correr usando Amazon SageMaker Jumpstart.  #alexa #llm #sagemaker \n\nContenido\n00:00 - Qu\u00e9 es AlexaTM 20B?\n09:05 - Diferencias con otros modelos como GPT3\n20:44 - Qu\u00e9 es y c\u00f3mo funciona SageMaker Jumpstart?\n32:20 - Demo y conclusiones\n\nLink a Notebook:\nhttps://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart_alexatm20b/Amazon_Jumpstart_AlexaTM_20B.ipynb \n\nLinks mencionados durante el video:\n\ud83d\udcbb Alexa teacher model: Pretraining and distilling multi-billion-parameter encoders for natural language understanding systems: https://www.amazon.science/publications/alexa-teacher-model-pretraining-and-distilling-multi-billion-parameter-encoders-for-natural-language-understanding-systems \n\ud83d\udcbb AlexaTM 20B is now available in Amazon SageMaker JumpStart: https://aws.amazon.com/blogs/machine-learning/alexatm-20b-is-now-available-in-amazon-sagemaker-jumpstart/ \n\ud83d\udcbb 20B-parameter Alexa model sets new marks in few-shot learning: https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning\n\ud83d\udcbb AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model: https://arxiv.org/abs/2208.01448\n\ud83d\udcbb alexa-teacher-models: https://github.com/amazon-science/alexa-teacher-models \n\ud83d\udcbb UL2 20B: An Open Source Unified Language Learner: https://ai.googleblog.com/2022/10/ul2-20b-open-source-unified-language.html \n\ud83d\udcbb Sequence to Sequence (seq2seq) and Attention: https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html\n\ud83d\udcbb CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing: https://arxiv.org/abs/2210.07074\n\ud83d\udcbb Easily get started with machine learning using Amazon SageMaker JumpStart - AWS Virtual Workshop: https://www.youtube.com/watch?v=1-AOLoOiuG4&ab_channel=AWSOnlineTechTalks \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\nMy opinions expressed in this video do not represent my employers / Las opiniones expresadas en este video no representan a mis empleadores."}, {"url": "ng8XC1ojoJU", "label": "[#51] Microsoft paga $10B por OpenAI y ChatGPT saldr\u00e1 pronto en \u201cAzure OpenAI Services\u201d", "imagen": "https://i.ytimg.com/vi/ng8XC1ojoJU/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnIDPxUQ0UwV--N33qwSxxT"], "position": 4, "type": "video", "id": "UExXaENRaTk3ZHN0bklEUHhVUTBVd1YtLU4zM3F3U3h4VC4wOTA3OTZBNzVEMTUzOTMy", "publishedAt": "2023-01-18", "description": "En el video de hoy vamos a salir un poco de la parte t\u00e9cnica y vamos a hablar del lanzamiento de las API de OpenAI con Microsoft y cuales son las consecuencias comerciales que va a tener.\n#openai #microsoft #chatgpt \n\nContenido\n00:00 - Que es lo que se anunci\u00f3?\n05:15 - C\u00f3mo es el deal que hicieron con Microsoft?\n10:10 - Cuando vamos a tener acceso a ChatGPT?\n11:30 - Como es el pricing de Azure y cuanto nos va a costar?\n16:05 - Que va a pasar ahora con el open source?\n\nLinks mencionados durante el video:\n\ud83d\udcbb \u201cChatGPT: Curso LLM-RLHF\u201d: https://www.youtube.com/playlist?list=PLWhCQi97dstnLKjstxZzEXoc1dyp4cXXB \n\ud83d\udcbb Together: https://www.together.xyz/ \n\ud83d\udcbb PETALS: https://petals.ml/ \n\ud83d\udcbb General availability of Azure OpenAI Service: https://azure.microsoft.com/en-us/blog/general-availability-of-azure-openai-service-expands-access-to-large-advanced-ai-models-with-added-enterprise-benefits/ \n\ud83d\udcbb OpenAI ChatGPT API Waitlist: https://share.hsforms.com/1u4goaXwDRKC9-x9IvKno0A4sk30 \n\ud83d\udcbb Waitlist for ChatGPT Professional access (Twitter): https://docs.google.com/forms/d/e/1FAIpQLSfCVqahRmA5OxQXbRlnSm531fTd8QBdUCwZag7mI9mrlOOIaw/viewform \n\ud83d\udcbb Inside the structure of OpenAI\u2019s looming new investment from Microsoft and VCs: https://archive.ph/c8lfw#selection-317.0-324.0 \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "AQ2TMU1eg1Q", "label": "El Amor y la Amistad, Seg\u00fan Borges [AI, \"RealBasicVSR\"]", "imagen": "https://i.ytimg.com/vi/AQ2TMU1eg1Q/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 0, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS41NkI0NEY2RDEwNTU3Q0M2", "publishedAt": "2022-03-06", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/watch?v=7K-Hk1qt_mk&ab_channel=jmgloomy.\n\ud83d\udcdd  Descripci\u00f3n Original: \"Reflexion de Borges, en una entrevista de 1980, acerca de la amistad y el amor. Para alguien muy importante.\"\n\nEste video es un ejemplo de los resultados que pueden obtenerse a trav\u00e9s del uso de VSR (Video Super Resolution). Para un tutorial paso a paso de como mejorar la calidad de tus propios videos con AI, pod\u00e9s ir a: https://github.com/machinelearnear/video-super-resolution-youtube o ver el video directamente aqui: https://studio.youtube.com/video/dpC71lWTkGw/edit\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#video #superresolution #python"}, {"url": "wM3yPjqgP98", "label": "\u201cY vol\u00f3.. y me hizo volar.. y yo vol\u00e9 de el\u201d  [AI, \"RealBasicVSR\"]", "imagen": "https://i.ytimg.com/vi/wM3yPjqgP98/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 1, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS4yODlGNEE0NkRGMEEzMEQy", "publishedAt": "2022-03-06", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/watch?v=kgCbG0q4jmc\n\ud83d\udcdd  Descripci\u00f3n Original: \"Este es un personaje que trata de contar como es que \u00e9l y su amigo se estrellaron con su moto... ni siquiera el reportero pudo aguantar la risa\u201d. Por favor visitar el canal original.\n\nEste video es un ejemplo de los resultados que pueden obtenerse a trav\u00e9s del uso de VSR (Video Super Resolution). Para un tutorial paso a paso de como mejorar la calidad de tus propios videos con AI, pod\u00e9s ir a: https://github.com/machinelearnear/video-super-resolution-youtube o ver el video directamente aqui: https://studio.youtube.com/video/dpC71lWTkGw/edit\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#video #superresolution #python"}, {"url": "ZZoB_iD-l4c", "label": "El Amor y la Amistad, Seg\u00fan Borges [AI, \"CodeFormer\"]", "imagen": "https://i.ytimg.com/vi/ZZoB_iD-l4c/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 2, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS4wMTcyMDhGQUE4NTIzM0Y5", "publishedAt": "2022-08-20", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/watch?v=7K-Hk1qt_mk&ab_channel=jmgloomy.\n\ud83d\udcdd  Descripci\u00f3n Original: \"Reflexion de Borges, en una entrevista de 1980, acerca de la amistad y el amor. Para alguien muy importante.\"\n\nEste video es un ejemplo de los resultados que pueden obtenerse a trav\u00e9s del uso de CodeFormer, un modelo que hace \"Blind Face Restoration\". \n\nPara un tutorial paso a paso de como mejorar la calidad de tus propios videos con AI, pod\u00e9s ir a: https://github.com/machinelearnear/towards_robust_blind_face_restoration o ver el video directamente aqui: https://youtu.be/wiPlHC6zbY4\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#video #superresolution #python"}, {"url": "DTlpwcTU4ns", "label": "Tato Bores y la devaluaci\u00f3n del peso en HD [AI, CodeFormer]", "imagen": "https://i.ytimg.com/vi/DTlpwcTU4ns/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 3, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS41MjE1MkI0OTQ2QzJGNzNG", "publishedAt": "2022-08-20", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/watch?v=XabgaiCrxUk&ab_channel=lottosilvia\n\ud83d\udcdd  Descripci\u00f3n Original: \" \"\n\nEste video es un ejemplo de los resultados que pueden obtenerse a trav\u00e9s del uso de CodeFormer, un modelo que hace \"Blind Face Restoration\". \n\nPara un tutorial paso a paso de como mejorar la calidad de tus propios videos con AI, pod\u00e9s ir a: https://github.com/machinelearnear/towards_robust_blind_face_restoration o ver el video directamente aqui: https://youtu.be/wiPlHC6zbY4\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#video #superresolution #python"}, {"url": "o3fsUoeJiec", "label": "\"Atiendo Boludos\" en HD [AI, CodeFormer]", "imagen": "https://i.ytimg.com/vi/o3fsUoeJiec/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 4, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS4wOTA3OTZBNzVEMTUzOTMy", "publishedAt": "2022-08-20", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/watch?v=yY7NSpZlQNc&ab_channel=InformesTVR.\n\ud83d\udcdd  Descripci\u00f3n Original: \" \"\n\nEste video es un ejemplo de los resultados que pueden obtenerse a trav\u00e9s del uso de CodeFormer, un modelo que hace \"Blind Face Restoration\". \n\nPara un tutorial paso a paso de como mejorar la calidad de tus propios videos con AI, pod\u00e9s ir a: https://github.com/machinelearnear/towards_robust_blind_face_restoration o ver el video directamente aqui: https://youtu.be/wiPlHC6zbY4\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#video #superresolution #python"}, {"url": "NGWZFXrNwVE", "label": "\"Reventado, Yo? Entrevista a Luca Prodan\" en HD [AI, CodeFormer]", "imagen": "https://i.ytimg.com/vi/NGWZFXrNwVE/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 5, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS4xMkVGQjNCMUM1N0RFNEUx", "publishedAt": "2022-08-21", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/watch?v=LhSuVGDV7Mw&ab_channel=MiguelAngelRodioBrown\n\nEste video es un ejemplo de los resultados que pueden obtenerse a trav\u00e9s del uso de CodeFormer, un modelo que hace \"Blind Face Restoration\". \n\nPara un tutorial paso a paso de como mejorar la calidad de tus propios videos con AI, pod\u00e9s ir a: https://github.com/machinelearnear/towards_robust_blind_face_restoration o ver el video directamente aqui: https://youtu.be/wiPlHC6zbY4\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#video #superresolution #python"}, {"url": "YLJuQ4ftuoQ", "label": "\"Sos inimputable, hermano\" en HD (AI, CodeFormer)", "imagen": "https://i.ytimg.com/vi/YLJuQ4ftuoQ/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 6, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS41MzJCQjBCNDIyRkJDN0VD", "publishedAt": "2022-08-26", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/watch?v=Z1B9E85_Msk&ab_channel=Blankosqui\n\nEste video es un ejemplo de los resultados que pueden obtenerse a trav\u00e9s del uso de CodeFormer, un modelo que hace \"Blind Face Restoration\". \n\nPara un tutorial paso a paso de como mejorar la calidad de tus propios videos con AI, pod\u00e9s ir a: https://github.com/machinelearnear/towards_robust_blind_face_restoration o ver el video directamente aqui: https://youtu.be/wiPlHC6zbY4\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#video #superresolution #python"}, {"url": "ot1Lof4EvZk", "label": "\"Atiendo Boludos\" pero si hubiera salido en Arcane (AI, VToonify)", "imagen": "https://i.ytimg.com/vi/ot1Lof4EvZk/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 7, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS5GNjNDRDREMDQxOThCMDQ2", "publishedAt": "2022-09-26", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/watch?v=o3fsUoeJiec&ab_channel=machinelearnear\n\ud83d\udcdd  Descripci\u00f3n: Este video es un ejemplo de los resultados que pueden obtenerse a trav\u00e9s del uso de \"VToonify: Controllable High-Resolution Portrait Video Style Transfer\"\n\nRepo original aca: https://github.com/williamyang1991/VToonify\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#styletransfer #video #vtoonify #transformers #python"}, {"url": "6CtSwbZpAow", "label": "\"Ah\u00ed veo al compa\u00f1ero de Garganta Profunda\" pero si fuera Arcane (AI, VToonify)", "imagen": "https://i.ytimg.com/vi/6CtSwbZpAow/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 8, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS40NzZCMERDMjVEN0RFRThB", "publishedAt": "2022-10-03", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/shorts/fceYMkcmt_M\n\nEste video es un ejemplo de los resultados que pueden obtenerse a trav\u00e9s del uso de VToonify. Repo ac\u00e1: https://github.com/williamyang1991/VToonify\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#Shorts #arcane #styletransfer"}, {"url": "azvuMqId5Z8", "label": "SUMO - No Tan Distintos (AI, Stable Diffusion)", "imagen": "https://i.ytimg.com/vi/azvuMqId5Z8/mqdefault.jpg", "playlistId": ["PLWhCQi97dstnHXQy-lOObayVekv2eHp0A"], "position": 9, "type": "video", "id": "UExXaENRaTk3ZHN0bkhYUXktbE9PYmF5VmVrdjJlSHAwQS5EMEEwRUY5M0RDRTU3NDJC", "publishedAt": "2022-10-03", "description": "\ud83d\udcf9  Video Original: https://www.youtube.com/watch?v=D3n9JrAcOBY&ab_channel=SumoVEVO\n\nLa intenci\u00f3n de este video es educacional. Quiero mostrar las posibilidades de trae Stable Diffusion para animar videos musicales. Sigo los pasos de este repositorio: https://github.com/dmarx/video-killed-the-radio-star hecho por Daniel Marx (@DigThatData). Aca hice un tutorial para hacer un video como este: https://www.youtube.com/watch?v=gRZsmvhoLOA&ab_channel=machinelearnear\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #musicvideo  #aivideo"}, {"url": "r-QGxuMMWfA", "label": "[#05] SAHI+DETIC: Detect\u00e1 Objetos a Gran Escala en Minutos", "imagen": "https://i.ytimg.com/vi/r-QGxuMMWfA/mqdefault.jpg", "playlistId": ["PLWhCQi97dstn3o5xgG79KcWRUeKLDmNKb"], "position": 0, "type": "video", "id": "UExXaENRaTk3ZHN0bjNvNXhnRzc5S2NXUlVlS0xEbU5LYi41NkI0NEY2RDEwNTU3Q0M2", "publishedAt": "2022-01-29", "description": "En este video vamos a ver un ejemplo r\u00e1pido de como podemos combinar la libreria SAHI junto a DETIC para hacer detecci\u00f3n de objetos a gran escala en minutos. SAHI es una libreria que nos sirve para poder detectar objetos muy peque\u00f1os en im\u00e1genes de grandes dimensiones de una forma sencilla e integrada a los mejores frameworks de CV (Detectron2, mmdet, YOLO, etc.). Detic, que vimos en otro video de @machinelearnear, nos permite detectar mas de 20 mil clases de objetos sin fine-tuning. Veremos como funcionan ambas cosas y para que situaciones nos pueden ser muy \u00fatiles.\n\nCapitulos\n00:00 - Intro\n01:30 - Que es DETIC y SAHI?\n08:19 - Intersection Over Union (IoU)\n10:14 - Como funciona NMS/Soft-NMS? \n14:10 - Que es una \u201csliced prediction\u201d?\n15:20 - Demo\n\nLinks mencionados durante el video:\n\ud83d\udcbb  Quick-start en GitHub: https://github.com/machinelearnear/large-scale-object-detection-with-sahi-detectron2\n\ud83d\udcbb  Repo de SAHI: https://github.com/obss/sahi \n\ud83d\udcdd  Paper de Detic: https://arxiv.org/abs/2201.02605 \n\ud83d\udcbb  Repo Original Detic: https://github.com/facebookresearch/Detic\n\ud83c\udf10  Intersection Over Union (IoU): https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\n\ud83d\udcdd  Paper de Soft-NMS: https://arxiv.org/abs/1704.04503\n\ud83c\udf10  Torchvision.ops.nms: https://pytorch.org/vision/stable/_modules/torchvision/ops/boxes.html#nms\n\ud83d\udcbb  DE\u2af6TR: End-to-End Object Detection with Transformers: https://github.com/facebookresearch/detr\n\ud83d\udcbb  FiftyOne: https://voxel51.com/docs/fiftyone/\n\ud83c\udf10  OpenAI CLIP: https://openai.com/blog/clip/ \n\ud83c\udf10  Google Open Images Dataset: https://opensource.google/projects/open-images-dataset\n\ud83c\udf10  LVIS Dataset: https://www.lvisdataset.org/\n\ud83d\udcf9  CS231n: What is Image Segmentation? https://www.youtube.com/watch?v=nDPWywWRIRo\n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#objectdetection #sahi #python #aws"}, {"url": "KZ551N9g99A", "label": "[#13] Open Source vs GPT-3 / Review & Demo de BLOOM-176B & OPT-175B", "imagen": "https://i.ytimg.com/vi/KZ551N9g99A/mqdefault.jpg", "playlistId": ["PLWhCQi97dstn3o5xgG79KcWRUeKLDmNKb"], "position": 1, "type": "video", "id": "UExXaENRaTk3ZHN0bjNvNXhnRzc5S2NXUlVlS0xEbU5LYi4yODlGNEE0NkRGMEEzMEQy", "publishedAt": "2022-08-12", "description": "En el video de hoy vamos hablar sobre 2 modelos de lenguaje gigantes, del mismo tama\u00f1o que GPT-3, y que cada uno demando el esfuerzo conjunto de cientos o miles de cientificos y que sali\u00f3 millones de d\u00f3lares el entrenamiento. Esto normalmente seria algo que uno no quiere compartir simplemente por el esfuerzo que demando, pero no es el caso, y ambos son completamente gratuitos y de libre acceso para la comunidad. Vamos a hablar de eso hoy.1\n\nContenido\n00:00 - GPT qui\u00e9n?\n03:38 - BLOOM-176B (BigScience) & Demo\n11:45 - OPT-175B (Meta) & Demo\n18:42 - \"bnb-int8\" demo!\n22:30 - Conclusiones\n\nAlgunos links mencionados durante el video:\n\ud83e\udd17 Big Science Bloom: https://huggingface.co/bigscience/bloom\n\ud83d\udcbb Open Pre-trained Transformers (Meta): https://github.com/facebookresearch/metaseq/tree/main/projects/OPT\n\ud83d\udcbb OPT ALPA Demo: https://opt.alpa.ai/#generation\n\ud83d\udcdd \u201cbnb-int8 for\u201d Hugging Face Models Public Beta: https://docs.google.com/document/d/1JxSo4lQgMDBdnd19VBEoaG-mMfQupQ3XvOrgmRAVtpU/edit#heading=h.w0hver14j8hf\n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#python #generativeart #gpt3 #bloom #huggingface"}, {"url": "f_YomnlexV4", "label": "[#16] C\u00f3mo borrar el fondo a una imagen en segundos con AI (Unsupervised Salient Object Detection)", "imagen": "https://i.ytimg.com/vi/f_YomnlexV4/mqdefault.jpg", "playlistId": ["PLWhCQi97dstn3o5xgG79KcWRUeKLDmNKb", "PLWhCQi97dstk-n_W_Eh90b_7tLWayDqcr"], "position": 2, "type": "video", "id": "UExXaENRaTk3ZHN0bjNvNXhnRzc5S2NXUlVlS0xEbU5LYi4wMTcyMDhGQUE4NTIzM0Y5", "publishedAt": "2022-08-17", "description": "Hoy vamos a aprender a detectar el objeto mas importante/relevante en una foto y a hacer algo interesante con eso, como por ejemplo sacarle el fondo o todo lo que no es ese objeto. Para eso vamos a ver 2 papers que hacen mas o menos lo mismo y son el estado del arte en este momento. Uno para objetos conocidos y otros no. \n\nContenido\n00:00 - Qu\u00e9 es \u201cSalient Object Detection\u201d?\n03:14 - Paper & Demo: \u201cHighly Accurate Dichotomous Image Segmentation\u201d\n06:12 - Paper & Demo: \"Unsupervised Salient Object Detection with Spectral Cluster Voting\u201d\n09:39 - Meta AI DINO\n12:45 - Conclusiones\n\nLinks mencionados durante el video:\n\ud83d\udcbb PapersWithCode Saliency Detection: https://paperswithcode.com/task/saliency-detection \n\ud83d\udcbb Open HF spaces in Studio Lab: https://github.com/machinelearnear/open-hf-spaces-in-studiolab \n\ud83d\udcbb \u201cU^2-Net: Going Deeper with Nested U-Structure for Salient Object Detection\u201d: https://github.com/xuebinqin/U-2-Net \n\ud83d\udcbb Demo \u201cHighly Accurate Dichotomous Image Segmentation\u201d: https://huggingface.co/spaces/doevent/dis-background-removal \n\ud83d\udcdd Paper \u201cUnsupervised Salient Object Detection with Spectral Cluster Voting\u201d: https://arxiv.org/pdf/2203.12614.pdf \n\ud83e\udd17 Demo en Hugging Face: https://huggingface.co/spaces/noelshin/selfmask \n\ud83d\udcbb SSL Demos (Meta): https://ssl-demos.metademolab.com/home\n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#python #huggingface #salientfeatures #imagesegmentation"}, {"url": "mf21HbO13is", "label": "[#44] Petals: Corr\u00e9 modelos de +100B parametros (GPT/BLOOM-176B) en tu casa, estilo BitTorrent", "imagen": "https://i.ytimg.com/vi/mf21HbO13is/mqdefault.jpg", "playlistId": ["PLWhCQi97dstn3o5xgG79KcWRUeKLDmNKb", "PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw"], "position": 3, "type": "video", "id": "UExXaENRaTk3ZHN0bjNvNXhnRzc5S2NXUlVlS0xEbU5LYi41MjE1MkI0OTQ2QzJGNzNG", "publishedAt": "2023-01-09", "description": "Hoy vamos a ver una nueva libreria que se acaba de publicar que se llama \u201cPETALS\u201d y esta hecha para poder correr modelos de language gigantes, de mas de 100 mil millones de parametros, de forma colaborativa tipo BitTorrent. Se acuerdan de ChatGPT o de GPT-3? Bueno, ese tipo de modelos. Vamos a ver como funciona.\n#huggingface #chatgpt #gpt3 \n\nContenido\n00:00 - Qu\u00e9 es BLOOM-176B y qu\u00e9 son los LLM?\n14:35 - Un poco de historia.. SETI@Home y BitTorrent\n18:03 - C\u00f3mo funciona Petals?\n19:55 - Demo en vivo\n\nLink a Notebook:\nhttps://colab.research.google.com/drive/1Ervk6HPNS6AYVr3xVdQnY5a-TjjmLCdQ?usp=sharing \n\nLinks mencionados durante el video:\n\ud83d\udcbb Petals: https://github.com/bigscience-workshop/petals \n\ud83d\udcbb PETALS: Collaborative Inference and Fine-tuning of Large Models: https://arxiv.org/pdf/2209.01188.pdf \n\ud83d\udcbb BigScience Large Open-science Open-access Multilingual Language Model: https://huggingface.co/bigscience/bloom \n\ud83d\udcbb [#13] Open Source vs GPT-3 / Review & Demo de BLOOM-176B & OPT-175B: https://www.youtube.com/watch?v=KZ551N9g99A \n\ud83d\udcbb Two minutes NLP \u2014 Most used Decoding Methods for Language Models: https://medium.com/nlplanet/two-minutes-nlp-most-used-decoding-methods-for-language-models-9d44b2375612 \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "uf9bWRxtBgY", "label": "[#03] Como detectar un choripan con ML paso a paso (OpenImages/Mask-RCNN)", "imagen": "https://i.ytimg.com/vi/uf9bWRxtBgY/mqdefault.jpg", "playlistId": ["PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw"], "position": 0, "type": "video", "id": "UExXaENRaTk3ZHN0bGtFeGFHUGdUWlBFY01LcnFPdU50dy41NkI0NEY2RDEwNTU3Q0M2", "publishedAt": "2022-01-07", "description": "En este video vamos a hacer un tutorial paso a paso sobre como entrenar tu propio modelo de segmentaci\u00f3n de im\u00e1genes en menos de 10 minutos. Vamos a estar usando la libreria de IceVision, un dataset open-source que se llama OpenImages, y el algoritmo Mask-RCNN. \n\nCapitulos\n00:00 - Intro\n01:16 - Que es la segmentaci\u00f3n de im\u00e1genes?\n04:22 - Que es el dataset de OpenImages?\n05:58 - Que es IceVision?\n08:45 - Preparar dataset y convertir a formato 'COCO'\n14:35 - Entrenamiento del modelo \n20:15 - Inferencia sobre una imagen de muestra\n\nLinks mencionados durante el video:\n\ud83d\udcbb  Tutorial en GitHub: https://github.com/machinelearnear/custom-segmentation-model-with-icevision-openimages\n\ud83d\udcbb  IceVision: https://github.com/airctic/icevision\n\ud83c\udf10  Google Open Images Dataset: https://opensource.google/projects/open-images-dataset\n\ud83c\udf10  LVIS Dataset: https://www.lvisdataset.org/\n\ud83c\udf10  COCO Dataset Format Walkthrough: https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch/#coco-dataset-format\n\ud83d\udcf9  COCO Dataset Format Walkthrough: https://www.youtube.com/watch?v=h6s61a_pqfM\n\ud83d\udcf9  CS231n: Convolutional Neural Networks for Visual Recognition - Image Segmentation: https://www.youtube.com/watch?v=nDPWywWRIRo\n\u270f\ufe0f  Train a Choripan Classifier with Fastai and Google Colab (2018): https://medium.com/@nicolas.metallo/train-a-choripan-classifier-with-fast-ai-v1-in-google-colab-6e438817656a\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#openimages #choripan #detectar"}, {"url": "EMrEsn13g9Y", "label": "[#06] C\u00f3mo usar Gradio & Streamlit con SageMaker Studio Lab", "imagen": "https://i.ytimg.com/vi/EMrEsn13g9Y/mqdefault.jpg", "playlistId": ["PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw"], "position": 1, "type": "video", "id": "UExXaENRaTk3ZHN0bGtFeGFHUGdUWlBFY01LcnFPdU50dy4yODlGNEE0NkRGMEEzMEQy", "publishedAt": "2022-03-14", "description": "En el video de hoy, vamos a ver como usar Gradio y Streamlit con Studio Lab. Si no sabes de lo que estoy hablando, no te preocupes, porque vamos a explicar que es cada cosa, para que sirve, y como usarlo.\n\nCapitulos\n00:35 - Intro\n03:42 - Que es Gradio?\n06:50 - Que es Streamlit?\n08:45 - Tutorial paso a paso\n12:25 - Demo\n\nLinks mencionados durante el video:\n\ud83d\udcbb  Quick-start en GitHub: https://github.com/machinelearnear/use-gradio-streamlit-sagemaker-studiolab\n\ud83d\udcf9  Introducci\u00f3n a Amazon SageMaker Studio Lab: https://www.youtube.com/watch?v=FUEIwAsrMP4\n\ud83d\udcbb  Monocular Depth Estimation: https://huggingface.co/spaces/keras-io/Monocular-Depth-Estimation \n\ud83c\udf10  Streamlit: https://streamlit.io \n\ud83c\udf10  Gradio: https://gradio.app \n\ud83d\udcbb  How to deploy Streamlit app on Fargate with AWS CDK: https://github.com/nicolasmetallo/deploy-streamlit-on-fargate-with-aws-cdk \n\ud83d\udcbb  Use TensorBoard with SageMaker Studio: https://docs.aws.amazon.com/sagemaker/latest/dg/studio-tensorboard.html \n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#studiolab #streamlit #gradio #aws"}, {"url": "skaYsdSuE70", "label": "[#08] C\u00f3mo usar \ud83e\udd17\u00a0Spaces + GPU sin costo en minutos con SM Studio Lab", "imagen": "https://i.ytimg.com/vi/skaYsdSuE70/mqdefault.jpg", "playlistId": ["PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw"], "position": 2, "type": "video", "id": "UExXaENRaTk3ZHN0bGtFeGFHUGdUWlBFY01LcnFPdU50dy4wMTcyMDhGQUE4NTIzM0Y5", "publishedAt": "2022-04-08", "description": "En este video, lo que vamos a hacer es a conectar cualquier aplicacion que esta hosteada en Hugging Face Spaces con SageMaker Studio Lab de manera que vamos a tener acceso a una GPU sin costo. Vamos a ver como hacer esto paso a paso.\n\nContenido\n00:00 - Intro \n01:18 - Que es HF Spaces?\n08:55 - Tutorial paso a paso\n10:51 - Demo\n\nRepo paso a paso:\n\ud83d\udcbb  https://github.com/machinelearnear/open-hf-spaces-in-studiolab\n\nLinks mencionados durante el video:\n\ud83d\udcf9  SageMaker Studio Lab Explainer Video: https://www.youtube.com/watch?v=FUEIwAsrMP4 \n\ud83d\udcbb  Use Gradio or Streamlit on SageMaker Studio Lab: https://github.com/machinelearnear/use-gradio-streamlit-sagemaker-studiolab\n\ud83d\udcbb  Hugging Face Spaces Documentation: https://huggingface.co/docs/hub/spaces#reference\n\ud83d\udcbb  Gradio Documentation: https://gradio.app/getting_started/\n\ud83d\udcbb  Streamlit Documentation: https://docs.streamlit.io/\n\ud83d\udcf9  [#02] GLIDE: Gener\u00e1 y edit\u00e1 im\u00e1genes en segundos en base a lo que escribis (+ Repo): https://www.youtube.com/watch?v=WG20CnktPbk\n\ud83d\udcdd  Restormer: Efficient Transformer for High-Resolution Image Restoration Demo:  https://huggingface.co/spaces/swzamir/Restormer\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#studiolab #streamlit #gradio"}, {"url": "wFjPxz22MEs", "label": "[#09] Reconocimiento autom\u00e1tico de voz con Python y HuggingFace en segundos (+ Repo)", "imagen": "https://i.ytimg.com/vi/wFjPxz22MEs/mqdefault.jpg", "playlistId": ["PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw"], "position": 3, "type": "video", "id": "UExXaENRaTk3ZHN0bGtFeGFHUGdUWlBFY01LcnFPdU50dy41MjE1MkI0OTQ2QzJGNzNG", "publishedAt": "2022-05-02", "description": "En el video de hoy vamos a movernos al campo del procesamiento de audio y vamos a ver como hacer reconocimiento autom\u00e1tico de voz en audios largos como podcasts, entrevistas, audiolibros, etc. Vamos a aprender como buscar el mejor modelo open source disponible para el lenguaje que queremos utilizar, como implementarlo, y finalmente, como se compara contra otras soluciones, por ejemplo contra la transcripci\u00f3n autom\u00e1tica de Google en videos de YouTube. \n\nContenido\n00:00 - Intro \n01:22 - Que es Automatic Speech Recognition (ASR)?\n04:45 - Demo en Hugging Face y Mozilla CommonVoice\n07:50 - Cual es el problema de los grandes archivos?\n13:15 - Como lo resolvemos?\n19:19 - Comparando resultados vs Google\n\nRepo paso a paso:\n\ud83d\udcbb  https://github.com/machinelearnear/long-audio-transcription-spanish\n\nLinks mencionados durante el video:\n\ud83d\udcf9  SageMaker Studio Lab Explainer Video: https://www.youtube.com/watch?v=FUEIwAsrMP4 \n\ud83d\udcbb  Intro to Automatic Speech Recognition on \ud83e\udd17: https://huggingface.co/tasks/automatic-speech-recognition\n\ud83d\udcbb  Robust Speech Challenge Results on \ud83e\udd17: https://huggingface.co/spaces/speech-recognition-community-v2/FinalLeaderboard\n\ud83d\udcbb  Mozilla Common Voice 9.0: https://huggingface.co/datasets/mozilla-foundation/common_voice_9_0\n\ud83d\udcbb  Thunder-speech, A Hackable speech recognition library: https://scart97.github.io/thunder-speech/Ultimate%20guide/\n\ud83d\udcbb  SpeechBrain - PyTorch powered speech toolkit: https://speechbrain.github.io/\n\ud83d\udcbb  Neural building blocks for speaker diarization: speech activity detection, speaker change detection, overlapped speech detection, speaker embedding: https://github.com/pyannote/pyannote-audio\n\ud83d\udcbb  SPEECH RECOGNITION WITH WAV2VEC2: https://pytorch.org/tutorials/intermediate/speech_recognition_pipeline_tutorial.html\n\ud83d\udcbb  How to add timestamps to ASR output: https://github.com/huggingface/transformers/issues/11307\n\ud83d\udcbb  Host Hugging Face transformer models using Amazon SageMaker Serverless Inference: https://aws.amazon.com/de/blogs/machine-learning/host-hugging-face-transformer-models-using-amazon-sagemaker-serverless-inference/\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#audio #huggingface #reconocimiento #python"}, {"url": "_0KGck2JU0w", "label": "[#11] Como hacer tu propia soluci\u00f3n de dictado autom\u00e1tico de informes m\u00e9dicos (+ repo)", "imagen": "https://i.ytimg.com/vi/_0KGck2JU0w/mqdefault.jpg", "playlistId": ["PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw"], "position": 4, "type": "video", "id": "UExXaENRaTk3ZHN0bGtFeGFHUGdUWlBFY01LcnFPdU50dy4xMkVGQjNCMUM1N0RFNEUx", "publishedAt": "2022-05-15", "description": "En el video de hoy vamos a charlar sobre como podes hacer tu propio soluci\u00f3n de dictado automatico por voz de informes medicos. Nos vamos a enfocar en ver que componentes estan disponibles de forma open-source, como los podemos usar juntos, como se veria una posible arquitectura en AWS, y que costo estimado tendria. Finalmente vamos a ver cuales son las limitaciones y como se podria mejorar la precisi\u00f3n de estos sistemas.\n\nContenido\n00:00 - Introducci\u00f3n a dictado x voz\n05:50 - Arreglar puntuaci\u00f3n y gram\u00e1tica\n12:05 - Detectar entidades medicas (NER)\n20:40 - Como hacer res\u00famenes\n22:00 - Arquitectura r\u00e1pida en AWS\n26:10 - Conclusion y pr\u00f3ximos pasos\n\nRepo paso a paso:\n\ud83d\udcbb  https://github.com/machinelearnear/asr-restore-punctuation-summarization-biomedical-ehr\n\nLinks mencionados durante el video:\n\ud83d\udcf9  Reconocimiento autom\u00e1tico de voz con Python y HuggingFace en segundos (+ Repo): https://www.youtube.com/watch?v=wFjPxz22MEs\n\ud83d\udcbb  \u201cSomosNLP\u201d, red internacional de estudiantes, profesionales e investigadores acelerando el avance del NLP en espa\u00f1ol: https://somosnlp.org/\n\ud83d\udcbb  How to Write a Spelling Corrector: https://norvig.com/spell-correct.html \n\ud83d\udcbb  Build Spell Checking Models For Any Language In Python: https://medium.com/mlearning-ai/build-spell-checking-models-for-any-language-in-python-aa4489df0a5f \n\ud83d\udcbb  Grammatical Error Correction: http://nlpprogress.com/english/grammatical_error_correction.html \n\ud83d\udcbb  Deep Multilingual Punctuation Prediction: https://github.com/oliverguhr/deepmultilingualpunctuation \n\ud83d\udcdd  FullStop: Multilingual Deep Models for Punctuation Prediction: http://ceur-ws.org/Vol-2957/sepp_paper4.pdf\n\ud83e\udd17  flexudy/t5-base-multi-sentence-doctor: https://huggingface.co/flexudy/t5-base-multi-sentence-doctor\n\ud83e\udd17  BioMedIA: Abstractive Question Answering for the BioMedical Domain in Spanish: https://huggingface.co/spaces/hackathon-pln-es/BioMedIA\n\ud83e\udd17  PlanTL-GOB-ES/bsc-bio-ehr-es-pharmaconer: https://huggingface.co/PlanTL-GOB-ES/bsc-bio-ehr-es-pharmaconer \n\ud83e\udd17  PlanTL-GOB-ES/bsc-bio-ehr-es-cantemist: https://huggingface.co/PlanTL-GOB-ES/bsc-bio-ehr-es-cantemist\n\ud83e\udd17  mrm8488/bioclinical-roberta-es-finenuned-clinical-ner: https://huggingface.co/mrm8488/bioclinical-roberta-es-finenuned-clinical-ner\n\ud83d\udcbb  Stanza model for Spanish (es): https://github.com/stanfordnlp/stanza \n\ud83d\udcbb  Host Hugging Face transformer models using Amazon SageMaker Serverless Inference: https://aws.amazon.com/de/blogs/machine-learning/host-hugging-face-transformer-models-using-amazon-sagemaker-serverless-inference/\n\ud83d\udcbb  Define and run Machine Learning pipelines on Step Functions using Python, Workflow Studio, or States Language: https://aws.amazon.com/blogs/machine-learning/define-and-run-machine-learning-pipelines-on-step-functions-using-python-workflow-studio-or-states-language/\n\ud83d\udcbb  AWS Amplify: https://aws.amazon.com/amplify/ \n\nReferences\n- Cover image: Photo by Irwan iwe (https://unsplash.com/photos/rbDE93-0hHs) \n- YouTube thumbnail: Photo by Waldemar Brandt (https://unsplash.com/photos/dqhmbjutkQA)\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#dictado #voz #python #huggingface #medicina #espa\u00f1ol #aws"}, {"url": "wiPlHC6zbY4", "label": "[#17] Como restaurar tus videos a HD paso a paso (CodeFormer: Robust Blind Face Restoration)", "imagen": "https://i.ytimg.com/vi/wiPlHC6zbY4/mqdefault.jpg", "playlistId": ["PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw", "PLWhCQi97dstk-n_W_Eh90b_7tLWayDqcr"], "position": 5, "type": "video", "id": "UExXaENRaTk3ZHN0bGtFeGFHUGdUWlBFY01LcnFPdU50dy5DQUNERDQ2NkIzRUQxNTY1", "publishedAt": "2022-08-21", "description": "Hoy vamos a ver hablar nuevamente de \"Super Resolution\" pero en este caso con un foco en aumentar la calidad de las caras que son detectadas en un video. No vamos a preocuparnos tanto por el fondo, sino por las personas. Para eso vamos a implementar uno de los \u00faltimos algoritmos que salieron sobre esto, CodeFormer, que es el estado del arte. Como queria probarlo con videos viejos, tambi\u00e9n hice un tutorial paso a paso de como bajar un video de YouTube, split en varios frames, correr el modelo contra cada frame, y finalmente, guardar una nueva versi\u00f3n con mayor calidad. \n\nContenido\n00:00 - Qu\u00e9 es \u201cBlind Face Restoration\u201d? GFPGAN?\n05:05 - Demo y Resultados\n07:27 - Paper: \u201cTowards Robust Blind Face Restoration with Codebook Lookup Transformer\u201d\n11:33 - Paso a Paso C\u00f3digo\n15:22 - Conclusiones\n\nRepositorio con todos los links:\n\ud83d\udcbb https://github.com/machinelearnear/towards_robust_blind_face_restoration\n\nLinks mencionados durante el video:\n\ud83d\udcdd Towards Robust Blind Face Restoration with Codebook Lookup Transformer:  https://arxiv.org/abs/2206.11253\n\ud83d\udcbb Repo CodeFormer: https://github.com/sczhou/CodeFormer\n\ud83d\udcbb Google Colab: https://colab.research.google.com/drive/1m52PNveE4PBhYrecj34cnpEeiHcC5LTb?usp=sharing\n\ud83d\udcbb GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior: https://github.com/TencentARC/GFPGAN \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#python #superresolution #upscaling"}, {"url": "0nOMMwwNCYk", "label": "[#23] Esta AI entiende cualquier idioma (OpenAI Whisper)", "imagen": "https://i.ytimg.com/vi/0nOMMwwNCYk/mqdefault.jpg", "playlistId": ["PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw", "PLWhCQi97dstk-n_W_Eh90b_7tLWayDqcr"], "position": 6, "type": "video", "id": "UExXaENRaTk3ZHN0bGtFeGFHUGdUWlBFY01LcnFPdU50dy40NzZCMERDMjVEN0RFRThB", "publishedAt": "2022-09-24", "description": "Hoy vamos a hablar sobre un nuevo modelo que sac\u00f3 OpenAI que se llama Whisper. Whisper fue entrenado con casi 700 mil horas de audio, muy diverso, y esto hizo que pueda entender muchisimos idiomas muy bien. Tambien, de paso, se encarga de arreglar la puntuaci\u00f3n y la gram\u00e1tica. Vamos a ver varias demos de como funciona y como lo pueden implementar en un Google Colab.\n\nContenido\n00:00 - Que es \u201cWhisper\u201d y c\u00f3mo funciona?\n03:00 - Demos con tiempo real\n07:15 - Google Colab\n11:35 - Conclusiones\n\nLink a Google Colab por OpenAI:\n\ud83d\udcbb https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb\n\nLinks mencionados durante el video:\n\ud83d\udcbb Repo Oficial de OpenAI: https://github.com/openai/whisper\n\ud83d\udcbb Spaces de OpenAI Whisper: https://huggingface.co/spaces/openai/whisper\n\ud83d\udcbb OpenAI Whisper WebUI: https://huggingface.co/spaces/aadnk/whisper-webui\n\ud83d\udcbb OpenAI Whisper + YouTube: https://huggingface.co/spaces/jeffistyping/Youtube-Whisperer\n\ud83d\udcbb Review por Andrej Karpathy: https://twitter.com/karpathy/status/1573019730851397632\n\ud83d\udcf9 \u201cWhisper: Robust Speech Recognition via Large-Scale Weak Supervision | Paper and Code Explained\u201d: https://www.youtube.com/watch?v=AwJf8aQfChE \n\ud83d\udcbb Speaker diarization: speech activity detection, speaker change detection, overlapped speech detection, speaker embedding: https://github.com/pyannote/pyannote-audio \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#openai #whisper #asr #reconocimiento #voz"}, {"url": "dvr1sqyhH8c", "label": "[#27] \"Toonific\u00e1\" tus videos con esta AI (Tutorial VToonify)", "imagen": "https://i.ytimg.com/vi/dvr1sqyhH8c/mqdefault.jpg", "playlistId": ["PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw", "PLWhCQi97dstk-n_W_Eh90b_7tLWayDqcr"], "position": 7, "type": "video", "id": "UExXaENRaTk3ZHN0bGtFeGFHUGdUWlBFY01LcnFPdU50dy4zMDg5MkQ5MEVDMEM1NTg2", "publishedAt": "2022-10-04", "description": "Hoy vamos a charlar sobre una nueva AI que hace algo asi como transferencia de estilo pero generando imagenes nuevas con unas arquitectura que combina GAN o redes adversariales con translaci\u00f3n de imagen. Todo esto que es una forma compleja de decir que podes hacer dibujitos en base a la gente que sale en tus videos.\n\nContenido\n00:00 - Qu\u00e9 es \u201ctoonificar\u201d y qu\u00e9 es VToonify?\n08:07 - Como hacer la inferencia paso a paso\n12:00 - Conclusiones\n\nLink a Notebook:\n\ud83d\udcbb https://github.com/machinelearnear/vtoonify-in-studio-lab \n\nLinks mencionados durante el video:\n\ud83d\udcbb Thread @Shuai Yang: https://twitter.com/ShuaiYang1991/status/1577251207155838976\n\ud83d\udcbb VToonify - Official PyTorch Implementation: https://github.com/williamyang1991/VToonify \n\ud83d\udcbb VToonify Project Page: https://www.mmlab-ntu.com/project/vtoonify \n\ud83d\udcbb VToonify Spaces: https://huggingface.co/spaces/PKUWilliamYang/VToonify\n\ud83d\udcbb Portrait Style Transfer with DualStyleGAN: https://huggingface.co/spaces/CVPR/DualStyleGAN \n\ud83d\udcbb [CVPR 2022] Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer: https://github.com/williamyang1991/DualStyleGAN\n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#styletransfer #vtoonify #aiart"}, {"url": "Nzwy_e8F3q0", "label": "[#31] Google acaba de mejorar Stable Diffusion (Tutorial de Prompt-to-Prompt)", "imagen": "https://i.ytimg.com/vi/Nzwy_e8F3q0/mqdefault.jpg", "playlistId": ["PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw"], "position": 8, "type": "video", "id": "UExXaENRaTk3ZHN0bGtFeGFHUGdUWlBFY01LcnFPdU50dy41QTY1Q0UxMTVCODczNThE", "publishedAt": "2022-10-16", "description": "Vamos a ver un nuevo paper y c\u00f3digo que sac\u00f3 Google (Prompt-to-Prompt Image Editing) donde usan cross-attention para cambiar un estilo artistico o solo una parte de una imagen generada con Stable Diffusion, pero igual manteniendo el contenido intacto.\n\nContenido\n00:00 - Que es la edici\u00f3n con cross-attention?\n07:50 - Demo paso a paso\n\nLink a Notebook:\nhttps://colab.research.google.com/github/google/prompt-to-prompt/blob/main/prompt-to-prompt_stable.ipynb\n\nLinks mencionados durante el video:\n\ud83d\udcbb Google's AI: Stable Diffusion On Steroids! \ud83d\udcaa: https://www.youtube.com/watch?v=XW_nO2NMH_g&ab_channel=TwoMinutePapers \n\ud83d\udcbb Prompt-to-Prompt Image Editing with Cross-Attention Control: https://prompt-to-prompt.github.io/\n\ud83d\udcbb Repo Google: https://github.com/google/prompt-to-prompt\n\ud83d\udcbb Prompt-to-Prompt: An efficient implementation: https://github.com/cccntu/efficient-prompt-to-prompt \n\ud83d\udcbb Cross Attention Control with Stable Diffusion: https://github.com/bloc97/CrossAttentionControl \n\ud83d\udcbb What the DAAM: https://arxiv.org/pdf/2210.04885.pdf \n\ud83d\udcbb What the DAAM Repo: https://github.com/castorini/daam\n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#stablediffusion #google #prompt-to-prompt"}, {"url": "WG20CnktPbk", "label": "[#02] GLIDE: Gener\u00e1 y edit\u00e1 im\u00e1genes en segundos en base a lo que escribis (+ Repo)", "imagen": "https://i.ytimg.com/vi/WG20CnktPbk/mqdefault.jpg", "playlistId": ["PLWhCQi97dstk-n_W_Eh90b_7tLWayDqcr"], "position": 0, "type": "video", "id": "UExXaENRaTk3ZHN0ay1uX1dfRWg5MGJfN3RMV2F5RHFjci41NkI0NEY2RDEwNTU3Q0M2", "publishedAt": "2021-12-28", "description": "En este video vamos a experimentar un poco con un nuevo paper que sac\u00f3 OpenAI llamado \"GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models\" donde usan text-guided diffusion models para generar y editar imagenes. Este nuevo paper es una mejora adicional sobre DALL-E, que habia sido lanzado al p\u00fablico hace aproximadamente 1 a\u00f1o tambi\u00e9n por OpenAI.\n\nIndice:\n00:00 - Intro\n00:38 - Que es GLIDE?\n05:16 - Porque es importante y de donde viene?\n09:18 - Como funciona GLIDE?\n12:40 - OpenAI's GLIDE: Demo\n\nLinks mencionados durante el video:\n- Repositorio de GitHub: https://github.com/machinelearnear/openai-glide-text2im\n- Paper Original: https://arxiv.org/pdf/2112.10741.pdf\n- Paper \"Diffusion Models Beat GANs on Image Synthesis\": https://arxiv.org/pdf/2105.05233.pdf\n- Paper Explained by Yannic \"Diffusion Models Beat GANs on Image Synthesis\": https://www.youtube.com/watch?v=W-O7AZNzbzQ\n- Post sobre GLIDE en Reddit (r/bigsleep): https://www.reddit.com/r/bigsleep/comments/rl5rgw/openai_paper_glide_towards_photorealistic_image/\n- Comunidad de Reddit: https://www.reddit.com/r/MediaSynthesis/\n- DALL-E: https://openai.com/blog/dall-e/\n- \"What are Diffusion Models?\" https://lilianweng.github.io/lil-log/2021/07/11/diffusion-models.html\n- \"A new SotA for generative modelling \u2014 Denoising Diffusion Probabilistic Models\": https://medium.com/graphcore/a-new-sota-for-generative-modelling-denoising-diffusion-probabilistic-models-8e21eec6792e\n- \"High Fidelity Image Generation Using Diffusion Models\" (Google): https://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html\n- (Nuevo) Paper explicado por Yannic: https://www.youtube.com/watch?v=gwI6g1pBD84\n- (Nuevo) Paper explicado por The AI Epiphany: https://www.youtube.com/watch?v=lvv4N2nf-HU\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#multimodal #art #glide #python"}, {"url": "Xw2icCN5ZpM", "label": "[#04] DETIC: Detectar +20K Clases de Objetos Sin Fine-tuning (+ Repo)", "imagen": "https://i.ytimg.com/vi/Xw2icCN5ZpM/mqdefault.jpg", "playlistId": ["PLWhCQi97dstk-n_W_Eh90b_7tLWayDqcr"], "position": 1, "type": "video", "id": "UExXaENRaTk3ZHN0ay1uX1dfRWg5MGJfN3RMV2F5RHFjci4yODlGNEE0NkRGMEEzMEQy", "publishedAt": "2022-01-14", "description": "En este video vamos a hacer un review a un paper que acaba de publicar Facebook donde expanden la cantidad de clases que un modelo de detecci\u00f3n de objetos puede encontrar de muy pocas a muchas gracias a que hacen un entrenamiento con las 21 mil clases que tiene ImageNet. Esto nunca se habia hecho antes. De esta forma puede generalizar muy bien a datasets nuevos sin necesidad de fine-tuning y, por si fuera poco, tambien lo combinan con CLIP para expandir la detecci\u00f3n a clases desconocidas durante el entrenamiento y terminan obteniendo una mejor performance en la detecci\u00f3n para datasets conocidos. \n\nCapitulos\n00:00 - Intro\n01:25 - Que es Detic?\n06:30 - Mini review del paper\n12:40 - Demo\n\nLinks mencionados durante el video:\n\ud83d\udcbb  Quickstart en GitHub: https://github.com/machinelearnear/detic-detecting-20k-classes-using-image-level-supervision\n\ud83d\udcdd  Paper: https://arxiv.org/abs/2201.02605 \n\ud83d\udcbb  Repo Original Detic: https://github.com/facebookresearch/Detic\n\ud83d\udcbb  Web Demo Detic: https://huggingface.co/spaces/akhaliq/Detic\n\ud83d\udcbb  DETR: End-to-End Object Detection with Transformers: https://github.com/facebookresearch/detr\n\ud83d\udcbb  FiftyOne: https://voxel51.com/docs/fiftyone/\n\ud83c\udf10  OpenAI CLIP: https://openai.com/blog/clip/ \n\ud83c\udf10  Google Open Images Dataset: https://opensource.google/projects/open-images-dataset\n\ud83c\udf10  LVIS Dataset: https://www.lvisdataset.org/\n\ud83d\udcf9  DETR: End-to-End Object Detection with Transformers (Paper Explained): https://www.youtube.com/watch?v=T35ba_VXkMY \n\ud83d\udcf9  CS231n: What is Image Segmentation? https://www.youtube.com/watch?v=nDPWywWRIRo\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#detic #objectdetection #python"}, {"url": "dpC71lWTkGw", "label": "[#07] RealBasicVSR: Aument\u00e1 la calidad de tus videos con AI (Video Super Resolution + Repo)", "imagen": "https://i.ytimg.com/vi/dpC71lWTkGw/mqdefault.jpg", "playlistId": ["PLWhCQi97dstk-n_W_Eh90b_7tLWayDqcr"], "position": 2, "type": "video", "id": "UExXaENRaTk3ZHN0ay1uX1dfRWg5MGJfN3RMV2F5RHFjci4wMTcyMDhGQUE4NTIzM0Y5", "publishedAt": "2022-03-19", "description": "En este video vamos a hacer un review a un paper (\u201cInvestigating Tradeoffs in Real-World Video Super-Resolution\u201d) que acaba de ser aceptado en CVPR 2022 que es, por ahora, el estado del arte para mejorar la calidad de un video. Aproveche de paso para conectarlo a YouTube y subi 2 ejemplos de los resultados, que son bastante buenos! Vamos a ver de que se trata!\n\nCapitulos\n00:00 - Intro\n05:18 - Que es \u201csuper resoluci\u00f3n\u201d?\n11:22 - Mini review del paper\n16:16 - Implementaci\u00f3n / Demo\n\nRepo paso a paso:\n\ud83d\udcbb  https://github.com/machinelearnear/video-super-resolution-youtube \n\nLinks mencionados durante el video:\n\ud83d\udcf9  El Amor y la Amistad, Seg\u00fan Borges (Mejorado con AI, RealBasicVSR): https://www.youtube.com/watch?v=AQ2TMU1eg1Q \n\ud83d\udcf9  \u201cY vol\u00f3.. y me hizo volar.. y yo vol\u00e9 de el\u201d (Mejorado con AI, RealBasicVSR): https://www.youtube.com/watch?v=wM3yPjqgP98 \n\ud83d\udcdd  Paper: https://arxiv.org/pdf/2111.12704.pdf \n\ud83d\udcbb  Repo oficial: https://github.com/ckkelvinchan/RealBasicVSR \n\ud83d\udcf9  How Super Resolution Works: https://www.youtube.com/watch?v=KULkSwLk62I&ab_channel=LeoIsikdogan\n\ud83d\udcf9  Enhance! Super Resolution Is Here!: https://www.youtube.com/watch?v=WCAF3PNEc_c&ab_channel=TwoMinutePapers\n\ud83d\udcbb  MMEditing: https://github.com/open-mmlab/mmediting\n\ud83d\udcbb  Structural Similarity Index: https://scikit-image.org/docs/dev/auto_examples/transform/plot_ssim.html#id4 \n\ud83c\udf10  NTIRE 2021 \"New Trends in Image Restoration and Enhancement\" Workshop: https://openaccess.thecvf.com/CVPR2021_workshops/NTIRE\n\ud83d\udcdd  VRT: A Video Restoration Transformer: https://arxiv.org/pdf/2108.10257.pdf\n\ud83c\udf10  SwinIR: Image Restoration Using Swin Transformer: https://huggingface.co/spaces/akhaliq/SwinIR\n\ud83d\udcdd  Perceptual Losses for Deep Image Restoration: https://towardsdatascience.com/perceptual-losses-for-image-restoration-dd3c9de4113 \n\ud83d\udcdd  Image Super-Resolution via Iterative Refinement: https://iterative-refinement.github.io/\n\ud83d\udcdd  Restormer: Efficient Transformer for High-Resolution Image Restoration: https://arxiv.org/abs/2111.09881 y https://github.com/swz30/Restormer \n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#superresolution #video #python #aws"}, {"url": "HNCWOS8A_MA", "label": "[#14] Donut \ud83c\udf69 OCR-free Document Understanding Transformer", "imagen": "https://i.ytimg.com/vi/HNCWOS8A_MA/mqdefault.jpg", "playlistId": ["PLWhCQi97dstk-n_W_Eh90b_7tLWayDqcr"], "position": 3, "type": "video", "id": "UExXaENRaTk3ZHN0ay1uX1dfRWg5MGJfN3RMV2F5RHFjci41MjE1MkI0OTQ2QzJGNzNG", "publishedAt": "2022-08-13", "description": "Hoy vamos a ver un nuevo modelo para entender documentos (recibos, formularios, etc.) y extraer informaci\u00f3n de ellos sin necesitar de un paso previo con OCR. Hace muy pocos d\u00edas el c\u00f3digo y los modelos se hicieron open-source y ya se implement\u00f3 dentro de Hugging Face para hacer su uso mas sencillo. Todo esto es gratuito y, por ahora, el estado del arte.\n\nContenido\n00:00 - Qu\u00e9 es \u201cDocument Understanding\u201d?\n04:33 - Paper de \u201cDonut: OCR-free Document Understanding Transformer\u201d\n09:54 - Implementaci\u00f3n en Hugging Face\n11:48 - Notebooks y demo\n16:45 - Conclusiones\n\nAlgunos links mencionados durante el video:\n\ud83d\udcbb Layout Parser: https://layout-parser.github.io/ \n\ud83d\udcdd Paper \u201cOCR-free Document Understanding Transformer\u201d: https://arxiv.org/pdf/2111.15664.pdf \n\ud83d\udcbb Repo de GitHub: https://github.com/clovaai/donut \n\ud83e\udd17 Implementaci\u00f3n de Donut en Hugging Face: https://huggingface.co/docs/transformers/main/en/model_doc/donut \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#python #huggingface #donut #ocr"}, {"url": "tko4p1ydgZY", "label": "[#18] Como clasificar video clips en segundos con Hugging Face (Microsoft X-CLIP)", "imagen": "https://i.ytimg.com/vi/tko4p1ydgZY/mqdefault.jpg", "playlistId": ["PLWhCQi97dstk-n_W_Eh90b_7tLWayDqcr"], "position": 6, "type": "video", "id": "UExXaENRaTk3ZHN0ay1uX1dfRWg5MGJfN3RMV2F5RHFjci41MzJCQjBCNDIyRkJDN0VD", "publishedAt": "2022-09-10", "description": "Hoy vamos a ver como clasificar clips de video, o mas o menos entender que esta pasando en una secuencia de im\u00e1genes, sin ning\u00fan tipo de entrenamiento previo. Esto es un problema multi-modal donde juntamos texto e im\u00e1genes. El paper del que vamos a hablar se llama \u201cX-CLIP\u201d, de Microsoft, y fue implementado en Hugging Face por Niels Rogge, hace muy poco tiempo. \n\nContenido\n00:00 Qu\u00e9 es \u201cvideo clip classification\u201d?\n05:30 Implementaci\u00f3n y demo: \u201cExpanding Language-Image Pretrained Models for General Video Recognition\u201d\n14:45 Conclusiones\n\nLink a notebook en Google Colab:\n\ud83d\udcbb https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/X-CLIP/Zero_shot_classify_a_YouTube_video_with_X_CLIP.ipynb\n\nLinks mencionados durante el video:\n\ud83d\udcbb [ECCV'2022 Oral] - Expanding Language-Image Pretrained Models for General Video Recognition: https://github.com/microsoft/VideoX/tree/master/X-CLIP \n\ud83e\udd17 Implementaci\u00f3n en Hugging Face: https://huggingface.co/docs/transformers/main/en/model_doc/xclip\n\ud83e\udd17 Real-time demo: https://huggingface.co/spaces/fcakyon/zero-shot-video-classification \n\ud83d\udcbb Niels Rogge\u2019s Transformers Tutorials: https://github.com/NielsRogge/Transformers-Tutorials\n\ud83d\udcbb Kinetics 700-2020 Dataset: https://www.deepmind.com/open-source/kinetics \n\ud83d\udcdd Paper \u201cExpanding Language-Image Pretrained Models for General Video Recognition\u201d: https://arxiv.org/abs/2208.02816\n\ud83e\udd17 X-CLIP model checkpoints: https://huggingface.co/models?other=xclip \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#python #huggingface #clip #classification"}, {"url": "cvrXCinggts", "label": "[#42] ICON: Modelando los mejores momentos de Qatar 2022 en 3D \u26bd con Blender y ML", "imagen": "https://i.ytimg.com/vi/cvrXCinggts/mqdefault.jpg", "playlistId": ["PLWhCQi97dstk-n_W_Eh90b_7tLWayDqcr"], "position": 10, "type": "video", "id": "UExXaENRaTk3ZHN0ay1uX1dfRWg5MGJfN3RMV2F5RHFjci41Mzk2QTAxMTkzNDk4MDhF", "publishedAt": "2022-12-23", "description": "Estos dias sali\u00f3 un articulo bastante bueno en el New York Times donde explican como hicieron para modelar momentos claves del Mundial Qatar 2022 en 3D. Me pareci\u00f3 muy copado y queria meterme mas en el tema. Vamos a verlo. #blender #3d #ai \n\nContenido\n00:00 - C\u00f3mo hizo el NYT para usar ML en el Mundial?\n06:30 - Qu\u00e9 es y como funciona ICON/ECON?\n11:30 - Demo con Blender\n\nLink a Hugging Face Spaces:\nhttps://huggingface.co/spaces/Yuliang/ICON \n\nLinks mencionados durante el video:\n\ud83d\udcbb NYT - Modeling Key World Cup Moments with Machine Learning: https://rd.nytimes.com/projects/modeling-key-world-cup-moments-with-machine-learning \n\ud83d\udcbb NYT - How Pulisic Crafted the U.S. Goal in Its World Cup Opener: https://www.nytimes.com/interactive/2022/11/21/sports/world-cup/wales-usmnt-world-cup-goal-pulisic.html \n\ud83d\udcbb ICON: Implicit Clothed humans Obtained from Normals (CVPR2022) - Yuliang Xiu on Talking Papers: https://www.youtube.com/watch?v=JPk9gu_dQD0&ab_channel=TalkingPapersPodcast \n\ud83d\udcbb ICON: Implicit Clothed humans Obtained from Normals: https://icon.is.tue.mpg.de/ \n\ud83d\udcbb Repo/ICON: Implicit Clothed humans Obtained from Normals (CVPR 2022): https://github.com/YuliangXiu/ICON \n\ud83d\udcbb ECON: Explicit Clothed humans Obtained from Normals: https://xiuyuliang.cn/econ/ \n\ud83d\udcbb Repo/ECON: Explicit Clothed humans Obtained from Normals: https://github.com/YuliangXiu/ECON#demo \n\ud83d\udcbb nerfstudio: https://docs.nerf.studio/en/latest/ \n\ud83d\udcbb Image to 3D Model Using ICON| Day 241 of Blender: https://www.youtube.com/watch?v=N4L4S-VbuAA&ab_channel=saraimarte \n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9"}, {"url": "FUEIwAsrMP4", "label": "[#01] Introducci\u00f3n a Amazon SageMaker Studio Lab, una alternativa gratuita a Google Colab  (+ Repo)", "imagen": "https://i.ytimg.com/vi/FUEIwAsrMP4/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm5iEA98w4seph_GNou8Sy-"], "position": 0, "type": "video", "id": "UExXaENRaTk3ZHN0bTVpRUE5OHc0c2VwaF9HTm91OFN5LS41NkI0NEY2RDEwNTU3Q0M2", "publishedAt": "2021-12-22", "description": "En este video, vamos a ver una demo de SageMaker Studio Lab, un servicio gratuito de Amazon para correr Jupyter notebooks de forma online. Excelente para estudiantes o personas que quieren hacer desarrollo/experimentar sin tener que desplegar recursos localmente ni preocuparse por CUDA drivers o problemas con librerias/dependencias, etc.\n\nCapitulos\n00:00 - Intro\n00:35 - Que es y para que sirve Amazon SageMaker Studio Lab?\n06:55 - Como se compara con Google Colab?\n09:39 - Benchmark entre Studio Lab vs Google Colab\n11:20 - SageMaker Studio Lab Demo\n14:28 - Demo de ASR en Espa\u00f1ol usando HuggingFace\n\nLinks mencionados durante el video:\n\ud83d\udcbb  SageMaker Studio Lab: https://studiolab.sagemaker.aws/\n\u270f\ufe0f  Benchmark entre StudioLab vs Google Colab: https://benjaminwarner.dev/2021/12/08/testing-amazon-sagemaker-studio-lab\n\ud83d\udcbb  Como hacer SSH a Google Colab: https://github.com/WassimBenzarti/colab-ssh\n\ud83d\udcbb  Repo con environment.yml + Speech Recognition para Espa\u00f1ol: https://github.com/machinelearnear/sagemaker-studio-lab-quickstart\n\ud83d\udcf9  Otro review por Julien Simon (ex. AWS): https://www.youtube.com/watch?v=SAxaucXVy_Q\n\n\ud83e\uddc9  No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#studiolab #aws #sagemaker"}, {"url": "2YGHxnpwKlQ", "label": "[#15] C\u00f3mo elegir la GPU correcta para Deep Learning en AWS", "imagen": "https://i.ytimg.com/vi/2YGHxnpwKlQ/mqdefault.jpg", "playlistId": ["PLWhCQi97dstm5iEA98w4seph_GNou8Sy-"], "position": 1, "type": "video", "id": "UExXaENRaTk3ZHN0bTVpRUE5OHc0c2VwaF9HTm91OFN5LS4yODlGNEE0NkRGMEEzMEQy", "publishedAt": "2022-08-16", "description": "Hoy vamos a hablar un poco m\u00e1s sobre que significa hacer ML a gran escala hablando sobre que cosas tenemos que tener en cuenta para entrenar modelos o para hacer inferencia en producci\u00f3n. Para empezar el tema, vamos a ver como elegir la GPU correcta dentro de AWS, optimizar el costo, y en que casos usar cada una.\n\nContenido\n00:00 - Porqu\u00e9 es importante?\n03:40 - Recomendaciones\n06:45 - Instancias GPU en detalle (P3, P4, G4, G5)\n13:30 - Qu\u00e9 software y framework usar en AWS?\n17:40 - Optimizar costo\n20:10 - Conclusiones\n\nLinks mencionados durante el video:\n\ud83d\udcbb State of AI Report 2022: https://www.stateof.ai/ \n\ud83d\udcbb Choosing the right GPU for deep learning on AWS: https://towardsdatascience.com/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86\n\ud83d\udcbb Cloud GPUs: https://fullstackdeeplearning.com/cloud-gpus/ \n\ud83d\udcbb MosaicML: https://github.com/mosaicml/composer \n\ud83d\udcbb Colossal-AI: https://github.com/hpcaitech/ColossalAI\n\ud83d\udcbb DeepSpeed (Microsoft): https://www.deepspeed.ai/\n\ud83d\udcbb Alpa: https://github.com/alpa-projects/alpa\n\ud83d\udcbb BigDL (Intel): https://github.com/intel-analytics/BigDL\n\ud83d\udcbb SageMaker (Amazon): https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html\n\n\ud83e\uddc9 No te olvides de suscribirte al canal para recibir notificaciones de nuevos videos \ud83e\uddc9\n\n#aws #sagemaker #gpu #nvidia"}, {"id": "PLWhCQi97dstnLKjstxZzEXoc1dyp4cXXB", "label": "ChatGPT: Curso LLM-RLHF", "type": "playlist"}, {"id": "PLWhCQi97dstmsOxjxtnSHDy0Eg21z-pxf", "label": "Stable Diffusion & Arte AI", "type": "playlist"}, {"id": "PLWhCQi97dstnIDPxUQ0UwV--N33qwSxxT", "label": "Noticias", "type": "playlist"}, {"id": "PLWhCQi97dstnHXQy-lOObayVekv2eHp0A", "label": "Demos", "type": "playlist"}, {"id": "PLWhCQi97dstn3o5xgG79KcWRUeKLDmNKb", "label": "Herramientas", "type": "playlist"}, {"id": "PLWhCQi97dstlkExaGPgTZPEcMKrqOuNtw", "label": "Tutoriales", "type": "playlist"}, {"id": "PLWhCQi97dstk-n_W_Eh90b_7tLWayDqcr", "label": "Papers", "type": "playlist"}, {"id": "PLWhCQi97dstm5iEA98w4seph_GNou8Sy-", "label": "AWS", "type": "playlist"}]}
    </div>

</body>

</html>